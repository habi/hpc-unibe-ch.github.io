{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to the High Performance Computing (HPC) documentation of the University of Bern","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>Official documentation site of the high performance computing and the HPC cluster UBELIX.</p> <p>Currently, the UBELIX cluster runs around 320 compute nodes featuring ~12k CPU cores and 160 GPUs with almost one million GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course.</p> <p>If your campus account is not yet activated for UBELIX, the Getting Started Guide might be a good place to get you started.</p> <p>UBELIX features a plethora of software and applications, which is outlined on the page Software, but the users are free to compile and install their own software within their home directories.</p> <p>If you are wondering\u2026 UBELIX is an acronym and stands for University of Bern Linux Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way).</p> <p>Job Monitoroing</p> <p>See what is currently running on UBELIX on the Job Monitoring pages.</p>"},{"location":"index.html#acknowledging-ubelix","title":"Acknowledging UBELIX","text":"<p>When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX.</p> <p>Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements:</p> <pre><code>\"Calculations were performed on UBELIX (https://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\"\n</code></pre>"},{"location":"contributing.html","title":"Contributing","text":"<p>You can support the UBELIX cluster in different ways:</p> <ul> <li>Investments</li> <li>Documentation Improvements</li> </ul>"},{"location":"contributing.html#investments","title":"Investments","text":"<p>Some text about money\u2026</p>"},{"location":"contributing.html#documentation-improvements","title":"Documentation Improvements","text":"<p>Some text aboutn contirbuting to the user guide.</p>"},{"location":"mdcheat.html","title":"Markdown Cheatsheet","text":"<p>This page outlines all stuff available by installing the base Python-Markdown (comes with MkDocs) and the additional bundle PyMdown Extensions.</p> <ul> <li>Markdown Cheatsheet<ul> <li>Headings<ul> <li>The 3rd level<ul> <li>The 4th level<ul> <li>The 5th level<ul> <li>The 6th level</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Headings with secondary text<ul> <li>The 3rd level with secondary text<ul> <li>The 4th level with secondary text<ul> <li>The 5th level with secondary text<ul> <li>The 6th level with secondary text</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Emphasis</li> <li>Lists</li> <li>Links</li> <li>Images</li> <li>Code and Syntax Highlighting</li> </ul> </li> <li>!/bin/bash</li> <li>include <ul> <li>Tables</li> <li>Blockquotes</li> <li>Inline HTML</li> <li>Horizontal Rule</li> <li>Line Breaks</li> <li>YouTube Videos</li> <li>Admonition</li> <li>Abbreviations</li> <li>Definition Lists</li> <li>Footnotes</li> </ul> </li> </ul>"},{"location":"mdcheat.html#headings","title":"Headings","text":"<pre><code>### The 3rd level\n\n#### The 4th level\n\n##### The 5th level\n\n###### The 6th level\n\n## Headings &lt;small&gt;with secondary text&lt;/small&gt;\n\n### The 3rd level &lt;small&gt;with secondary text&lt;/small&gt;\n\n#### The 4th level &lt;small&gt;with secondary text&lt;/small&gt;\n\n##### The 5th level &lt;small&gt;with secondary text&lt;/small&gt;\n\n###### The 6th level &lt;small&gt;with secondary text&lt;/small&gt;\n</code></pre>"},{"location":"mdcheat.html#the-3rd-level","title":"The 3rd level","text":""},{"location":"mdcheat.html#the-4th-level","title":"The 4th level","text":""},{"location":"mdcheat.html#the-5th-level","title":"The 5th level","text":""},{"location":"mdcheat.html#the-6th-level","title":"The 6th level","text":""},{"location":"mdcheat.html#headings-with-secondary-text","title":"Headings with secondary text","text":""},{"location":"mdcheat.html#the-3rd-level-with-secondary-text","title":"The 3rd level with secondary text","text":""},{"location":"mdcheat.html#the-4th-level-with-secondary-text","title":"The 4th level with secondary text","text":""},{"location":"mdcheat.html#the-5th-level-with-secondary-text","title":"The 5th level with secondary text","text":""},{"location":"mdcheat.html#the-6th-level-with-secondary-text","title":"The 6th level with secondary text","text":""},{"location":"mdcheat.html#emphasis","title":"Emphasis","text":"<pre><code>Emphasis, aka italics, with *asterisks* or _underscores_.\n\nStrong emphasis, aka bold, with **asterisks** or __underscores__.\n\nCombined emphasis with **asterisks and _underscores_**.\n\nStrikethrough uses two tildes. ~~Scratch this.~~\n</code></pre> <p>Emphasis, aka italics, with asterisks or underscores.</p> <p>Strong emphasis, aka bold, with asterisks or underscores.</p> <p>Combined emphasis with asterisks and underscores.</p> <p>Strikethrough uses two tildes. ~~Scratch this.~~</p>"},{"location":"mdcheat.html#lists","title":"Lists","text":"<p>(In this example, leading and trailing spaces are shown with with dots: \u22c5)</p> <pre><code>1. First ordered list item\n2. Another item\n\u22c5\u22c5\u22c5\u22c5* Unordered sub-list. \n\u22c5\u22c5\u22c5\u22c5* Item 2\n\u22c5\u22c5\u22c5\u22c5* Item 3\n1. Actual numbers don't matter, just that it's a number\n\u22c5\u22c5\u22c5\u22c51. Ordered sub-list\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Ordered subsub-list\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 2\n\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 3\n\u22c5\u22c5\u22c5\u22c51. Item 2\n\u22c5\u22c5\u22c5\u22c51. Item 3\n4. And another item.\n\n\u22c5\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).\n\n\u22c5\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5\n\u22c5\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)\n\n* Unordered list can use asterisks\n- Or minuses\n+ Or pluses\n</code></pre> <ol> <li>First ordered list item</li> <li>Another item<ul> <li>Unordered sub-list. </li> <li>Item 2</li> <li>Item 3</li> </ul> </li> <li>Actual numbers don\u2019t matter, just that it\u2019s a number<ol> <li>Ordered sub-list<ol> <li>Ordered subsub-list</li> <li>Item 2</li> <li>Item 3</li> </ol> </li> <li>Item 2</li> <li>Item 3</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we\u2019ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> </li> </ol> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul>"},{"location":"mdcheat.html#links","title":"Links","text":"<p>There are two ways to create links.</p> <pre><code>[I'm an inline-style link](https://www.google.com)\n\n[I'm an inline-style link with title](https://www.google.com \"Google's Homepage\")\n\n[I'm a reference-style link][Arbitrary case-insensitive reference text]\n\n[I'm a relative reference to a repository file](../blob/master/LICENSE)\n\n[You can use numbers for reference-style link definitions][1]\n\nOr leave it empty and use the [link text itself].\n\nURLs and URLs in angle brackets will automatically get turned into links. \nhttp://www.example.com or &lt;http://www.example.com&gt; and sometimes \nexample.com (but not on Github, for example).\n\nSome text to show that the reference links can follow later.\n\n[arbitrary case-insensitive reference text]: https://www.mozilla.org\n[1]: http://slashdot.org\n[link text itself]: http://www.reddit.com\n</code></pre> <p>I\u2019m an inline-style link</p> <p>I\u2019m an inline-style link with title</p> <p>I\u2019m a reference-style link</p> <p>I\u2019m a relative reference to a repository file</p> <p>You can use numbers for reference-style link definitions</p> <p>Or leave it empty and use the link text itself.</p> <p>URLs and URLs in angle brackets will automatically get turned into links.  http://www.example.com or http://www.example.com and sometimes  example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p>"},{"location":"mdcheat.html#images","title":"Images","text":"<pre><code>Here's our logo (hover to see the title text):\n\nInline-style: \n![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\")\n\nReference-style: \n![alt text][logo]\n\n[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\"\n</code></pre> <p>Here\u2019s our logo (hover to see the title text):</p> <p>Inline-style:  </p> <p>Reference-style:  </p>"},{"location":"mdcheat.html#code-and-syntax-highlighting","title":"Code and Syntax Highlighting","text":"<p>Code blocks are part of the Markdown spec, but syntax highlighting isn\u2019t. However, many renderers \u2013 like Github\u2019s and Markdown Here \u2013 support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page.</p> <pre><code>Inline `code` has `back-ticks around` it.\n</code></pre> <p>Inline <code>code</code> has <code>back-ticks around</code> it.</p> <p>Blocks of code are either fenced by lines with three back-ticks <code>```</code>, or are indented with four spaces. I recommend only using the fenced code blocks \u2013 they\u2019re easier and only they support syntax highlighting.</p> <pre><code>var s = \"JavaScript syntax highlighting\";\nalert(s);\n</code></pre> <pre><code>s = \"Python syntax highlighting\"\nprint s\n</code></pre> <pre><code>No language indicated, so no syntax highlighting in Markdown Here (varies on Github). \nBut let's throw in a &lt;b&gt;tag&lt;/b&gt;.\n</code></pre> <p>Even tabbed code example for different language are possible:</p> <p>```Bash tab=</p>"},{"location":"mdcheat.html#binbash","title":"!/bin/bash","text":"<p>STR=\u201dHello World!\u201d echo $STR <pre><code>```C tab=\n#include\n\nint main(void) {\n  printf(\"hello, world\\n\");\n}\n</code></pre></p> <p>```C++ tab=</p>"},{"location":"mdcheat.html#include","title":"include  <p>int main() {   std::cout &lt;&lt; \u201cHello, world!\\n\u201d;   return 0; } <pre><code>```C# tab=\nusing System;\n\nclass Program {\n  static void Main(string[] args) {\n    Console.WriteLine(\"Hello, world!\");\n  }\n}\n</code></pre></p>","text":""},{"location":"mdcheat.html#tables","title":"Tables","text":"<p>Tables aren\u2019t part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email \u2013 a task that would otherwise require copy-pasting from another application.</p> <pre><code>Colons can be used to align columns.\n\n| Tables        | Are           | Cool  |\n| ------------- |:-------------:| -----:|\n| col 3 is      | right-aligned | $1600 |\n| col 2 is      | centered      |   $12 |\n| zebra stripes | are neat      |    $1 |\n\nThere must be at least 3 dashes separating each header cell.\nThe outer pipes (|) are optional, and you don't need to make the \nraw Markdown line up prettily. You can also use inline Markdown.\n\nMarkdown | Less | Pretty\n--- | --- | ---\n*Still* | `renders` | **nicely**\n1 | 2 | 3\n</code></pre> <p>Colons can be used to align columns.</p> Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u2019t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> Markdown Less Pretty Still <code>renders</code> nicely 1 2 3"},{"location":"mdcheat.html#blockquotes","title":"Blockquotes","text":"<pre><code>&gt; Blockquotes are very handy in email to emulate reply text.\n&gt; This line is part of the same quote.\n\nQuote break.\n\n&gt; This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. \n</code></pre> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> <p>Quote break.</p> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let\u2019s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. </p> <p>Blockquote nesting is possible:</p> <pre><code>&gt; **Sed aliquet**, neque at rutrum mollis, neque nisi tincidunt nibh, vitae\n  faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem\n  [libero fermentum](#) urna, ut efficitur elit ligula et nunc.\n\n&gt; &gt; Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla.\n    Ut sit amet placerat ante. Proin sed **elementum** __nulla__. Nunc vitae sem odio.\n    Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum.\n    eu odio.\n\n&gt; &gt; &gt; `Suspendisse rutrum facilisis risus`, eu posuere neque commodo a.\n      Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo\n      bibendum, sodales mauris ut, tincidunt massa.\n</code></pre> <p>Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh, vitae   faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem   libero fermentum urna, ut efficitur elit ligula et nunc.</p> <p>Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla.     Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio.     Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum.     eu odio.</p> <p><code>Suspendisse rutrum facilisis risus</code>, eu posuere neque commodo a.       Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo       bibendum, sodales mauris ut, tincidunt massa.</p> <p>Other content blocks within a blockquote</p> <p>Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu   lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl,   sit amet laoreet nibh.   <pre><code>var _extends = function(target) {\n  for (var i = 1; i &lt; arguments.length; i++) {\n    var source = arguments[i];\n    for (var key in source) {\n      target[key] = source[key];\n    }\n  }\n  return target;\n};\n</code></pre></p> <p>Praesent at <code>:::js return target</code>, sodales nibh vel, tempor felis. Fusce       vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices.       Donec consectetur mauris non neque imperdiet, eget volutpat libero.</p>"},{"location":"mdcheat.html#inline-html","title":"Inline HTML","text":"<p>You can also use raw HTML in your Markdown, and it\u2019ll mostly work pretty well. </p>"},{"location":"mdcheat.html#horizontal-rule","title":"Horizontal Rule","text":"<pre><code>Three or more...\n\n---\n\nHyphens\n\n***\n\nAsterisks\n\n___\n\nUnderscores\n</code></pre> <p>Three or more\u2026</p> <p>Hyphens</p> <p>Asterisks</p> <p>Underscores</p>"},{"location":"mdcheat.html#line-breaks","title":"Line Breaks","text":"<p>My basic recommendation for learning how line breaks work is to experiment and discover \u2013 hit &lt;Enter&gt; once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You\u2019ll soon learn to get what you want. \u201cMarkdown Toggle\u201d is your friend. </p> <p>Here are some things to try out:</p> <pre><code>Here's a line for us to start with.\n\nThis line is separated from the one above by two newlines, so it will be a *separate paragraph*.\n\nThis line is also a separate paragraph, but...\nThis line is only separated by a single newline, so it's a separate line in the *same paragraph*.\n</code></pre> <p>Here\u2019s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a separate paragraph.</p> <p>This line is also begins a separate paragraph, but\u2026 This line is only separated by a single newline, so it\u2019s a separate line in the same paragraph.</p>"},{"location":"mdcheat.html#youtube-videos","title":"YouTube Videos","text":"<p>They can\u2019t be added directly but you can add an image with a link to the video like this:</p> <pre><code>&lt;a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=YOUTUBE_VIDEO_ID_HERE\n\" target=\"_blank\"&gt;&lt;img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" \nalt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /&gt;&lt;/a&gt;\n</code></pre> <p>Or, in pure Markdown, but losing the image sizing and border:</p> <pre><code>[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)\n</code></pre> <p>Referencing a bug by #bugID in your git commit links it to the slip. For example #1. </p>"},{"location":"mdcheat.html#admonition","title":"Admonition","text":"<pre><code>!!! type \"optional explicit title within double quotes\"\n    Any number of other indented markdown elements.\n\n    This is the second paragraph.\n</code></pre> <p>Some title</p> <p>Any number of other indented markdown elements.</p> <p>This is the second paragraph.</p> <p>And this is outside the admonition again.</p> <p>If you don\u2019t want a title, use a blank string \u201c\u201d.</p> <p>Don\u2019t do this at home!</p> <p>rST suggests the following \u201ctypes\u201d: attention, caution, danger, error, hint, important, note, tip, and warning:  </p> <p>Some title</p> <p>This is type note</p> <p>Some title</p> <p>This is type hint</p> <p>Some title</p> <p>This is type tip</p> <p>Some title</p> <p>This is type important</p> <p>Some title</p> <p>This is type attention</p> <p>Some title</p> <p>This is type caution</p> <p>Some title</p> <p>This is type warning</p> <p>Some title</p> <p>This is type danger</p> <p>Some title</p> <p>This is type error</p>"},{"location":"mdcheat.html#abbreviations","title":"Abbreviations","text":"<pre><code>The HTML specification\nis maintained by the W3C.\n\n*[HTML]: Hyper Text Markup Language\n*[W3C]:  World Wide Web Consortium\n</code></pre> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"mdcheat.html#definition-lists","title":"Definition Lists","text":"<pre><code>Apple\n:   Pomaceous fruit of plants of the genus Malus in\n    the family Rosaceae.\n\nOrange\n:   The fruit of an evergreen tree of the genus Citrus.\n</code></pre> Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus."},{"location":"mdcheat.html#footnotes","title":"Footnotes","text":"<p>Footnotes<sup>1</sup> have a label<sup>2</sup> and the footnote\u2019s content. Another Footnote<sup>3</sup></p> <p>License: CC-BY</p> <ol> <li> <p>This is a footnote content.\u00a0\u21a9</p> </li> <li> <p>A footnote on the label: \u201c@#$%\u201d.\u00a0\u21a9</p> </li> <li> <p>Another content\u00a0\u21a9</p> </li> </ol>"},{"location":"file-system/file-transfer.html","title":"File Transfer from/to UBELIX","text":""},{"location":"file-system/file-transfer.html#description","title":"Description","text":"<p>This page contains some basic information about moving files between your local workstation and the cluster.</p>"},{"location":"file-system/file-transfer.html#maclinuxwindows","title":"Mac/Linux/Windows","text":"<p>You can use different protocols/programs for transferring files from/to the cluster, depending on your need: Sftp, SCP, Rsync, Wget, and others.</p> <p>The following commands are from on your local workstation as indicated by \u201clocal$\u201d</p> <p>If you have customized your SSH environment as described here, you can substitute your host alias for @submit03.unibe.ch in the following commands"},{"location":"file-system/file-transfer.html#secure-copy-scp-maclinux","title":"Secure Copy (SCP) - Mac/Linux","text":"<p>Secure Copy is a program (also a protocol) that allows you to securely transfer files between local and remote hosts. SCP uses SSH for transferring data and managing authentication. SCP performs a plain linear copy of the specified files, while replacing already existing files with the same name. If you need more sophisticated control over your copy process, consider Rsync.</p> <p>Syntax</p> <p>scp [options] source destination</p> <p>Some common options</p> <ul> <li><code>-r</code>: copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal)</li> <li><code>-p</code>: preserve modification time, access time, and modes from the original file</li> <li><code>-v</code>: verbose mode</li> </ul>"},{"location":"file-system/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix","title":"Copying Files from Your Local Workstation to UBELIX","text":"<p>Copy the file <code>~/dir/file01</code> to your remote home directory:</p> <pre><code>$ scp ~/dir/file01 &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy multiple files to the remote directory <code>~/bar</code>:</p> <p>The destination directory must already exist. You can create a directory from remote with: ssh @submit03.unibe.ch \u2018mkdir -p ~/bar\u2019 <pre><code>$ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 &lt;user&gt;@submit03.unibe.ch:bar\n</code></pre> <p>Copy all files within directory <code>~/dir</code> to the remote directory <code>~/bar</code>:</p> <p>Add the -r option (recursive) to also copy all subdirectories of ~/dir</p> <pre><code>$ scp -r ~/dir/* &lt;user&gt;@submit03.unibe.ch:bar\n</code></pre> <p>Copy the directory <code>~/dir</code> to your remote home directory:</p> <p>This will create a new directory ~/dir on the remote host. If the directory ~/dir already exists, the following command adds the content of the source directory to the destination directory</p> <pre><code>$ scp -r ~/dir &lt;user&gt;@submit03.unibe.ch:\n</code></pre>"},{"location":"file-system/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation","title":"Copying Files from UBELIX to Your Local Workstation","text":"<p>Copy the remote file <code>~/bar/file01</code> to the current working directory on your local workstation:</p> <pre><code>$ scp &lt;user&gt;@submit03.unibe.ch:bar/file01 .\n</code></pre> <p>Copy multiple remote files to the local directory <code>~/dir</code>:</p> <p>The local directory ~/dir will be automatically created if it does not already exist</p> <pre><code>$ scp &lt;user&gt;@submit03.unibe.ch:bar/\\{file02,file03,file04\\} ~/dir\n</code></pre> <p>Copy the remote directory <code>~/bar</code> to the current working directory on your local workstation:</p> <pre><code>$ scp -r &lt;user&gt;@submit03.unibe.ch:bar .\n</code></pre>"},{"location":"file-system/file-transfer.html#remote-sync-rsync-maclinux","title":"Remote Sync (Rsync) - Mac/Linux","text":"<p>Rsync implements a sophisticated algorithm that allows to transfer only missing/non-matching parts of a source file to update a target file. With this the process of transferring data may be significantly faster than simply replacing all data. Among other things, Rsync also allows you to specify complex filter rules to exclude certain files or directories located inside a directory that you want to sync.</p> <p>Syntax</p> <p>rsync [options] source destination </p> <p>Some common options</p> <ul> <li><code>-r</code>: copy directories recursively (does not preserve timestamps and permissions)</li> <li><code>-a</code>: archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks)</li> <li><code>-z</code>: compress data</li> <li><code>-v</code>: verbose mode (additional v\u2019s will increase verbosity level)</li> <li><code>-n</code>: dry-run</li> <li><code>-h</code>: output numbers in a human readable format</li> </ul>"},{"location":"file-system/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix_1","title":"Copying Files from Your Local Workstation to UBELIX","text":"<p>Copy the file <code>~/dir/file01</code> to your remote home directory:</p> <pre><code>$ rsync ~/dir/file01 &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy multiple files to your remote home directory:</p> <pre><code>$ rsync file01 file02 file03 &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy the local directory ~/dir to the remote directory ~/bar:</p> <p>With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory</p> <pre><code>$ rsync -az ~/dir/ &lt;user&gt;@submit03.unibe.ch:bar\n</code></pre>"},{"location":"file-system/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation_1","title":"Copying Files from UBELIX to Your Local Workstation","text":"<p>Copy the remote file <code>~/foo/file01</code> to your current working directory:</p> <pre><code>$ rsync &lt;user&gt;@submit03.unibe.ch:foo/file01 .\n</code></pre> <p>Copy the remote files <code>~/foo/file01</code> and <code>~/bar/file02</code> to your the local directory <code>~/dir</code>:</p> <pre><code>$ rsync &lt;user&gt;@submit03.unibe.ch:\\{foo/file01,bar/file02\\} ~/dir\n</code></pre> <p>Copy the remote directory <code>~/foo</code> to the local directory <code>~/dir</code>:</p> <p>With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory.</p> <pre><code>$ rsync -az &lt;user&gt;@submit03.unibe.ch:foo/ ~/dir\n</code></pre>"},{"location":"file-system/file-transfer.html#includingexcluding-files","title":"Including/Excluding Files","text":"<p>With the <code>--include</code>/<code>--exclude</code> options you can specify patterns, that describe which files are not excluded/excluded from the copy process.</p> <p>Use the <code>-n</code> option with the <code>-v</code> option to perform a dry-run while listing the files that would be copied</p> <p>Exclude a specific directory:</p> <pre><code>rsync -av --exclude \"subdir1\" ~/dir/ &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy only files with suffix <code>.txt</code> and <code>.m</code>:</p> <pre><code>rsync -av --include \"*.txt\" --include \"*.m\" --exclude \"*\" ~/dir/ &lt;user&gt;@submit03.unibe.ch:\n</code></pre> <p>Copy all files with suffix <code>.m</code> within the source directory <code>~/dir</code> (including matching files within subdirectories) to the remote destination directory <code>~/foo</code>:</p> <p>Use the <code>--prune-empty-dirs</code> option to omit copying empty directories</p> <pre><code>$ rsync -av --prune-empty-dirs --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ &lt;user&gt;@submit03.unibe.ch:foo\n</code></pre>"},{"location":"file-system/file-transfer.html#deleting-files","title":"Deleting Files","text":"<p>None of the following commands will delete any files in your source folder</p> <p>This delete options can be dangerous if used incorrectly! Perform a dry-run (<code>-n</code> option) first and verify that important files are not listed (<code>-v</code> option) for deletion</p> <p>Use the <code>--delete</code> option to delete files/directories from the destination directory that are not/no more present in the source directory:</p> <pre><code>$ rsync -av --delete ~/dir/ &lt;user&gt;@submit03.unibe.ch:mfiles\n</code></pre> <p>With the <code>--delete-excluded</code> option you can additionally delete files from the destination directory that are excluded from transferring/syncing (not in the generated file list):</p> <pre><code>$ rsync -av --prune-empty-dirs --delete-excluded --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ &lt;user&gt;@submit03.unibe.ch:foo\n</code></pre>"},{"location":"file-system/file-transfer.html#mobaxterm-windows","title":"MobaXterm - Windows","text":"<p>In MobaXterm there are multiple ways to transfer files. After initializing/starting a session to UBELIX copied by \u201cdrag and drop\u201d in the File browser on the left hand side. </p> <p>Further the local terminal can be used to transfer files using commands described above.   </p>"},{"location":"file-system/filesystem-overview.html","title":"File Systems Overview","text":""},{"location":"file-system/filesystem-overview.html#home","title":"HOME:","text":"<p><code>$HOME</code> directories are located under <code>/home/$USER</code>, where <code>$USER</code> is the campus account. <code>$HOME</code> is limited to maximum <code>1TB</code> and is meant for private and configuration data. There is no data sharing workflow considerred for <code>$HOME</code>. If you want to share data with collaborators please ask your research group leader to request an HPC Workspace. Reqular Snapshots provide possibility to recover accidentally modified or deleted data.  Some application, by default, use the <code>$HOME</code> file system even for larger amount of data, e.g. user packages in Python or R. Often this can be redirected, e.g. using <code>--prefix</code> option or in worse case using a symbolic link to a project directory. Get in touch with us if you need assistance.</p>"},{"location":"file-system/filesystem-overview.html#hpc-workspaces","title":"HPC Workspaces","text":"<p>In HPC Workspaces access is defined in two user defined access groups. A primary read/write group is meant to be the data owner, while a secondary has only read access. The sizes of these spaces are managed by the Workspace owners. Beside a free of charge quota, the space can be extended with costs. </p> <p>For more details see Workspace management</p>"},{"location":"file-system/filesystem-overview.html#research-storage-shares","title":"Research Storage shares","text":"<p>The predecessor of HPC Workspaces are Research Storage Shares. These file spaces are located under <code>/storage/research/...</code>. These are fully managed by HPC support team. All modifications need to be requested via support request on the Service Portal.  We aim to migrate these spaces and their data into HPC Workspaces. Therewith users can use all the tools and framework around HPC Workspaces and also the free of charge quota. </p>"},{"location":"file-system/filesystem-overview.html#scratch","title":"SCRATCH","text":"<p>SCRATCH is a temporary space with less restrictive limitations in size, but more restrictive limitation in time.  There is no snapshot or backup service implemented in that space.</p>"},{"location":"file-system/filesystem-overview.html#cleaning-policy","title":"Cleaning Policy","text":"<p>under Construction</p> <p>detailed information will follow soon</p>"},{"location":"file-system/filesystem-overview.html#quota","title":"Quota","text":"<p>For <code>HOME</code> and <code>WORKSPACES</code> total storage size and amount of files are limited. The current used amount and limits can be displayed using the <code>quota</code> tool, see File System Quota</p>"},{"location":"file-system/quota.html","title":"File System Quota","text":""},{"location":"file-system/quota.html#description","title":"Description","text":"<p>This page contains information about quota limits on the parallel file system. Quotas are enabled to control the file system usage.</p> <p> Job abortion</p> <p>Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded </p>"},{"location":"file-system/quota.html#quotas","title":"Quotas","text":"space quota file quota backup expiration HOME 1TB 1M yes - WORKSPACE free: up to 10TB per research group<sup>1</sup> 1M per TB yes 1 year<sup>2</sup> SCRATCH 30TB<sup>3</sup> 10M<sup>3</sup> no 1 month<sup>4</sup>"},{"location":"file-system/quota.html#display-quota-information","title":"Display quota information","text":"<p>A quota tool is delivered with the Workspace module:</p> <p><pre><code>$ quota\nUniBE Workspace Quota report\n============================\n                    :  used (GB)(   %), quota (GB) |  files used(   %),      quota\n==================================================================================\nHOME                :        420( 41%),       1024 |      773425( 77%),    1000000\nWorkspace1          :        101(  1%),      10240 |           3(  0%),   10000000\n</code></pre> Furthermore, there is a more detailed version using the <code>-l</code> or <code>--long</code> option <pre><code>$ quota -l\nUniBE Workspace Quota report\n============================\n                    : free quota,  used (GB)(   %), quota (GB) |  files used(   %),      quota | start date(1), average quota(2)\n================================================================================================================================\nHOME                :        all,        421( 41%),       1024 |      796058( 79%),    1000000 |              ,\nSCR_usr             :        all,        269(  0%),      30720 |          22(  0%),   10000000 |              ,\nWorkspace1          :          5,        101(  1%),      10240 |           4(  0%),   10000000 |    2021-02-25,           7.5833\n\n(0) space names starting with \"SCR_\" refer to the personal (usr) or Workspace SCRATCH quota.\n(1) accounting period start date, The date from which the average usage is computed.\n(2) file space average quota (not files), calculated by the average of messured values in the actual accounting period.\n</code></pre></p> <p>In the last example the workspace <code>Workspace1</code> has <code>5TB</code> of free quota, and a total of <code>10TB</code> of quota (<code>5TB</code> additional storage requested). The <code>start date</code> defines the start of the accounting period and the average quota is computed as average over all datapoints starting from <code>start date</code>.  Furthermore, the SCRATCH quota is presented starting with <code>SCR_</code>, where <code>SCR_usr</code> is your personal SCRATCH quota and <code>SCR_</code> plus Workspace name the group quota of the Workspace group in the scratch fileset. Workspace SCRATCH quota is only presented if a quota is set and the <code>quota -l</code> option selected.</p> <p>data gathering</p> <ul> <li>Workspaces: Workspace quota information is gathered twice a day. Thus the presented data may not completely represent the current state.</li> <li>HOME and SCRATCH: values presented are actual values directly gathered from the file system</li> </ul> <p>Note: the coloring of the relative values is green (&lt;70%), yellow (70% &lt; x &lt; 90%), red (&gt;90%).</p> <ol> <li> <p>Each research group can use up to 10TB of free disk storage in multiple Workspaces free of charge. Quota increase can be purchased, see Workspace Management.\u00a0\u21a9</p> </li> <li> <p>Workspaces are meant to be active directories and no archive. Workspace are active by default for one year. The duration can every time be extended to \u201ccurrent date plus one year\u201d.\u00a0\u21a9</p> </li> <li> <p>Scratch quota is currently implemented per user\u00a0\u21a9\u21a9</p> </li> <li> <p>There is a automatic removal policy planned, but not implemented yet\u00a0\u21a9</p> </li> </ol>"},{"location":"file-system/scratch.html","title":"Scratch","text":"<p>Scratch - temporary file space</p>"},{"location":"file-system/scratch.html#description","title":"Description","text":"<p>Scratch file space are meant for temporary data storage. Interim computational data should be located there.  We distinguish local and network scratch spaces. </p>"},{"location":"file-system/scratch.html#network-scratch","title":"Network Scratch","text":"<p>Network scratch spaces are located on the parallel file system and accessible to all nodes. In contrast to <code>HOME</code> or <code>WORKSPACEs</code>, scratch is meant for temporary data, especially with larger quota requirements.  Jobs creating a lot of temporary data, which may need or may not need to be post-processed should run in this space. As an example, a application creating a huge amount of temporary output, which need to be analysed, but only partly need to be stored for a longer term. Quota and file quota is less restrictive on scratch compared to <code>HOME</code> or permanent <code>WORKSPACE</code> directories. Every user can use up to 30TB and 10M files. There is no snapshot and no backup feature available. Furthermore, an automatic deletion policy is planned, deleting files which are older than 30 days. </p> <p>Scratch file space can be accessed using the Workspace module and the <code>$SCRATCH</code> environment variable. </p> <pre><code>module load Workspace\ncd $SCRATCH\n</code></pre> <p>For personal Scratch see below</p>"},{"location":"file-system/scratch.html#workspace-scratch","title":"Workspace Scratch","text":"<p>Each Workspace has a <code>$SCRATCH</code> space with the same access permissions like the permanent Workspace directory (using primary and secondary groups). The Workspace can be accessed using <code>$SCRATCH</code> variable (after loading the Workspace module). It will point to <code>/storage/scratch/&lt;researchGroupID&gt;/&lt;WorkspaceID&gt;</code>. Please use <code>$SCRATCH</code> to access it. </p>"},{"location":"file-system/scratch.html#personal-scratch","title":"Personal Scratch","text":"<p>Users without a Workspace can also use \u201cpersonal\u201d Scratch. This space does need to be created initially: <pre><code>module load Workspace_Home\nmkdir $SCRATCH\ncd $SCRATCH\n</code></pre></p> <p>Please note that this space is per default no private space. If you want to restrict access you can change permissions using:</p> <pre><code>chmod 700 $SCRATCH\n</code></pre>"},{"location":"file-system/scratch.html#local-scratch","title":"Local Scratch","text":"<p>Cases:</p> <ul> <li>temporary files are produced, which are not relevant after the computation</li> <li>files need to be read or written multiple times within a job</li> </ul> <p>Local storage (<code>$TMPDIR</code>) should be used instead of network storage. </p> <p><code>$TMPDIR</code> is a node local storage which only exists during the job life time and cleaned automatically afterwards. The actual directory is <code>/scratch/local/&lt;jobID&gt;</code>, but it is highly recommended to use <code>$TMPDIR</code>.  If necessary data can be copied there initially at the beginning of the job, processes (multiple times) and necessary results copied back at the end. </p> <p><code>$TMPDIR</code> instead of <code>/tmp</code></p> <p><code>$TMPDIR</code> is much larger than <code>/tmp</code> and cleaned automatically. Especially in case of job errors data in <code>/tmp</code> will persist and clog the nodes memory. </p>"},{"location":"file-system/scratch.html#example-temporary-files","title":"Example: temporary files","text":"<p>In the following example the <code>example.exe</code> will need a place to store temporary/intermediate files, not necessary after the computation. The location is provided using the <code>--builddir</code> option. And the local scratch (<code>$TMPDDIR</code>) is specified. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name tmpdir\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n\nsrun example.exe --builddir=$TMPDIR input.dat\n</code></pre> <p>If you want to have the advantage of low latency file system (local) but you need to keep files, you still can use <code>$TMPDIR</code> and copy files to the network storage (e.g. <code>$WORKSPACE</code> or <code>$HOME</code>) at the end of your job. This is only efficient if a) more files are manipulated local (in <code>$TMPDIR</code>) than copied to the network storage or b) files are manipulated multiple times, before copying to the network storage.</p>"},{"location":"file-system/scratch.html#example-including-data-movement","title":"Example: including data movement","text":"<p>In the following example script, all files from the submitting directory are copied to the head compute node. At the end of the job all files from the compute node local directory is copied back. The compute node local <code>$TMPDIR</code> is used, which points to <code>/scratch/local/&lt;jobid&gt;</code>, a job specific directory in the nodes internal disc.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name tmpdir\n#SBATCH --nodes 1\n#SBATCH --ntasks 1\n#SBATCH --cpus-per-task 1\n###SBATCH --output slurm.out # when specifying output file name, add rm slurm.out in cleanup function\n\n# I. Define directory names [DO NOT CHANGE]\n# =========================================\n# get name of the temporary directory working directory, physically on the compute-node\nworkdir=\"${TMPDIR}\"\n# get submit directory\n# (every file/folder below this directory is copied to the compute node)\nsubmitdir=\"${SLURM_SUBMIT_DIR}\"\n\n# 1. Transfer to node [DO NOT CHANGE]\n# ===================================\n# create/empty the temporary directory on the compute node\nif [ ! -d \"${workdir}\" ]; then\n  mkdir -p \"${workdir}\"\nelse\n  rm -rf \"${workdir}\"/*\nfi\n\n# change current directory to the location of the sbatch command\n# (\"submitdir\" is somewhere in the home directory on the head node)\ncd \"${submitdir}\"\n# copy all files/folders in \"submitdir\" to \"workdir\"\n# (\"workdir\" == temporary directory on the compute node)\ncp -prf * ${workdir}\n# change directory to the temporary directory on the compute-node\ncd ${workdir}\n\n# 3. Function to transfer back to the head node [DO NOT CHANGE]\n# =============================================================\n# define clean-up function\nfunction clean_up {\n  # - remove log-file on the compute-node, to prevent overwiting actual output with empty file\n  rm slurm-${SLURM_JOB_ID}.out\n  # - TODO delete temporary files from the compute-node, before copying. Prevent copying unnecessary files.\n  # rm -r ...\n  # - change directory to the location of the sbatch command (on the head node)\n  cd \"${submitdir}\"\n  # - copy everything from the temporary directory on the compute-node\n  cp -prf \"${workdir}\"/* .\n  # - erase the temporary directory from the compute-node\n  rm -rf \"${workdir}\"/*\n  rm -rf \"${workdir}\"\n  # - exit the script\n  exit\n}\n\n# call \"clean_up\" function when this script exits, it is run even if SLURM cancels the job\ntrap 'clean_up' EXIT\n\n# 2. Execute [MODIFY COMPLETELY TO YOUR NEEDS]\n# ============================================\n# TODO add your computation here\n# simple example, hello world\nsrun echo \"hello world from $HOSTNAME\"\n</code></pre> <p>Further aspects to consider:</p> <ul> <li>copy only necessary files<ul> <li>have only necessary files in the submit directory</li> <li>remove all unnecessary files before copying the data back, e.g. remove large input files</li> </ul> </li> <li>In case of a parallel job, you need to verify that all process run on one single node (<code>--nodes=1</code>) OR copy the data to all related nodes (e.g. <code>srun -n1 cp ...</code>).</li> </ul>"},{"location":"general/code-of-conduct.html","title":"Code of Conduct","text":"<p>On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the  user and the system administrators. Most of those contents are quite self-explanatory  while others help to reduce the amount of support time needed to allocate.</p>"},{"location":"general/code-of-conduct.html#general","title":"General","text":"<ul> <li>We assume that you are familiar with some basic knowledge about Linux command    line (shell) navigation and shell scripting. If you never worked on the command    line, consider some Linux tutorials on the subject first.</li> <li>We expect you to exploit this valuable documentation before asking for help.   All that is needed to get some simple jobs done on UBELIX is documented here.</li> </ul>"},{"location":"general/code-of-conduct.html#account","title":"Account","text":"<ul> <li>Staff accounts are preferred over student accounts! If you currently use your    student Campus Account to access UBELIX, but you also possess a staff Campus    Account, get in contact with us so we can activate your staff Campus Account,   migrate all your user data to the new account and deactivate your student account   for UBELIX.</li> </ul>"},{"location":"general/code-of-conduct.html#mailing-list","title":"Mailing List","text":"<ul> <li>We communicate upcoming events (e.g. maintenance downtimes) on our mailing list.    Make sure that you are subscribed to this list, otherwise you will miss important   announcements.</li> </ul>"},{"location":"general/code-of-conduct.html#security","title":"Security","text":"<ul> <li>Do not share your account</li> <li>If using public key authentication, do not share your private key</li> </ul>"},{"location":"general/code-of-conduct.html#general-communication-with-the-cluster-administrators","title":"General Communication with the Cluster Administrators","text":"<ul> <li>Use the Service Portal    for questions, issues or comments regarding UBELIX. </li> <li>Do not use the personal email address of a cluster administrator. This   is important because it keeps all administrators informed about the ongoing    problem-solving process, and if one administrator is on vacation, another   administrator can help you with your question</li> <li>For each new problem start a new conversation with a new subject. Avoid to write   to us by replying to an old answer mail from the last problem that you received    from us or even worse by replying to mailing list email you received from us. The    point here is that though it looks like an ordinary email, you actually are opening   a new ticket in our ticket system (or reopening an old ticket if replying to an old email).</li> </ul>"},{"location":"general/code-of-conduct.html#problem-solving-process","title":"Problem-Solving Process","text":"<ul> <li>Exploit resources provided by your institute/research group before asking the UBELIX    staff about domain-specific problems. We make an effort to help you, but we are no    experts in your field, hence a colleague from your group who uses the cluster to solve    a similar problem like you do might be a better first contact</li> <li>Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer,   and then forward the outcome to you.</li> <li>Do not ask for/expect step-by-step solutions to a problem. Sometimes we give    step-by-step instructions, but generally you should use our answers to do some    refined research on the problem. If you still stuck, we are happy to provide further    support</li> <li>Always give an exact as possible description of the problem. Provide your username,    error messages, the path to the job script, the id of the job, and other hints that make   the problem-solving process as economic as possible.</li> </ul>"},{"location":"general/code-of-conduct.html#housekeeping","title":"Housekeeping","text":"<ul> <li>Clean up your home directory frequently, in particular before asking for an increase of your quota limit</li> <li>Do not save thousands of files in a single directory. Distribute the files to subdirectories</li> </ul>"},{"location":"general/code-of-conduct.html#job-submission","title":"Job Submission","text":"<ul> <li>Before submitting the same job a hundred times, please verify that the job finishes   successfully. We often experience that hundreds of jobs getting killed due to an   invalid path referenced in the job script, which generates hundreds of notification    mails in our system.</li> </ul>"},{"location":"general/code-of-conduct.html#cluster-performance","title":"Cluster Performance","text":"<ul> <li>DO NOT run resource-intensive computations directly on the login nodes AKA submit nodes. This   will have a negative impact on the performance of the whole cluster. Instead, generate a job script   that carries out the computations and submit this job script to the cluster using sbatch.</li> <li>DO NOT run server applications (PostgreSQL server, web server, \u2026) on the login nodes. Such a program usually run as a background process (daemon) rather   than being under the direct control of an interactive user. We will immediately kill such processes.</li> </ul>"},{"location":"general/costs_investments.html","title":"Costs and Investment","text":"<p>The HPC cluster is financed and maintained by the IT office of the University of Bern.</p> <p>The base usage of our cluster is free of charge. Base usage includes:</p> <ul> <li>1TB of disk space (aka your home directory)</li> <li>20TB of temporary scratch space for raw data/results</li> <li>Access to CPUs and GPUs for computation within limits, see     topic Slurm QosS for the details</li> </ul>"},{"location":"general/costs_investments.html#investments","title":"Investments","text":"<p>Interested users can invest into GPUs, CPUs and storage usage. The investors get elevated access to specific resources. The money raised by investments is used to buy additional resources for the cluster/storage will be purchased with the additional budget.</p> <p>Get in touch with us!</p> <p>If you are interested in our investment opportunities, get in touch with us by starting a service request at the Service Portal.</p>"},{"location":"general/costs_investments.html#disk-storage-costs","title":"Disk Storage Costs","text":""},{"location":"general/costs_investments.html#workspaces","title":"Workspaces","text":"<p>Every research group has 10TB free of charge quota. This can be used within one or more Workspaces. The amount used per Workspace is set at application time and can be changed later within the limitation.</p> <p>Additional storage can be purchased. On the application or modification form an quota upper limit can be set. Accounted will be the actual usage only. Therefore, the actual usage is monitored twice a day. The average value of all data points is used for accounting and billed once a year</p>"},{"location":"general/costs_investments.html#research-storage","title":"Research Storage","text":"<p>If you need a storage space for research data that can be used independent from the HPC cluster, that can be mounted using SMB protocol on your desktop, then the research storage opportunity get you covered. Price and service details can be found at Nevertheless, the data on such a research storage share can be directly in our HPC cluster.</p> <p>Price and service details can be found at the official service page. Beware that the documents are in German only currently.</p> <p>Statement of Cost for SNSF</p> <p>On the official service page of the research storage service you can also find a statement of cost for the service that you can add to your SNF proposal in order to budget money for storage costs in your project.</p>"},{"location":"general/faq.html","title":"FAQ","text":"<p>This page provides a collection of frequently asked questions.</p>"},{"location":"general/faq.html#file-system","title":"File system","text":""},{"location":"general/faq.html#what-if-my-home-is-full","title":"What if my HOME is full?","text":"<p>If you reached your quota, you will get strange warning about not being able to write temporary files etc. You can check your quota using the <code>quota</code> command. To resolve the situation you can follow these strategies:</p> <ol> <li>Decluttering: Check for unnecessary data. This could be:</li> </ol> <ul> <li>unused application packages, e.g. Python(2) packages in <code>$HOME/.local/lib/python*/site-packages/*</code></li> <li>temporary computational data, like already post processed output files</li> <li>duplicated data</li> </ul> <ol> <li>Pack and archive: The HPC storage is a high performance parallel storage and not meant to be an archive. Data not used in the short to midterm should be packed and moved to an archive storage. </li> </ol> <p>In general, we consider data on our HPC systems as research data. Further we consider research data to be shared sooner or later. And we aim to support and enhance collaborations. Therefore, we introduce group shared spaces, called HPC Workspaces. Ask your research group manager to add you to an existing Workspace or create a new one.  There will be no quota increase for HOME directories. </p>"},{"location":"general/faq.html#workspaces","title":"Workspaces","text":""},{"location":"general/faq.html#i-need-access-to-a-hpc-workspace-who-do-i-need-to-ask","title":"I need access to a HPC Workspace. Who do I need to ask?","text":"<p>HPC Workspaces are managed by the group manager/leader and if applicable a deputy. Therewith you need to ask them to add you to the primary or secondary group. See also HPC Workspace members.</p>"},{"location":"general/faq.html#i-need-to-share-data-with-my-colleges-what-can-i-do","title":"I need to share data with my colleges. What can I do?","text":"<p>HPC Workspaces are meant to host shared data. See HPC Workspaces</p>"},{"location":"general/faq.html#software-issues","title":"Software issues","text":""},{"location":"general/faq.html#why-the-system-is-complaining-abount-not-finding-an-existing-module","title":"Why the system is complaining abount not finding an existing module?","text":"<p>There are cases modules could not be found. This could be that the modules is not exiting in the target software stack, it could be hidden, or a version inconsitency. </p>"},{"location":"general/faq.html#hidden-modules","title":"hidden modules","text":"<p>Some modules are provided as hidden modules to keep the presented software stack nice and clean. Hidden modules can be listed using <code>module --show-hidden avail</code>.</p>"},{"location":"general/faq.html#software-stacks","title":"software stacks","text":"<p>On UBELIX there are multiple software stacks. There are software stacks for each architecture. There are custom software stacks in Workspaces (again architectural software stacks included) and the VitalIT software stack.  The targeted software stack need to be available. The different architectural software stacks are available on the related architecture, e.g. epyc2 in a job on the epyc2 partion. The Workspace and VitalIT software stack can be loaded using <code>module load Workspace</code> or module load vital-it. </p>"},{"location":"general/faq.html#software-stack-inconstency","title":"software stack inconstency","text":"<p>It is strongly suggested to not mix different toolchains like foss or intel. Additionally, it is advised to stay with one version of a toolchain, e.g. foss/2021a and its dependency versions, e.g. GCC/10.3.0 etc.  Further, LMOD has a confusing effect when loading inconsitent module combinations, e.g.</p> <p><pre><code>$ module load foss/2021a\n$ module load intel/2020b\nLmod has detected the following error:  The following module(s) are unknown: \"zlib/.1.2.11-GCCcore-10.2.0\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date; it may help to try:\n  $ module --ignore-cache load \"zlib/.1.2.11-GCCcore-10.2.0\"\n\nAlso make sure that all modulefiles written in TCL start with the string #%Module\n</code></pre> The mentioned module <code>zlib/.1.2.11-GCCcore-10.2.0</code> is available in general.  When loading <code>foss/2021a</code>, the <code>zlib/.1.2.11-GCCcore-10.3.0</code> should get loaded, but LMOD will not swap its version, but report the mentioned error. </p> <p>Please take this as an indication that you accidentality mix different toolchains, and rethink your procedure, and stay within the same toolchain and toolchain version. </p>"},{"location":"general/faq.html#environment-issues","title":"Environment issues","text":""},{"location":"general/faq.html#i-modified-my-bashrc-but-its-not-doing-what-i-expect-how-can-i-debug-that-bash-script","title":"I modified my bashrc, but its not doing what I expect, how can I debug that bash script?","text":"<p>The bashrc can be debugged as all other bash scripts, using </p> <ul> <li><code>set -x</code> at the beginning of the script. This will print all commands executed on screen, including all subcommand also included in called scripts and tools</li> <li>print statements, e.g. <code>echo \"DEBUG: variable PATH=$PATH\"</code></li> </ul> <p>These should provide a good indication where the script diverge from your expectation. </p>"},{"location":"general/faq.html#job-issues","title":"Job issues","text":""},{"location":"general/faq.html#why-is-my-job-still-pending","title":"Why is my job still pending?","text":"<p>The REASON column of the squeue output gives you a hint why your job is not running.</p> <p>(Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled.</p> <p>(Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources.</p> <p>(Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option).</p> <p>(DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs.</p> <p>(QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish.</p> <p>(AssocGrpCpuLimit) dito.</p> <p>(AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish.</p> <p>(ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, <code>DOWN</code>, <code>DRAINED</code>, or not responding.Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of <code>scontrol show reservation</code>) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using <code>scontrol show reservation</code>.</p>"},{"location":"general/faq.html#why-can-i-not-submit-jobs-anymore","title":"Why can I not submit jobs anymore?","text":"<p>After joining an HPC Workspace the private SLURM account gets deactivated and a Workspace account need to be specified.  This can be done by loading the Workspace module, see Workspace environment:</p> <pre><code>module load Workspace\n</code></pre> <p>Otherwise Slurm will present the following error message: <pre><code>sbatch: error: AssocGrpSubmitJobsLimit\nsbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n</code></pre></p> <p>With this method we aim to distribute our resources in a more fair manner. HPC resources including compute power should be distriuted between registered research groups. We can only relate users with research groups by utilizing Workspace information. </p>"},{"location":"general/faq.html#why-cant-i-submit-further-jobs","title":"Why can\u2019t I submit further jobs?","text":"<p>sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits)</p> <p>\u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.</p>"},{"location":"general/faq.html#job-in-state-failed-although-job-completed-successfully","title":"Job in state FAILED although job completed successfully","text":"<p>Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero.</p> <p>The following simplified example illustrates the issue:</p> <p>simple.c</p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) {\n  char hostname[128];\n  gethostname(hostname, sizeof(hostname));\n  printf(\"%s says: Hello World.\\n\", hostname);\n}\n</code></pre> <p>job.sh</p> <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=foo@bar.unibe.ch\n#SBATCH --mail-type=END\n#SBATCH --job-name=\"Simple Hello World\"\n#SBATCH --time=00:05:00\n#SBATCH --nodes=1\n# Put your code below this line\n./simple\n</code></pre> <pre><code>bash$ sbatch job.sh\nSubmitted batch job 104\n</code></pre> <p>Although the job finished successfully\u2026</p> <p>slurm-104.out</p> <pre><code>knlnode02.ubelix.unibe.ch says: Hello World.\n</code></pre> <p>\u2026Slurm reports job FAILED:</p> <pre><code>bash$ sacct -j 104\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n104          Simple He+        all                     1     FAILED     45:0\n104.batch         batch                                1     FAILED     45:0\n</code></pre> <p>Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output.</p> <p>Solution: Explicitly return a value:</p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\nint main (int argc, char *argv[]) {\n  char hostname[128];\n  int n;\n  gethostname(hostname, sizeof(hostname));\n  // If successful, the total number of characters written is returned. On failure, a negative number is returned.\n  n = printf(\"%s says: Hello World.\\n\", hostname);\n  if (n &lt; 0)\n    return 1;\n  return 0;\n}\n</code></pre> <pre><code>bash$ sacct -j 105\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n105          Simple He+        all                     1  COMPLETED      0:0\n105.batch         batch                                1  COMPLETED      0:0\n</code></pre>"},{"location":"general/halloffame.html","title":"Hall of Fame","text":"<p>If you previously used UBELIX to do your computational work and you acknowledged this in your publication and want to your publication listed here, please drop us a note via https://serviceportal.unibe.ch/hpc. If you are wondering how you can acknowledge the usage of UBELIX in your publication, have a look at the homepage of this documentation, where you will find a text recommendation acknoowledging the use of our cluster.</p>"},{"location":"general/halloffame.html#papers-and-articles","title":"Papers and Articles","text":"Authors Title Journal Boris DOI 2023 C. Gontier et al. Efficient sampling-based Bayesian Active Learning for synaptic characterization PLOS Computational Biology Details Direct Link J. Sutter et al. Climate intervention on a high-emissions pathway could delay but not prevent West Antarctic Ice Sheet demise Nature climate change Nature Publishing Group Details Direct Link Dominik Moser et al. Within-network brain connectivity during a social optimism task is related to personal optimism and optimism for in-group members Human brain mapping, 44(12), S. 4561-4571 Details Direct Link Gunter Stober et al. Identifying gravity waves launched by the Hunga Tonga\u2013Hunga Ha\u2032apai volcanic eruption in mesosphere/lower-thermosphere winds derived from CONDOR and the Nordic Meteor Radar Cluster Annales geophysicae, 41(1), S. 197-208 Details Direct Link 2022 Raviteja Kotikalapudi et al. Brain structure and optimism bias: A voxel-based morphometry approach Brain Sciences, 12(3), S. 315 Details Direct Link Raviteja Kotikalapudi et al. Whole-brain white matter correlates of personality profiles predictive of subjective well-being Scientific reports, 12(1), S. 4558 Details Direct Link Gunter Stober et al. Meteor radar vertical wind observation biases and mathematical debiasing strategies including the 3DVAR+DIV algorithm Atmos. Meas. Tech., 15, 2022 Details Direct LInk 2021 Tatjana Aue et al. Enhanced sensitivity to optimistic cues is manifested in brain structure: A voxel-based morphometry study Social cognitive and affective neuroscience, 16(11), S. 1170-1181 Details Direct Link Dominik Moser et al. Reduced network integration in default mode and executive networks is associated with social and personal optimism biases Human brain mapping, 42(9), S. 2893-2906 Details Direct Link Deng S., Zhu J., Aschauer U. Critical Role of Sc Substitution in Modulating Ferroelectricity in Multiferroic LuFeO<sub>3</sub> Nano Lett. 2021,21 Direct Link Stober G., Weryk R. J. Triple-frequency meteor radar full wave scattering - Measurements and comparison to theory A&amp;A 654 A108 Direct Link Bertone S, J\u00e4ggi A Assessing reduced-dynamic parametrizations for GRAIL orbit determination and the recovery of independent lunar gravity field solutions Earth and Space Science Direct Link 2020 Dominik Moser et al. Social optimism biases are associated with cortical thickness Social cognitive and affective neuroscience, 15(7), S. 745-754 Details Direct Link Riou J, Hauser A et al. Estimation of SARS-CoV-2 mortality during the early stages of an epidemic: A modeling study in Hubei, China, and six regions in Europe PLOS Medicine Direct Link Ricca C, Aschauer U Local polarization in oxygen-deficient LaMnO<sub>3</sub> induced by charge localization in the Jahn-Teller distorted structure \u00a0Phys. Rev. Res. Details Direct Link Burns E, Lippert T et al. LaTiO<sub>2</sub>N crystallographic orientation control significantly increases visible-light induced charge extraction J. Mat. Chem. A Details Direct Link Ninova S, Aschauer U, et al. Suitability of Cu-substituted \u03b2-Mn<sub>2</sub>V<sub>2</sub>O<sub>7</sub> and Mn-substituted \u03b2-Cu<sub>2</sub>V<sub>2</sub>O<sub>7</sub> for photocatalytic water-splitting J. Chem Phys. 153 Details Direct Link Vonr\u00fcti N, Aschauer U Catalysis on oxidized ferroelectric surfaces\u2014Epitaxially strained LaTiO<sub>2</sub>N and BaTiO<sub>3</sub> for photocatalytic water splitting Chem. Mater. Details Direct Link Bouri M, Aschauer U Suitability of Different Sr<sub>2</sub>TaO<sub>3</sub>N Surface Orientations for Photocatalytic Water Oxidation Chem. Mater Details Direct Link Flores E, Berg E, et al. Cation Ordering and Redox Chemistry of Layered Ni-Rich Li<sub>x</sub>Ni<sub>1\u20132y</sub>Co<sub>y</sub>Mn<sub>y</sub>O<sub>2</sub>: An Operando Raman Spectroscopy Study Chem. Mater. Details Direct Link Pawlak R, Meyer E, et al. Bottom-up Synthesis of Nitrogen-Doped Porous Graphene Nanoribbons J.Am.Chem.Soc. Details Direct Link Ricca C, Aschauer U, et al. Self-consistent DFT + U + V study of oxygen vacancies in SrTiO<sub>3</sub> Phys. Rev. Research 2 Details Direct Link Ninova S, Aschauer U, et al. Surface Orientation and Structure of LaTiO<sub>2</sub>N Nanoparticles ACS Appl. Energy Mater Details Direct Link Primasov\u00e1 H, Furrer J, et al. Dinuclear thiolato-bridged arene ruthenium complexes: from reaction conditions and mechanism to synthesis of new complexes RSC Advances Details Direct Link Pfister J-P, Gontier C Identifiability of a Binomial Synapse Front. Comput. Neurosci. Details Direct Link Riou J, Althaus C Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020 Euro Surveillance Direct Link 2019 Vonr\u00fcti N, Aschauer U The role of metastability in enhancing water-oxidation activity Phys.Chem.Chem.Phys. Details Bizzotto F, Arenz M, et al. Examining the Structure Sensitivity of the Oxygen Evolution Reaction on Pt Single\u2010Crystal Electrodes: A Combined Experimental and Theoretical Study ChemPhysChem Details Direct Link Vonr\u00fcti N, Aschauer U Band-gap engineering in AB(O<sub>x</sub>S<sub>1\u2212x</sub>)<sub>3</sub> perovskite oxysulfides: a route to strongly polar materials for photocatalytic water splitting J.Mat. Chem. A Details Direct Link Ouhbi H, Aschauer U Nitrogen Loss and Oxygen Evolution Reaction Activity of Perovskite Oxynitrides ACS Materials Lett. Details Direct Link Hussain H, Thornton G, et al. Water-Induced Reversal of the TiO<sub>2</sub>(011)-(2 \u00d7 1) Surface Reconstruction: Observed with in Situ Surface X-ray Diffraction J.Phys. Chem C Details Direct Link Mantella V, Buonsanti R, et al. Synthesis and Size-Dependent Optical Properties of Intermediate Band Gap Cu<sub>3</sub>VS<sub>4</sub> Nanocrystals Chem. Mater Details Direct Link Aschauer U, Spaldin N, et al. Strain-induced heteronuclear charge disproportionation in EuMnO<sub>3</sub> Phys. Rev. Materials 3 Details Direct Link Ninova S, Aschauer U Anion-order driven polar interfaces at LaTiO<sub>2</sub>N surfaces Journal of Materials Chemistry A Details Direct Link Ricca C, Aschauer U et al. Self-consistent site-dependent DFT+U study of stoichiometric and defective SrMnO<sub>3</sub> Physical Review B Details Direct Link Ouhbia H, Aschauer U Water oxidation catalysis on reconstructed NaTaO<sub>3</sub> (001) surfaces Journal of Materials Chemistry A Details Direct Link Counotte M, Althaus C et al. Impact of age-specific immunity on the timing and burden of the next Zika virus outbreak PLOS NeglectedTropical Diseases Details Direct Link Brugger J, Althaus C Transmission of and susceptibility to seasonal influenza in Switzerland from 2003 to 2015 Epidemics, Elsevier Details Direct Link 2018 Horton P, Br\u00f6nnimann S Impact of global atmospheric reanalyses on statistical precipitation downscaling Climate Dynamics Details Direct Link Vonr\u00fcti N, Aschauer U Epitaxial strain dependence of band gaps in perovskite oxynitrides compared to perovskite oxides American Physical Society Details Direct Link Aschauer U <p>Ultrafast Relaxation Dynamics of the Antiferrodistortive Phase in Ca Doped SrTiO\u2083</p> American Physical Society Details Direct Link Vonr\u00fcti N, Aschauer U, et al. Elucidation of Li<sub>x</sub>Ni<sub>0.8</sub>Co<sub>0.15</sub>Al<sub>0.05</sub>O<sub>2</sub> Redox Chemistry by Operando Raman Spectroscopy American Chemical Society Details Direct Link Ouhbi H, Aschauer U Water oxidation chemistry of oxynitrides and oxides: Comparing NaTaO<sub>3</sub>\u00a0and SrTaO<sub>2</sub>N Surface Science Details Direct Link Aschauer U Surface and Defect Chemistry of Oxide Material CHIMIA Details Direct Link Kasper, C, Hebert, F, Aubin-Horth N, Taborsky B Divergent brain gene expression profiles between alternative behavioural helper types in a cooperative breeder Wiley Molecular Ecology Direct Link <p>Panyasantisuk J,Dall'Ara E,Pretterklieber M,Pahr D.H.,Zysset P.K.</p> <p>| Mapping anisotropy improves QCT-based finite element estimation of hip strength in pooled stance and side-fall load configurations |  <p>Medical Engineering &amp; Physics,Elsevier</p></p> <p>|  | Direct Link Vonr\u00fcti, N, Aschauer U | Anion Order and Spontaneous Polarization in LaTiO<sub>2</sub>N Oxynitride Thin Films | American Physical Society | Details | Direct Link Bouri M, Aschauer U | Bulk and surface properties of the Ruddlesden-Popper oxynitride Sr<sub>2</sub>TaO<sub>3</sub>N | Physical Chemistry Chemical Physics | Details | Direct Link 2017 Aschauer, U et al. | Surface Structure of TiO2 Rutile (011) Exposed to Liquid Water | Journal of Physical Chemistry | Details | Direct Link Kasper, C, K\u00f6lliker, M, Pstma, E, Taborsky B | Consistent cooperation in a cichlid fish is caused by maternal and developmental effects rather than heritable genetic variation | Proceedings of the Royal Society, Biological Sciences |  | Direct Link Riesen M, Garcia V, Low N, Althaus C | Modeling the consequences of regional heterogeneity in human papillomavirus (HPV) vaccination uptake on transmission in Switzerland | Vaccine, Elsevier | Details | Direct Link Kilic C, Raible C, Stocker T | Multiple climate States of Habitable Exoplanets: The Rolf of Obliquity and Irradiance | The Astrophysical Journal | Details | Direct Link Kilic C, Raible C, Kirk | Impact of variations of gravitational acceleration on the general circulation of the planetary atmosphere | Planetary and Space Science | Details | Direct Link Mueller S, Fix M et al. | Simultaneous optimization of photons and electrons for mixed beam radiotherapy al. | Physics in Medicine &amp; Biology |  | Direct Link Ninova S, Aschauer U | Surface structure and anion order of the oynitride LaTiO<sub>2</sub>N | Journal of Materials Chemistry A | Details | Direct Link Ninova S, Aschauer U et al. | <p>LaTiOxNy Thin Film Model Systems for Photocatalytic Water Splitting: Physicochemical Evolution of the Solid-Liquid Interface and the Role of the Crystallographic Orientation</p> | Advanced functional materials | Details | Direct Link Struchen R, Vial F, Andersson M. G. | Value of evidence from syndromic surveillance with cumulative evidence from multiple data stream with delayed reporting | Scientific Reports |  | Direct Link 2013 Leichtle A, Fiedler G et al. | Pancreatic carcinoma, pancreatitis, and healthy controls: metabolite models in a three-class diagnostic dilemma | Metabolomics, Springer | Details | Direct Link</p>"},{"location":"general/halloffame.html#posters","title":"Posters","text":""},{"location":"general/halloffame.html#newspapers","title":"Newspapers","text":"Title Newspaper Year of Publication Link Berner Forscher entdecken neue Klimazust\u00e4nde, in denen Leben m\u00f6glich ist. Der Bund Direct Link"},{"location":"general/halloffame.html#create-an-entry","title":"Create an Entry","text":"<p>If you used UBELIX for your publication please let your entry added to the list. Please open a ticket with the details of your publication.</p>"},{"location":"general/news.html","title":"News","text":"<p>12.01.2024:</p> <ul> <li>The user documentation has been streamlined and updated with recent information</li> <li> <p>The UBELIX9 testing system, previewing the next generation OS is now availble for all users.</p> <p>In our ongoing commitment to providing a secure and efficient computing environment, we are migrating the HPC system from CentOS 7 to Rocky Linux 9.  </p> <p>We are pleased to inform you that a part of our infrastructure has been migrated and is ready to be tested by you. To get you started, please take the time to read this information thoroughly. </p> <p>As part of the migration, we have implemented general software and security updates to ensure a secure and optimized computing environment. Please consult the manual pages (i.e., <code>man &lt;command&gt;</code>) to review the latest command syntax. </p> <p>The list of software modules managed by the UBELIX Team accessible via the module commands has been updated. Please note that old software versions may have been discontinued in favor of more recent versions. Additionally, the Vital-IT and UBELIX software stacks have been merged. Explore the enhanced range of modules to benefit from the latest tools and applications available on UBELIX using the module spider command. </p> <p>While we have taken measures to minimize user impact, it is crucial to be aware of potential adjustments needed on your end. Most importantly, please verify that your workflows, scripts, and applications are compatible with the new environment.  </p> <p>It is important to note that there may be a need to recompile your executables for compatibility with the new system. Existing Python environments are expected to remain functional unless special libraries such as TensorFlow with GPU support are used. These may require a fresh installation. </p> <p>Additionally, older software modules that are no longer managed by the UBELIX team may need to be installed by users if required. Instructions for custom software modules installations can be found in the documentation section on EasyBuild.  </p> <p>The testing system is kept simple, and therefore, only default Quality of Service (QOS) is available now. Investor resources have not been migrated yet and are still fully accessible on the old system. Existing job scripts that use the debug, long, gpu_preempt and invest QOS need to be updated. Investors are encouraged to reach out to us if they wish to proceed with the migration of their resources. </p> <p>To access the new system please login to submit02: <code>ssh &lt;username&gt;@submit02.unibe.ch</code></p> <p>Note that the graphical monitoring (https://ubelix.unibe.ch/)) does not cover the new testing environment yet. Please use the <code>squeue --me</code> command to query your jobs status on the new system. More details on the monitoring of the new system will follow. </p> <p>If you encounter any issues, we are ready to assist you. Feel free to reach out via https://serviceportal.unibe.ch/hpc. Please make sure to specify that your problem is related to the UBELIX testing environment and provide as much information as possible. </p> <p>We appreciate your attention to these details and your cooperation as we work together to ensure a smooth transition to Rocky Linux 9.</p> <p>Happy computing!</p> </li> </ul>"},{"location":"general/quick-start.html","title":"Quick Start","text":"<p>This section is intended as a brief introduction into HPC, especially to the present system UBELIX.  This page is an summary, a hands-on introduction, which targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online.  After reading this page you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to more in-depth information on the topic.</p>"},{"location":"general/quick-start.html#cluster-rules","title":"Cluster Rules","text":"<p>Before we start: as everywhere where people come together, a common sense is needed to allow for a good cooperation and to enable a positive HPC experience. Be always aware that you are working on a shared system where your behaviour could have a negative impact on the workflow of other users. Please find the list of the most important guidelines in our code of conduct.</p>"},{"location":"general/quick-start.html#request-an-account","title":"Request an Account","text":"<p>Before you can start working on the HPCs, staff and students of the University of Bern must have their Campus Account (CA) registered for the HPCs. External researchers that collaborate with an institute of the University of Bern must apply for a CA through that institute. See Accounts and Activation for more information getting access to UBELIX.</p>"},{"location":"general/quick-start.html#hpc-workspace","title":"HPC Workspace","text":"<p>Workspaces provide are a collaborative environment, including group based access to permanent and temporary storage, as well as group based compute resource accounting. Research group leaders need to apply for an workspace, see Workspace Management.  For an introduction to HPC Workspaces see Workspace Overview</p>"},{"location":"general/quick-start.html#login","title":"Login","text":"<p>To connect to the cluster, you must log in to a login node from inside the university network (e.g. from a workstation on the campus). If you want to connect from a remote location (e.g. from your computer at home) you must first establish a VPN connection to get access to the university network. To connect from a UNIX-like system (Linux, Mac OS X, MobaXterm on Windows) simply use a secure shell (SSH) to log in to a login node. There are four login nodes (submit[01-04].unibe.ch), you can pick any one:</p> <pre><code># here we choose submit03.unibe.ch as our login node\nssh &lt;user&gt;@submit03.unibe.ch\n</code></pre>"},{"location":"general/quick-start.html#welcome-home","title":"Welcome <code>$HOME</code>","text":"<p>After successful login to the cluster, your will find yourself in the directory <code>/storage/homefs/$USER</code>, where <code>$USER</code> is your Campus Account username. This is your home directory and serves as the repository for your personal files, and configurations.  You can reference your home directory by <code>~</code> or <code>$HOME</code>. </p> <p>Your home directory is located on a shared file system. Therefore, all files and directories are always available on all cluster nodes and must hence not be copied between those nodes. HOME directories have a daily snapshot and backup procedures. Disk space is managed by quotas. By default, each user has 1TB of disk space available. Keep your home directory clean by regularly deleting old data or by moving data to a private storage.</p> <p>You can always print the current working directory using the <code>pwd</code> (present working directory) command: <pre><code>pwd\n/storage/homefs/&lt;user&gt;\n</code></pre></p>"},{"location":"general/quick-start.html#copy-data","title":"Copy Data","text":"<p>At some point, you will probably need to copy files between your local computer and the cluster. There are different ways to achieve this, depending on your local operating system (OS). To copy a file from your local computer running a UNIX-like OS use the secure copy command <code>scp</code> on your local workstation:</p> <pre><code>scp /path/to/file &lt;user&gt;@submit03.unibe.ch:/path/to/target_dir/\n</code></pre> <p>To copy a file from the cluster to your local computer running a UNIX-like OS also use the secure copy command <code>scp</code> on your local workstation:</p> <pre><code>scp &lt;user&gt;@submit03.unibe.ch:/path/to/file /path/to/target_dir/\n</code></pre> <p>More information about file transfer can be found on the page File Transfer to/from UBELIX.</p>"},{"location":"general/quick-start.html#use-software","title":"Use Software","text":"<p>On our HPCs you can make use of already pre-installed software or you can compile and install your own software. We use a module system to manage software packages, even different versions of the same software. This allows you to focus on getting your work done instead of compiling software. E.g. to get a list of all provided packages:</p> <pre><code>module avail\n</code></pre> <p>A package name can be added to list all packages containing that string. </p> <p>The <code>module spider</code> command encountering also results from the VitalIT software stack. </p> <p>Workspace software stacks</p> <p><code>module spider</code> or <code>module avail</code> will only find packages in a Workspace software stack if the <code>Workspace</code> module for that workspace is loaded</p> <p>Furthermore, we are suggesting to work with so called toolchains. These are collections of modules build on top of each other.  E.g. setting the environment for compiling an scientific application with math. libraries, OpenMPI and GCC, load:</p> <pre><code>$ module load foss\n$ module list \nmodule list\n\nCurrently Loaded Modules:\n  1) GCCcore/9.3.0                      4) GCC/9.3.0                          7) libxml2/.2.9.10-GCCcore-9.3.0    (H)  10) UCX/1.8.0-GCCcore-9.3.0   13) gompi/2020a                  16) foss/2020a\n  2) zlib/.1.2.11-GCCcore-9.3.0   (H)   5) numactl/2.0.13-GCCcore-9.3.0       8) libpciaccess/.0.16-GCCcore-9.3.0 (H)  11) OpenMPI/4.0.3-GCC-9.3.0   14) FFTW/3.3.8-gompi-2020a\n  3) binutils/.2.34-GCCcore-9.3.0 (H)   6) XZ/.5.2.5-GCCcore-9.3.0      (H)   9) hwloc/2.2.0-GCCcore-9.3.0             12) OpenBLAS/0.3.9-GCC-9.3.0  15) ScaLAPACK/2.1.0-gompi-2020a\n\n  Where:\n   H:  Hidden Module\n</code></pre> <p>You can also specify version numbers there. </p> <p>Scope</p> <p>The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script.</p> <p>But also keep in mind that the current environment will get forwarded into a job submitted from it. This may lead to conflicting versions of loaded modules and modules loaded in the script. </p> <p>With the module environment you can also easily install, maintain and provide software packages in your workspaces and share with your collaborators. </p> <p>The Software section is dedicated to this topic. More information can be found there.</p> <p>Managing different working environments can be done with \u201cMeta Modules\u201d or user collections, see Environment Definitions</p>"},{"location":"general/quick-start.html#hello-world","title":"Hello World","text":"<p>Doing useful computations consists of running commands that work on data and generate a result. These computations are resource-intensive. That is what the compute nodes are there for. These over 300 servers do the heavy lifting as soon as resources are free for you. Currently you are on a submit server also known as login node. This server is for preparing the computations, i.e. downloading data, writing a job script, prepare some data etc. But you are not allowed to run computations on login nodes as those servers are quite a weak machines that you are sharing with others. So, you have to bring the computations to the compute nodes - by generating a job script and sending it to the cluster.</p> <p>Working interactively on a compute node</p> <p>When developing stuff it\u2019s often useful to have short iterations of try-error. Therefore it\u2019s also possible to work interactively on a compute node for a certain amount of time without having to send jobs to the cluster and wait until they finish just to see it didn\u2019t work. See Interactive Jobs for more information about this topic.</p> <p>It\u2019s now time for your first job script. To do some work on the cluster, you require certain resources (e.g. CPUs and memory) and a description of the computations to be done. A job consists of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1GB\n\n# Put your code below this line\n...\n</code></pre> <p>The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the <code>sbatch</code> command. The page Jobs Submission outlines the most important options of <code>sbatch</code>.</p> <p>Now, let\u2019s write a simple \u201chello, world\u201d-task:</p> <pre><code>...\n# Put your code below this line\nmodule load Workspace_Home\nmkdir -p $SCRATCH/my_first_job\ncd $SCRATCH/my_first_job\necho \"Hello, UBELIX from node $(hostname)\" &gt; hello.txt\n</code></pre> <p>After loading the Workspace module, we create a new directory \u2018my_first_job\u2019 within our \u201cpersonal\u201d SCRATCH directory. The variable <code>$SCRATCH</code> expands to <code>/storage/scratch/users/&lt;user&gt;</code>. Then, we change directory to the newly created directory. In the third line we print the line <code>Hello, UBELIX from node &lt;hostname_of_the_executing_node&gt;</code> and redirect the output to a file named <code>hello.txt</code>. The expression <code>$(hostname)</code> means, run the command hostname and put its output here. Save the content to a file named <code>first.sh</code>.</p> <p>The complete job script looks like this:</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=1GB\n\n# Put your code below this line\nmodule load Workspace_Home\nmkdir -p $SCRATCH/my_first_job\ncd $SCRATCH/my_first_job\necho \"Hello, UBELIX from node $(hostname)\" &gt; hello.txt\n</code></pre>"},{"location":"general/quick-start.html#schedule-your-job","title":"Schedule Your Job","text":"<p>We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the <code>sbatch</code> command:</p> <p><pre><code>sbatch first.sh\n</code></pre> <pre><code>Submitted batch job 32490640\n</code></pre> If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on. There are various options for different types of jobs provided in the scheduler. See sections Array Jobs, GPUs, and Interactive Jobs for more information</p>"},{"location":"general/quick-start.html#monitor-your-job","title":"Monitor Your Job","text":"<p>You can inspect the state of our active jobs (running or pending) with the squeue command:</p> <p><pre><code>squeue --job=32490640\n</code></pre> <pre><code>      JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n   32490640     epyc2    job01 testuser  R       0:22      1 bnode23\n</code></pre></p> <p>Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018epyc2\u2019 partition (default partition) on bnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to SLURM because the requested resources are not yet available. In this case, the output could look like this:</p> <p><pre><code>squeue --job=32490640\n</code></pre> <pre><code>       JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n    32490640     epyc2    job01 testuser PD       0:00      1 (Priority)\n</code></pre></p> <p>Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one other job with higher priority. See here for a list of other reasons why a job might be pending.</p> <p>You can always list all your active (pending or running) jobs with squeue:</p> <p><pre><code>squeue --user=testuser\n</code></pre> <pre><code>      JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n   34651451     epyc2 slurm.sh  testuser PD       0:00      2 (Priority)\n   34651453     epyc2 slurm.sh  testuser PD       0:00      2 (Priority)\n   29143227     epyc2     Rjob  testuser PD       0:00      4 (JobHeldUser)\n   37856328       bdw   mpi.sh  testuser  R       4:38      2 anode[012-014]\n   32634559       bdw  fast.sh  testuser  R    2:52:37      1 anode12\n   32634558       bdw  fast.sh  testuser  R    3:00:54      1 anode14\n   32634554       bdw  fast.sh  testuser  R    4:11:26      1 anode08\n   32633556       bdw  fast.sh  testuser  R    4:36:10      1 anode08\n</code></pre></p> <p>Further information on on job monitoring you find on page Monitoring Jobs. Furthermore, in the Job handling section you find additional information about Investigating a Job Failure and Check-pointing. </p>"},{"location":"general/quick-start.html#training-courses","title":"Training Courses","text":"<p>Data Science Lap (DSL)) regularly conducts introductory and advanced courses on Linux, UBELIX and other topics. Details outlined on their pages</p>"},{"location":"general/support.html","title":"Support","text":""},{"location":"general/support.html#mailing-list","title":"Mailing List","text":"<p>The official channel for getting information about upcoming UBELIX events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster:</p> <p>https://listserv.unibe.ch/mailman/listinfo/hpc-users </p>"},{"location":"general/support.html#unibe-hpc-support","title":"UniBE HPC support","text":"<p>In case of questions, comments or issues get in touch with us.  Please first collect all necessary details. In case of issues, this should include:</p> <ul> <li>JobID</li> <li>location of job scripts</li> <li>location of output/error files</li> <li>the full error message(s)</li> <li>your user name</li> <li>detailed description of expectation and observation</li> </ul> <p>Please open a ticket via https://serviceportal.unibe.ch/hpc</p>"},{"location":"general/ubelix-overview.html","title":"UBELIX - Overview","text":""},{"location":"general/ubelix-overview.html#description","title":"Description","text":"<p>This page provides a high-level system overview of an HPC cluster such as UBELIX. It describes the different hardware components that constitute the cluster and gives a quantitative list of the different generations of compute nodes in UBELIX.</p> <p>UBELIX (University of Bern Linux Cluster) is an HPC cluster that currently consists of about 320 compute nodes featuring ~12k CPU cores and 160 GPUs and a software-defined storage infrastructure providing <code>~3.5</code> PB of disk storage net. UBELIX is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. The front-end servers also provide a link to the outside world. UBELIX is used by various institutes and research groups within chemistry, biology, physics, astronomy, computer science, geography, medical radiology and others for scientific research and by students working on their thesis.</p>"},{"location":"general/ubelix-overview.html#high-level-system-overview","title":"High-level system overview","text":"<p> The HPCs can only be reached within the UniBE network. User landing point are the login nodes, where jobs can be prepared and submitted. Computational tasks are scheduled and managed on the compute nodes using SLURM, the workload manager. All compute nodes as well as the login nodes have access to the parallel file system.</p>"},{"location":"general/ubelix-overview.html#login-nodes-aka-submit-nodes","title":"Login nodes aka. Submit nodes","text":"<p>A user connects to the cluster by logging into one of the submit hosts with his Campus Account\u2019s username and password via SSH. You can use these hosts for medium-performance tasks, e.g. to edit files or to compile smaller programs. Resource-demanding/high-performance tasks must be submitted to the batch queuing system as jobs, and will finally run on one or multiple compute nodes. Also long running compile tasks should be submitted as a job on a compute node instead of running it on the submit hosts.</p>"},{"location":"general/ubelix-overview.html#slurm-batch-queueing-system","title":"SLURM Batch-Queueing System","text":"<p>On UBELIX we use the open-source batch-queueing system Slurm, managing all jobs on the compute nodes. The job submission is described in detail in the Job handling section, starting with Submitting jobs. </p> <p>The procedure look like:</p> <ul> <li>resource definition: resources required for your job need to be defined, including numbers of CPU cores, time limit, memory, partitions, QOS, etc.. These resources can be defined in the batch script or as command line arguments. Not explicitly specified parameters are chosen with default values.</li> <li>submitting: job can be submitted using <code>sbatch</code> (using a batch script), <code>srun</code> (directly running the executable), or <code>salloc</code> (interactive submission). The submission is checked from SLURM if it is within the specification and limits. </li> <li>scheduling: Slurm is finding the optimal spots for the registered jobs on the resources and time. This also includes priority handling and optimizing for best coverage. </li> <li>launch: Slurm prepares the environment on the selected compute resources. This also includes setting up the MPI environment, if requested interactive sessions, etc., and launching your batch script. </li> <li>serial/parallel tasks: per default all the tasks defined in your batch script are run on the first core of your allocation. Compute tasks should be started with \u2018srun\u2019. Parallel task are launched on all (or as defined) job related resources. </li> <li>cancelling/completing: When tasks finished, wall time limit or memory limit is reached the job ant its environment gets removed from the resources. All output is written into file(s) (except of interactive sessions)</li> </ul>"},{"location":"general/ubelix-overview.html#cluster-partitions-queues-and-their-compute-nodes","title":"Cluster Partitions (Queues) and their Compute Nodes","text":"<p>UBELIX is a heterogeneous machine, consisting of different architectures. There are CPU compute nodes with: </p> <ul> <li>Intel Broadwell (20 cores per node; usage of full or multiple nodes)</li> <li>AMD Epyc2 (128 cores per node)</li> </ul> <p>and GPU nodes with:</p> <ul> <li>Nvidia Geforce GTX 1080 Ti </li> <li>Nvidia Geforce RTX 2080 Ti</li> <li>Nvidia Geforce RTX 3090 </li> <li>Nvidia Tesla P100</li> </ul> <p>Without explicit changes, jobs are scheduled in the AMD Epyc2 partition, running up to 3 days. </p> <p>Partitions group nodes into logical sets, which share the same limits. Furthermore, specific limits and privileges are managed using Quality Of Service (QOS), like high priorities or node limitations for long running jobs.</p> <p>Different partitions and QOS are listed in the SLURM Partition/QOS article.</p>"},{"location":"general/ubelix-overview.html#storage-infrastructure","title":"Storage Infrastructure","text":"<p>A modular, software-defined storage system (IBM Spectrum Scale) provides a shared, parallel file system that is mounted on all frontend servers and compute nodes.  For more information see storage infrastructure and File System Quota.</p>"},{"location":"getting-Started/account.html","title":"Account and Activation","text":""},{"location":"getting-Started/account.html#description","title":"Description","text":"<p>UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern.  The cluster is meant to be used for research related to the University of Bern.  Before you can use this service your CA need to be activated for UBELIX.  Please respect the Code of Conduct On this page you will find useful information regarding the activation of your CA and the login procedure.  Furthermore, the provided service structure is outlined.</p>"},{"location":"getting-Started/account.html#account-activation","title":"Account activation","text":"<p>Request for activation</p> <p>To request the activation of your Campus Account, please send a request via https://serviceportal.unibe.ch/hpc including:</p> <ul> <li>a brief description of what you want to use the cluster for</li> <li>your Campus Account username</li> </ul> <p>Students must additionally provide:</p> <ul> <li>the name of the institute (e.g. Mathematical Institute)</li> <li>if available, the name of the research group (e.g. Numerical Analysis)</li> </ul> <p>If you possess multiple Campus Accounts (staff and student) use your staff account since this one is more specific. As soon as we get your email we will activate your account for UBELIX. Once activated, you will receive a confirmation email containing initial instructions</p> <p>You cannot choose a new username for UBELIX. The username/password combination will be the same as for your Campus Account that you also use to access other services provided by the University of Bern (e.g: email, Ilias).</p>"},{"location":"getting-Started/account.html#apply-for-a-campus-account-for-external-coworkers","title":"Apply for a Campus Account for external coworkers","text":"<p>If you do not have a Campus Account of the University of Bern, but you need access to the cluster for your cooperative scientific research with an UniBE institute, the account manager of the institute has to request a Campus Account from the IT department of the University of Bern. Please ask your coworker at the institute to arrange this for you. The responsible account manager at your institute can be found from the following link: Account managers</p>"},{"location":"getting-Started/account.html#mailing-list","title":"Mailing List","text":"<p>The official channel for informing the UBELIX community about upcoming events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster:</p> <p>https://listserv.unibe.ch/mailman/listinfo/hpc-users </p>"},{"location":"getting-Started/login-ssh.html","title":"Login","text":""},{"location":"getting-Started/login-ssh.html#description","title":"Description","text":"<p>UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern.  Before you can use this service we have to activate your CA for UBELIX, see Accounts and Activation.  This page contains information on how to configure your SSH environment for a simplified login procedure and information regarding the application of a CA for external researchers.</p>"},{"location":"getting-Started/login-ssh.html#log-in-to-ubelix","title":"Log in to UBELIX","text":"<p>Before proceeding make sure that:</p> <ul> <li>you have your Campus Account activated for UBELIX (see above)</li> <li>you have a working SSH client<ul> <li>Linux/Mac: e.g ssh command in a terminal </li> <li>Microsoft Windows: MobaXterm or Windows Subsytem Linux. Alternatively, a flavor of Linux can be installed on Microsoft Windows using virtualization software (e.g VirtualBox). We strongly encourage you to familiarize with a Unix-based Terminal commands. </li> </ul> </li> </ul> <p>Requirement</p> <p>Login to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial.</p> <p>Login nodes</p> <p>There are four login nodes in UBELIX:</p> <ul> <li>submit01.unibe.ch</li> <li>submit02.unibe.ch</li> <li>submit03.unibe.ch</li> <li>submit04.unibe.ch</li> </ul> <p>To access UBELIX, you can choose any one. If the load on a login node is high, you can log out and pick another one. When using a terminal multiplexer like tmux or screen, you can directly log in to the login node where your tmux/screen session is running.</p>"},{"location":"getting-Started/login-ssh.html#maclinuxunix","title":"Mac/Linux/Unix","text":"<p>Run the following commands in a terminal. Open an SSH connection to :</p> <p><pre><code>$ ssh &lt;user&gt;@submit03.unibe.ch\nOR\n$ ssh -l &lt;user&gt; submit03.unibe.ch\n</code></pre> At the password prompt enter your Campus Account password:</p> <pre><code>$ ssh &lt;user&gt;@submit03.unibe.ch\nPassword:\n</code></pre> <p>Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019.</p> <p>After log in successfully you will see the welcome message and the command prompt:</p> <pre><code>Last login: Mon Aug 15 10:22:09 2022 from 130.92.8.162\n\nCentOS 7.9.2009.x86_64\n\nFQDN:      submit03.ubelix.unibe.ch (10.1.129.23)\nProcessor: 128x AMD EPYC 7742 64-Core Processor\nKernel:    3.10.0-1160.62.1.el7.x86_64\nMemory:    125.67 GiB\n\n[user@submit03 ~]$\n</code></pre> <p>Customize your SSH session</p> <p>Useful feartures like SSH alias, X and port forwarding are described on our page SSH customization. </p>"},{"location":"getting-Started/login-ssh.html#mobaxterm-at-microsoft-windows","title":"MobaXterm at Microsoft Windows","text":"<p>Here we present the configuration and first steps using MobaXterm. This tool combines Terminal sessions with file transfer (scp/ftp) and X Window Server. There are many more features which are not described here. For a productive work environment you should get familiar with the tools, configuration and features. </p> <p>MobaXterm can be downloaded on the MobaXterm Website. There are two versions, portable and installation, you can choose one.</p> <p>After installing and starting MobaXterm, a SSH session need to be configured:</p> <ul> <li>Click \u2018Session\u2019 in the top left corner:  </li> <li>In \u201cSSH\u201d tab:<ul> <li>Set the remote host to a login node, e.g. submit01.unibe.ch</li> <li>Enable the \u201cSpecify username\u201d option and put your Campus Account short name in the corresponding box (here user ms20e149 will be used)</li> </ul> </li> <li> <p>In the \u201cAdvanced SSH settings\u201d</p> <ul> <li>Set SSH-browser type to \u2018SCP (enhanced speed)\u2019</li> <li>Optionally, tick the \u2018Follow SSH path\u2019 button </li> </ul> </li> <li> <p>From now one the settings are stored and you can access the session on the left at the star icon </p> </li> <li> <p>MobaXterm will ask you to store the Password and manage a MasterPassword. </p> </li> </ul> <p>After starting the session, you should see the UBELIX login message and prompt.   On the left hand side a File browser is located. There the UBELIX file system can be browsed and files up or downloaded, e.g. using drag and drop or the context menue.  </p>"},{"location":"getting-Started/ssh-customization.html","title":"Customize your SSH environment","text":""},{"location":"getting-Started/ssh-customization.html#description","title":"Description","text":"<p>This page is listing useful tricks and features with SSH connections.</p>"},{"location":"getting-Started/ssh-customization.html#create-a-ssh-alias","title":"Create a SSH alias","text":"<p>Mac/Linux/Unix</p> <p>To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config on your local desktop/laptop (substitute your own alias and username):</p> <p>~/.ssh/config <pre><code>Host &lt;alias&gt;\n    Hostname submit03.unibe.ch\n    User &lt;user&gt;\n</code></pre></p> <p>From now on you can log in to the cluster by using the specified alias:</p> <pre><code>$ ssh &lt;alias&gt;\n</code></pre> <p>You still have to provide your password!</p>"},{"location":"getting-Started/ssh-customization.html#ssh-session-timeout","title":"SSH session timeout","text":"<p>Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file:</p> <pre><code>ServerAliveInterval 60\n</code></pre> <p>The host declaration may now look like this:</p> <p>~/.ssh/config <pre><code>Host &lt;alias&gt;\n    Hostname submit03.unibe.ch\n    User &lt;user&gt;\n    ServerAliveInterval 60\n</code></pre></p>"},{"location":"getting-Started/ssh-customization.html#ssh-key-pairs","title":"SSH key pairs","text":"<p>Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Here SSH keys are created on your local desktop/laptop which are later used to authenticate during a SSH login into UBELIX.  Therefore, the following steps are required:</p> <ul> <li>Creation of key pair consisting of a private and a public key</li> <li>Adding a public key to your UBELIX account</li> <li>Adding the key to the SSH config</li> </ul>"},{"location":"getting-Started/ssh-customization.html#creation-of-key-pair","title":"Creation of key pair","text":"<p>Remember to always keep your private keys private! Share only public keys, never share your private key.</p> <p>If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. </p> <p>First, generate a private/public key pair. You can substitute your own comment (-C).  To accept the default name/location simply press Enter, otherwise specify a different name/location:</p> <pre><code>$ ssh-keygen -t rsa -b 4096 -C \"ubelix\"\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/&lt;user&gt;/.ssh/id_rsa):\n</code></pre> <p>Enter and confirm a secure passphrase:</p> <p>If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key!</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre>"},{"location":"getting-Started/ssh-customization.html#adding-a-public-key-to-your-ubelix-account","title":"Adding a public key to your UBELIX account","text":"<p>Now, the public key need to be added to the <code>~/.ssh/authorized_keys</code> file in your UBELIX account. This step can be done by simply issuing:</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub `&lt;alias&gt;`\n</code></pre>"},{"location":"getting-Started/ssh-customization.html#adding-the-key-to-the-ssh-config","title":"Adding the key to the SSH config","text":"<p>Add the key to your host declaration in your local ssh configuration:</p> <p>~/.ssh/config <pre><code>Host &lt;alias&gt;\n    Hostname submit03.unibe.ch\n    User &lt;user&gt;\n    ServerAliveInterval 60\n    IdentityFile ~/.ssh/id_rsa_ubelix\n</code></pre></p> <p>If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time.</p>"},{"location":"getting-Started/ssh-customization.html#adding-your-key-to-ssh-agent","title":"Adding your Key to SSH-Agent","text":"<p>The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in.</p> <p>Add the key to ssh-agent:</p> <pre><code>$ ssh-add ~/.ssh/id_rsa_ubelix\n</code></pre>"},{"location":"getting-Started/ssh-customization.html#x11-forwarding","title":"X11 - forwarding","text":"<p>For applications with graphical interfaces X11-forwarding is sometimes necessary. You can enable X11-forwarding by using <code>-Y</code> option during your login process: <pre><code>ssh -Y &lt;alias&gt;\n</code></pre> The success can be tested e.g. by calling <code>xterm</code> on the login node, which should open a new window.  Keep in mind your local operating system need to have a X server running. E.g. Xming on Windows or XQuartz for Mac.</p>"},{"location":"getting-Started/ssh-customization.html#passwordless-ssh-within-the-hpcs","title":"Passwordless SSH within the HPCs","text":"<p>Some application require passwordless SSH within the HPC machine, e.g. for establishing reverse port forwarding.  Please verify that you created and registered a SSH key within UBLEIX. If you can perform the following command without entering your password your are ready to go: <pre><code>ssh localhost\n</code></pre> otherwise create and register a new key on a login node. <pre><code>ssh-keygen -t rsa -b 4096\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 640 .ssh/authorized_keys\n</code></pre></p>"},{"location":"getting-Started/ssh-customization.html#port-forwarding","title":"Port forwarding","text":"<p>Some application like JupyterLab require port forwarding, where a port on the remote machine gets connected with a port on the local machine.  The ssh command need to be called with additional arguments:</p> <pre><code>ssh -Y -L 15051:localhost:15051 submit03.unibe.ch\n</code></pre> <p>Here port 15051 is selected for both sides. Ports are numbers between 2000 and 65000, which needs to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this port on the machine at a time. To avoid the need for modifying your workflow again and again, we suggest to (once) select a unique number (between 2000 and 65000), which hopfully and most likely will not be used by another user. </p>"},{"location":"hpc-workspaces/environment.html","title":"HPC Workspace Data and Software Tools","text":""},{"location":"hpc-workspaces/environment.html#description","title":"Description","text":"<p>HPC Workspace modules provides support for user-friendly file system access, custom software stacks in HPC Workspaces, and SLURM accounting.  The module can also be used to set up HOME for a custom software stack. </p> <p>Furthermore, there are modules provided to load a working Workspace and additionally a software stacks from another Workspace, see Additional software stacks</p>"},{"location":"hpc-workspaces/environment.html#workspace-module","title":"Workspace module","text":"<p>The Workspace module adjust the environment to work in a specific HPC Workspace.  <pre><code>module load Workspace\n</code></pre> sets the following environment variables (Shortcuts) and Software stacks</p> <p>There are the following possibilities:</p> <ul> <li>you belong to no Workspace: load <code>module load Workspace_Home</code> to use your software stack in your HOME directory and set <code>$WORKSPACE</code> and <code>$SCRATCH</code> variables to your private directories.</li> <li>you belong to one Workspace: this Workspace gets loaded when <code>module load Workspace</code></li> <li>you belong to multiple Workspaces: you need to specify the Workspace to load using the variable <code>$HPC_WORKSPACE</code>. If not specified, the module presents the possible options, e.g.:     <pre><code>$ module load Workspace\nWorkspaces are available:\n    HPC_SW_test, hpc_training, \nPlease select and load ONE of the following:\n    HPC_WORKSPACE=HPC_SW_test module load Workspace\n    HPC_WORKSPACE=hpc_training module load Workspace\n</code></pre><ul> <li>Thus you load a specific Workspace using: <pre><code>HPC_WORKSPACE=&lt;WorkspaceName&gt; module load Workspace\n</code></pre> There are also ways to load an additional Workspace for an additional software stack, see Additional Software Stacks below.</li> </ul> </li> </ul>"},{"location":"hpc-workspaces/environment.html#shortcuts","title":"Shortcuts","text":"<p>The workspace module provides the following variables:</p> Variable Function <code>$WORKSPACE</code> full path to the Workspace. Thus, you can access the workspace using: <code>cd $WORKSPACE</code> <code>$SCRATCH</code> full path to the Workspace SCRATCH directory. Thus you can access it using: <code>cd $SCRATCH</code>"},{"location":"hpc-workspaces/environment.html#slurm-account","title":"SLURM Account","text":"<p>by loading the <code>Workspace</code> module by default the Workspace account is selected. Thus all jobs submitted without a specified <code>--account</code> will be submitted to this Workspace account.</p>"},{"location":"hpc-workspaces/environment.html#additional-settings","title":"Additional Settings","text":"<p>the module provides the following settings for a more user-friendly usage of applications. You may not need to use them directly, but tools like SLURM, Singularity, Python, and R will use them. </p> Variable Function <code>$SBATCH_ACCOUNT</code> <code>$SLURM_ACCOUNT</code> <code>$SALLOC_ACCOUNT</code> sets the SLURM account to the Workspace account. Thus all submitted jobs with that module are accounted to the Workspace account automatically. No need to set it in the sbatch script <code>$SINGULARITY_BINDPATH</code> using singularity, the Workspace directory will be bind into the container without manual specification. The <code>WORKSPACE</code> variable as well as the <code>SCRATCH</code> variable will also be ported into the container. Thus, you can specify locations with <code>$WORKSPACE</code> or <code>$SCRATCH</code> within the container. <code>$PYTHONPATH</code> if <code>Python</code> or <code>Anaconda</code> is loaded beforehand, it is set to: <code>$WORKSPACE/PyPackages/lib/pythonXXX/site-packages</code> where <code>XXX</code> is the Python major and minor version. And also add the <code>bin</code> directory to <code>$PATH</code>. <code>$PYTHONPACKAGEPATH</code> if <code>Python</code> or <code>Anaconda</code> is loaded beforehand, it is set to: <code>$WORKSPACE/PyPackages</code>. This can be used for e.g. <code>pip install --prefix $PYTHONPACKAGEPATH</code> <code>$CONDA_ENVS_PATH</code> therewith conda environments can be created and shared within the Workspace <code>$R_LIBS</code> therewith additional R packages can be installed and searched in the shared Workspace. The directory need to be created first. See R page"},{"location":"hpc-workspaces/environment.html#software-stacks","title":"Software stacks","text":"<p>Beside, a set of software packages we provide for our different CPU architecture, the Workspace module provides tools to install custom software stacks within your Workspace.  Especially with EasyBuild shortcuts are provided to install and use custom software stacks easily build for all architectures. </p> <p>For installing packages with EasyBuild, see EasyBuild description.  Manual package can also be installed in the similar manner. Adding a Modulefile provides the users to load packages as used to. Please see Installing Custom Software. </p> <p>As a result all users of the Workspace can use the software packages by loading the Workspace module and the software product module. </p>"},{"location":"hpc-workspaces/environment.html#python-packages","title":"Python packages","text":"<p>The Workspace module provides support to install and use Python packages in a shared manner, by installing them into the Workspace. </p> <p>First load Python/Anaconda3</p> <p>Python or Anaconda3 module need to be loaded before loading the Workspace module, since variables to be set depend on Python version. Workspace module can also be reloaded, e.g.: <pre><code>HPC_WORKSPACE=HPC_SW_test module load Workspace\nmodule load Anaconda3\nmodule load Workspace\n</code></pre></p>"},{"location":"hpc-workspaces/environment.html#conda-environments","title":"Conda environments","text":"<p>The Workspace module provides support for creating and using conda environments in the shared Workspace. See Anaconda Conda environments.</p>"},{"location":"hpc-workspaces/environment.html#r-packages","title":"R packages","text":"<p>The Workspace module provides support for installing and using additional R packages in the shared Workspace. Thus a package once installed by one user can be used by all Workspace members. See R installing packages.</p>"},{"location":"hpc-workspaces/environment.html#umask","title":"UMASK","text":"<p>The Workspace module sets the umask to 002. Thus files and directories get group-writeable, e.g.: <pre><code>-rw-rw-r-- 1 user group 0 Mar 15 15:15 /path/to/file\n</code></pre></p>"},{"location":"hpc-workspaces/environment.html#additional-software-stacks","title":"Additional Software Stacks","text":"<p>The module <code>Workspace_SW_only</code> provide the access to a software stack of an HPC Workspace <code>B</code> while working in an HPC Workspace <code>A</code>. </p> <p>This could be that a common software stack is provided for multiple (data) Workspaces or that you are trying software packages in your <code>HOME</code></p> <p>As an example you could load:</p> <pre><code>HPC_WORKSPACE=A module load Workspace\nHPC_WORKSPACE=B module load Workspace_SW_only\n</code></pre> <p>When you want to load packages from your <code>HOME</code> while working in <code>A</code>, you can </p> <pre><code>HPC_WORKSPACE=$HOME module load Workspace_SW_only\nHPC_WORKSPACE=A module load Workspace\n</code></pre> <p>Note that the variable <code>HPC_WORKSPACE</code> is cleared after each loading of a <code>Workspace*</code>module. The currently loaded Workspace names are stored in <code>$HPC_WORKSPACE_LOADED</code> for the Workspace module and <code>$HPC_WORKSPACE_SW_ONLY</code> for the Workspace_SW_only module. </p>"},{"location":"hpc-workspaces/environment.html#reloading","title":"Reloading","text":"<p>The Workspace modules are configured to \u201cremember\u201d the selected Workspace you once loaded in the current session, even after unload the Workspace module. Therefore, environment variables are set in the current session, <code>$HPC_WORKSPACE</code> for the <code>Workspace</code> module, and <code>$HPC_WORKSPACE_SW_ONLY</code> for the <code>Workspace_SW_only</code> module. </p> <p>If you belong to multiple Workspaces and you want to list the available Workspaces you need to <code>unset HPC_WORKSPACE</code> before loading the <code>Workspace</code> module. If you already know the name, you can switch into another Workspace (here Workspace <code>foo</code>) by:</p> <pre><code>module unload Workspace\nHPC_WORKSPACE=foo module load Workspace\n</code></pre> <p>When you are creating a new module, there are cases where you need to reload a Workspace module, e.g. for a Python packages. This can be obtained using e.g. the following lua modulefile syntax:</p> <pre><code>if mode() == 'load' then\n  if not ( isloaded(\"Python\") ) then\n    load(\"Python\")\n    if ( isloaded(\"Workspace\") ) then\n        load(\"Workspace\")\n    end\n    if ( isloaded(\"Workspace_SW_only\") ) then\n        load(\"Workspace_SW_only\")\n    end\n    if ( isloaded(\"Workspace_HOME\") ) then\n        load(\"Workspace_HOME\")\n    end\n  end\nend\n</code></pre> <p>Assuming we want to create a module for an additional Python package. In this example we not only verify that <code>Python</code> is loaded, but also that the Workspace Python settings (incl. <code>PYTHONPATH</code> and <code>PYTHONPACKAGEPATH</code>) are set properly by reloading the Workspace module. </p>"},{"location":"hpc-workspaces/management.html","title":"Workspace management","text":""},{"location":"hpc-workspaces/management.html#description","title":"Description","text":"<p>This article targets Workspace managers. It covers applications, Workspace properties, up to modifications and suggested usage of Workspaces.</p>"},{"location":"hpc-workspaces/management.html#workspace-intention","title":"Workspace Intention","text":"<p>Workspaces are group shared resources. This group could be for example:</p> <ul> <li>whole research group working together on shared data and software packages</li> <li>student(s) an their supervisor</li> <li>collaborating researchers from different research groups/institutes</li> <li>sub-group within the research group, which requires data separation</li> </ul>"},{"location":"hpc-workspaces/management.html#workspace-properties","title":"Workspace Properties","text":""},{"location":"hpc-workspaces/management.html#ownership","title":"Ownership","text":"<p>HPC Workspaces manage compute and storage resources. Especially the freely available resources are meant to be shared between participating research groups. Therefore, the Workspaces belong to a beforehand registered research group.</p> <p>An HPC Workspace can be only requested by a research group leader, who responsible and accountable, since costly extensions can be added to the Workspace.  Additionally, a deputy can be nominated, who also can manage the Workspace (see Workspace Modification). Owners and deputies are called Workspace managers. </p> <p>Deputies</p> <p>Deputies have the same privileges than the owner, up to purchasing costly resources without additional notification. </p> <p>If the workspace is meant to be for a collaboration of researchers from different research groups, you need to agree to one research group which is responsibilities and gets accounted for the resources. This research group leader need to request the workspace. </p>"},{"location":"hpc-workspaces/management.html#member-groups","title":"Member groups","text":"<p>Each Workspace has two member groups:</p> <ul> <li>primary users with read and write access and</li> <li>secondary users with read only access</li> </ul> <p>Only members of the primary group can create and modify data, belonging to the Workspace, as well as submitting jobs to the Workspace account. The member lists are defined at Workspace application time and can be modified later. </p> <p>Members can be anyone with an UniBE Campus Account, but need to be registered for UBELIX usage, see Account Activation.</p>"},{"location":"hpc-workspaces/management.html#free-of-charge-quota","title":"Free Of Charge Quota","text":"<p>Every research group has 10TB free of charge quota. This can be used within one or more Workspaces. The amount used per Workspace is set at application time and can be changed later within the limitation. </p>"},{"location":"hpc-workspaces/management.html#additional-storage","title":"Additional Storage","text":"<p>See costs page for details.</p>"},{"location":"hpc-workspaces/management.html#availability","title":"Availability","text":"<p>Storage is a limited and expensive resource. Abandoned, unused workspaces should be prevented by design. Therefore, an HPC Workspace has a default live time of one year. A notification will be send before the Workspace expiry. The Workspace expiry date can be changed at any time to any date within the next 365 days by any Workspace manager.  ServicePortal -&gt; Shop -&gt; HPC -&gt; Edit HPC Workspace -&gt; Workspace Duratio</p>"},{"location":"hpc-workspaces/management.html#application","title":"Application","text":""},{"location":"hpc-workspaces/management.html#prerequisite","title":"Prerequisite","text":"<p>Since we are sharing the HPC resources on basis of research groups, a registration of these is required. Only official University of Bern research groups can register. These need to be officially represented within the unibe.ch sites. Since research group definition are vague. In unclear situation please specify the responsible professor.</p> <p>Required information:</p> <ul> <li>research group name</li> <li>research group responsible, in best case responsible professor <ul> <li>A long term responsible person, e.g. for data in case actual research group lead leaves university. Research group or topic need to be at least stated on the UniBE webpage.</li> </ul> </li> <li>cost center for location in organizational tree (a UniBE cost center)<ul> <li>for determining the proper cost center, please ask YOUR UniBE secretary</li> </ul> </li> <li>official unibe.ch research group URL, where the research group name and research group head is mentioned</li> <li>research group ID which is merged with the institute ID. This code is used in the UBELIX file system tree</li> </ul> <p>Registration Form: Service Portal -&gt; Register Research Group</p> <p>The request need to be verified manually. This may take some time. </p>"},{"location":"hpc-workspaces/management.html#application-form","title":"Application Form","text":"<p>An HPC Workspace can (only) be requested by a registered research group lead/manager using the ServicePortal application form:</p> <p>Service Portal -&gt; Create HPC Workspace</p> <p>The following information are required:</p> <ul> <li>Workspace ID (max. 20 characters), please choose a unique name</li> <li>Workspace Name</li> <li>Workspace Description</li> <li>registered research group (see prerequisites)</li> <li>Deputy (permissions for managing the Workspace) (optional)</li> <li>Free Quota (see above)</li> <li>additional Storage (optional): an upper limit of quota, where the actual quota will be charged. When selected this requires a valid cost center for accounting. </li> <li>Cost Center (necessary when requesting Additional Storage)</li> <li>primary group members (all accounts need UBELIX activation beforehand)</li> <li>secondary group members (optional) (all accounts need UBELIX activation beforehand) member lists can be selected one by one or as a comma separated list of Campus accounts (see Import Bulk User)</li> </ul> <p>All members need to have their account activated for UBELIX, otherwise the process will fail and need manual intervention. See Account Activation</p> <p>If the requester only want to hold the managing position without UBELIX, the requester can remove his/her account from the primary member list. This can be changed any time.</p> <p>Workspace ID</p> <p>The Workspace ID need to be a unique string. Please avoid duplications with user IDs, Research Group IDs and other Workspace names. </p> <p>processing time</p> <p>The Workspace creation for now relies on a temporary automatic process which is running only once a day at 20:00. In future the process will be much faster. </p>"},{"location":"hpc-workspaces/management.html#workspace-modifications","title":"Workspace modifications","text":"<p>After creation, owners and deputies can modify Workspace properties using the Service Portal Form:</p> <p>Service Portal -&gt; Edit HPC Workspace</p> <p>Properties to change are: - adding/removing members to/from primary and secondary group - storage extension - Workspace live time extension - Workspace closure (so far you need to \u201cdeactivate\u201d AND THEN \u201cdelete\u201d the workspace)</p> <p>Note</p> <p>During the processing of a modification no other modification can be requested. The Workspace is even not visible in the ServicePortal for that time.  Most modification will be processed within few minutes, but adding non-free-of-charge features like additional storage, need human approval, which may delay the process.  The Workspace itself (file storage and Slurm, etc.) will not be interrupted by a change. </p>"},{"location":"hpc-workspaces/management.html#investor-qos","title":"Investor QoS","text":"<p>Investors get elevated priviledges for specific queues. These are managed in so called Slurm QoS (Quality of Service). Where in the past the investors specified a list of users, who can use the QoS, in future Workspaces are able to manage the QoS on Workspace level. Therefore, you need to open a request to add an existing QoS to a (list of) Workspace(s). The membership managment is done within the Workspace. </p> <p>Warning</p> <p>Investor QoS are bind to SLURM accounts. Since personal SLURM accounts get deactivated when joining an HPC Workspace, the investor QoS need to get transfered. This process is not done automatically, please request the transfer using our ServicePortal</p>"},{"location":"hpc-workspaces/management.html#import-bulk-users","title":"Import Bulk Users","text":"<p>The \u201cImport Bulk Users\u201d field provides the possibility to list a larger set of members for primary or secondary group without selecting them one by one. There the members need to be specified as a comma seperated list of Campus Accounts (not full names).  Keep in mind: you need to specify the full list of members in these field. After leaving the field the upper primary/secondary member list will be replaced with this list.</p>"},{"location":"hpc-workspaces/management.html#permission-background","title":"Permission Background","text":"<p>This sections provides more advanced information about the implementation of the permission handling for both permission groups. Nevertheless, we strongly suggest to keep default permissions. <code>-rw-rw-r--</code> for files and <code>drwxrwxr-x</code> for directories. </p> <p>In Linux permissions are manged using: <code>user</code> (owner), <code>group</code> and <code>other</code>. <code>user</code> will always be the username creating the file/directory. In a Workspace the <code>group</code> will always be the Workspace primary group. This should (and this is default) get read/write permissions. The Workspace secondary group gets access to the Workspace main directory using ACLs. Then, within the Workspace these members acts as <code>other</code> and need to get read permissions. </p> <p>Thus a file:</p> <pre><code>-rw-rw-r--\u00a0 2 user ws_group\u00a0 4096 Jan 01 09:11 filename\n</code></pre> <p>can be modified by all members of the <code>ws_group</code> and read by everyone else (with access to that location, which is grated for secondary Workspace group members). And a file</p> <pre><code>-rw-rw----\u00a0 2 user ws_group\u00a0 4096 Jan 01 09:11 filename2\n</code></pre> <p>can be modified by all members of the <code>ws_group</code>, but NOT read by anyone else (even not secondary Workspace group members).</p> <p>Please, make sure that all your files and directories keep this permissions. In case of moving data from other locations, these could vary and can be corrected using:</p> <pre><code>find /storage/workspace/&lt;researchGroupID&gt;/&lt;workspaceID&gt;/ \\\n\\( -type f -exec chmod 664 {} \\; \\) , \\\n\\( -type d -exec chmod 775 {} \\; \\)\n</code></pre>"},{"location":"hpc-workspaces/workspaces.html","title":"Workspaces Introduction","text":""},{"location":"hpc-workspaces/workspaces.html#description","title":"Description","text":"<p>The HPC Workspaces provide a group shared environment, including storage with user-defined access, SLURM accounting, and tools. </p> <p>An HPC Workspace belong to a research group and need to be requested by the research group leader, see Workspace management. </p>"},{"location":"hpc-workspaces/workspaces.html#short-summary","title":"Short Summary","text":"<p>Workspaces provide a collaborative environment with user defined access groups:</p> <ul> <li>a primary group with read/write access and </li> <li>a secondary group with read only access</li> </ul> <p>Each Workspace provide:</p> <ul> <li>permanent storage (<code>/storage/workspaces/&lt;researchGroupID&gt;/&lt;workspaceID&gt;</code>) </li> <li>temporary storage (<code>/storage/scratch/&lt;researchGroupID&gt;/&lt;workspaceID&gt;</code>)</li> <li>user-friendly access to a custom software repositories and monitoring tools and</li> <li>SLURM accounting to that Workspace. Fair share between research groups.</li> </ul> <p>Attention</p> <p>Please always load the <code>Workspace</code> module, even if only just copying files into it. The module corrects the <code>umask</code> and therewith the created file and directory permissions. </p> <p>Furthermore, it is good practice to use <code>$WORKSPACE/file1</code> instead of absolute path <code>/path/to/Workspace/file1</code></p>"},{"location":"hpc-workspaces/workspaces.html#motivation","title":"Motivation","text":"<p>HPC data typically is shared data. This could be between students and supervisors, between researchers of a research group, or even between researchers of different institutes. These data needs to be accessible even if people leave the team.  Furthermore, these data is usually processed with a set of custom software tools, which need to be easy accessible, share, between the collaborators. </p>"},{"location":"hpc-workspaces/workspaces.html#advantages","title":"Advantages","text":"<ul> <li>group based storage access (data and software)</li> <li>enhanced collaborations between researchers, even across institutes</li> <li>user-friendly access control by Workspace managers</li> <li>high bandwidth storage and backup</li> <li>temporary space with less restricted quota</li> <li>research group based compute resource sharing</li> </ul>"},{"location":"hpc-workspaces/workspaces.html#storage-access-permissions","title":"Storage Access Permissions","text":"<p>HOME is meant to be a private space, mainly for configurational data.  All group oriented data is meant to be located in a HPC Workspace directories:</p> <ul> <li><code>/storage/workspaces/&lt;researchGroupID&gt;/&lt;workspaceID&gt;</code>  for permanent storage</li> <li><code>/storage/scratch/</code> for temporary storage, see File Systems</li> </ul> <p>All files and directories in the group shared spaces are meant to stay with default permissions, like <code>-rw-rw-r--</code>: </p> <ul> <li>read and write for owner and primary group members</li> <li>read only access for others. Since the Workspace directory itself is limited to primary and secondary group members using ACLs, secondary group members get read permissions. </li> </ul>"},{"location":"hpc-workspaces/workspaces.html#members","title":"Members","text":"<p>The Workspace owner can define a deputy who gets same permissions as owner, including booking at cost. </p> <p>Workspace permissions are managed with two lists of members:</p> <ul> <li>primary users, with read and write access to all data in the spaces, permission to account to that Workspace</li> <li>secondary users, with read only access</li> </ul> <p>Thus, the Workspace and SCRATCH space are accessible to all members of the both lists of users, but only the members of the primary list can write or modify data. </p> <p>Members can be anyone with an active UniBE Campus Account.</p> <p>The Workspace owner or its deputies can manage the lists using the Service Portal Workspace edit Form: Service Portal -&gt; Edit HPC Workspace</p>"},{"location":"hpc-workspaces/workspaces.html#backup","title":"Backup","text":"<p>All data in the permanent space (<code>/storage/workspaces/</code>) is protected using daily snapshots and backups.</p> <p>Scratch will not be protected by snapshots or backups.</p>"},{"location":"hpc-workspaces/workspaces.html#quota","title":"Quota","text":"<p>For default and actual quota information use <code>quota</code> tool. More details see File System Quota.</p>"},{"location":"hpc-workspaces/workspaces.html#slurm","title":"SLURM","text":"<p>Computational work is accounted to a Workspace account. Every workspace belongs to a research group. The freely available resources are shared between research groups. </p> <p>In contrast to the previous implementation, the priorities are based on research group usage and shared between all workspaces related to this research group. </p>"},{"location":"slurm/array-jobs.html","title":"Array Jobs with Slurm","text":""},{"location":"slurm/array-jobs.html#description","title":"Description","text":"<p>Array jobs are jobs where the job setup, including job size, memory, time etc. is constant, but the application input varies. One use case are parameter studies. </p> <p>Instead of submitting N jobs independently, you can submit one array job unifying N tasks. These provide advantages in the job handling as well as for the SLURM scheduler.</p>"},{"location":"slurm/array-jobs.html#submitting-an-array","title":"Submitting an Array","text":"<p>To submit an array job, specify the number of tasks as a range of task IDs using the \u2013array option:</p> <pre><code>#SBATCH --array=n[,k[,...]][-m[:s]]%&lt;max_tasks&gt;\n</code></pre> <p>The task id range specified in the option argument may be: </p> <ul> <li>comma separated list of values: <code>#SBATCH --array=1,3,5</code> </li> <li>simple range of the form n-m: <code>#SBATCH --array=201-300</code> (201, 202, 203, \u2026, 300)</li> <li>range with a step size s: <code>#SBATCH --array=100-200:2</code> (100, 102, 104, \u2026 200)</li> <li>combination thereof: <code>#SBATCH --array=1,3,100-200</code> (1, 3, 100, 101, 102, \u2026, 200)</li> </ul> <p>Furthermore, the amount of concurent running jobs can limited using the <code>%</code> seperator, e.g. for max 100 concurrent jobs of 1-400: <code>#SBATCH --array=1-400%100</code>. Therewith you can prevent fully filling your available resources.  The task IDs will be exported to the job tasks via the environment variable <code>SLURM_ARRAY_TASK_ID</code>. Additionally, <code>SLURM_ARRAY_TASK_MAX</code>, <code>SLURM_ARRAY_TASK_MIN</code>, <code>SLURM_ARRAY_TASK_STEP</code> are available in job, describing the task range of the job.</p> <p>Specifying <code>--array=10</code> will not submit an array job with 10 tasks, but an array job with a single task with task id 10. To run an array job with multiple tasks you must specify a range or a comma separated list of task ids.</p>"},{"location":"slurm/array-jobs.html#output-files","title":"Output files","text":"<p>Per default the output files are named as <code>slurm-&lt;jobid&gt;_&lt;taskid&gt;.out</code>. When renaming the output/error files variables for the job ID (<code>%A</code>) and for the task ID (<code>%a</code>) can be used. For example:</p> <p><pre><code>#SBATCH --output=array_example_%A_%a.out\n#SBATCH --error=array_example_%A_%a.err\n</code></pre> Thus a file <code>array_example_6543212_12.out</code> will be written for the 12th task of job 6543212.</p>"},{"location":"slurm/array-jobs.html#canceling-individual-tasks","title":"Canceling Individual Tasks","text":"<p>You can cancel individual tasks of an array job by indicating tasks ids to the scancel command:</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n79265_[49-99:2%20]      test Simple H  foo      PD      0:00      1 (QOSMaxCpuPerUserLimit)\n          79265_41      test Simple H  foo      R       0:10      1 fnode03\n          79265_43      test Simple H  foo      R       0:10      1 fnode03\n          79265_45      test Simple H  foo      R       0:10      1 fnode03\n          79265_47      test Simple H  foo      R       0:10      1 fnode03\n</code></pre> <p>Use the <code>--array</code> option to the squeue command to display one tasks per line:</p> <pre><code>$ squeue --array\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          79265_65      test Simple H  foo      PD      0:00      1 (QOSMaxCpuPerUserLimit)\n          79265_67      test Simple H  foo      PD      0:00      1 (QOSMaxCpuPerUserLimit)\n          79265_69      test Simple H  foo      PD      0:00      1 (QOSMaxCpuPerUserLimit)\n          79265_97      test Simple H  foo      PD      0:00      1 (QOSMaxCpuPerUserLimit)\n          79265_57      test Simple H  foo      R       0:47      1 fnode03\n          79265_59      test Simple H  foo      R       0:47      1 fnode03\n          79265_61      test Simple H  foo      R       0:47      1 fnode03\n          79265_63      test Simple H  foo      R       0:47      1 fnode03\n</code></pre>"},{"location":"slurm/array-jobs.html#examples","title":"Examples","text":""},{"location":"slurm/array-jobs.html#use-case-1-1000-computations-same-resource-requirements-different-inputoutput-arguments","title":"Use case 1: 1000 computations, same resource requirements, different input/output arguments","text":"<p>Instead of submitting 1000 individual jobs, submit a single array jobs with 1000 tasks:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:30:00     # Each task takes max 30 minutes\n#SBATCH --mem-per-cpu=2G    # Each task uses max 2G of memory\n#SBATCH --array=1-1000      # Submit 1000 tasks with task ID 1,2,...,1000.\n\n# The name of the input files must reflect the task ID!\nsrun ./foo input_data_${SLURM_ARRAY_TASK_ID}.txt &gt; output_${SLURM_ARRAY_TASK_ID}.txt\n</code></pre> <p>Task with ID 20 will run the program foo with the following arguments: ./foo input_data_20.txt &gt; output_20.txt</p>"},{"location":"slurm/array-jobs.html#use-case-2-read-arguments-from-file","title":"Use case 2: Read arguments from file","text":"<p>Submit an array job with 1000 tasks. Each task executes the program foo with different arguments:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:30:00    # Each task takes max 30 minutes\n#SBATCH --mem-per-cpu=2G   # Each task uses max 2G of memory\n### Submit 1000 tasks with task ID 1,2,...,1000. Run max 20 tasks concurrently\n#SBATCH --array=1-1000%20  \n\ndata_dir=$WORKSPACE/projects/example/input_data        \nresult_dir=$WORKSPACE/projects/example/results\n\nparam_store=$WORKSPACE/projects/example/args.txt     \n### args.txt contains 1000 lines with 2 arguments per line.\n###    Line &lt;i&gt; contains arguments for run &lt;i&gt;\n# Get first argument\nparam_a=$(cat $param_store | awk -v var=$SLURM_ARRAY_TASK_ID 'NR==var {print $1}')\n# Get second argument\nparam_b=$(cat $param_store | awk -v var=$SLURM_ARRAY_TASK_ID 'NR==var {print $2}')\n\n### Input files are named input_run_0001.txt,...input_run_1000.txt\n###    Zero pad the task ID to match the numbering of the input files\nn=$(printf \"%04d\" $SLURM_ARRAY_TASK_ID)\n\n\nsrun ./foo -c $param_a -p $param_b -i ${data_dir}/input_run_${n}.txt -o ${result_dir}/result_run_${n}.txt\n</code></pre>"},{"location":"slurm/checkpointing.html","title":"Checkpointing","text":""},{"location":"slurm/checkpointing.html#description","title":"Description","text":"<p>Checkpointing: Saving the state of a computation so that it can be resumed later. On this page we provide some useful information for making your own code checkpoint-able.</p> <p>Currently, we do not provide and support programs and libraries (e.g. BLCR) that allow to checkpoint proprietary (closed source) software.</p> <p>Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more.</p>"},{"location":"slurm/checkpointing.html#why-checkpointing","title":"Why checkpointing","text":"<p>Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be:</p> <ul> <li>Exceeding the time limit</li> <li>Exceeding allocated memory</li> <li>Job gets preempted by another job (gpu partition only!)</li> <li>Node failure</li> </ul> <p>Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a disastrous event.</p>"},{"location":"slurm/checkpointing.html#general-recipe-for-checkpointing-your-own-code","title":"General recipe for checkpointing your own code","text":"<p>Introducing checkpointing logic in your code consists of 3 steps</p> <pre><code> 1. Look for a state file containing a previously saved state.\n 2. If a state file exists, then restore the state. Else, start from scratch.\n 3. Periodically save the state.\n</code></pre>"},{"location":"slurm/checkpointing.html#using-unix-signals","title":"Using UNIX signals","text":"<p>You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program:</p> Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process <p>kill -l for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL</p> <p>Slurm sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using scancel and jobs canceled by Slurm, e.g. because of exceeding time limit.</p>"},{"location":"slurm/checkpointing.html#register-a-signal-handler-for-a-unix-signal","title":"Register a signal handler for a UNIX signal","text":"<p>The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page.</p> <p>Bash</p> <pre><code>#!/bin/bash\n\nfunction signal_handler {\n  # Save program state and exit\n  (...)\n  exit\n}\n\ntrap signal_handler TERM\n(...)\n</code></pre> <p>C/C++</p> <pre><code>#include &lt;signal.h&gt;  // C\n#include &lt;csignal&gt;   // C++\n\nvoid signal_handler(int signal) {\n  // Save program state and exit\n  (...)\n exit(0);\n}\n\n// Register signal handler for SIGTERM\nsignal(SIGTERM, signal_handler);  // signal_handler: function to handle signal\n(...)\n</code></pre> <p>Python</p> <pre><code>#! /usr/bin/env python\nimport signal\nimport sys\n\n\ndef signal_handler(sig, frame):\n  # Save program state and exit\n  (...)\n  sys.exit(0)\n\n\nsignal.signal(signal.SIGTERM, signal_handler)\n(...)\n</code></pre>"},{"location":"slurm/checkpointing.html#signaling-checkpoint-creation-without-canceling-the-job","title":"Signaling checkpoint creation without canceling the job","text":"<p>Slurm distinguishes between the job script, its child processes and job steps. Job steps are launched using <code>srun</code>. </p> <p>All applications which should reveive more signals than the default SIGERM at the end of the job, does need to be started using <code>srun</code>. Then signals (here <code>SIGUSR1</code>) can be send using:</p> <pre><code>scancel --signal=USR1 &lt;jobID&gt;\n</code></pre> <p>If you need/want to handle signals with the batch script, add <code>--batch</code> (signals only to the batch script) or <code>--full</code> (signal to all steps and the batch script), e.g.</p> <pre><code>scancel --full --signal=USR1 &lt;jobid&gt;\n</code></pre> <p>Therewith, you can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job.</p> <p>Using scancel with the \u2013signal option won\u2019t terminate the job or job step.</p>"},{"location":"slurm/deleting-jobs.html","title":"Deleting Jobs","text":""},{"location":"slurm/deleting-jobs.html#description","title":"Description","text":"<p>This page provides information on how to delete jobs.</p>"},{"location":"slurm/deleting-jobs.html#scancel","title":"scancel","text":"<p>Use the scancel command to delete active jobs</p> <p>Syntax</p> <pre><code>scancel [options] &lt;jobid&gt; ...\n</code></pre> <p><code>scancel</code> can be restricted to a subset of jobs, using the following options with the related value, e.g:</p> <pre><code>-u, --user $USER    jobs of current user\n-A, --account       jobs under this charge account\n-n, --jobname       jobs with this job name\n-p, --partition     jobs in this partition\n-t, --state         jobs in this state\n</code></pre>"},{"location":"slurm/deleting-jobs.html#examples","title":"Examples","text":"<p>Delete specific job:</p> <pre><code>scancel 12345678\n</code></pre> <p>Delete all running jobs:</p> <pre><code>scancel --state=R\n</code></pre> <p>Delete all of your jobs:</p> <pre><code>scancel --user $USER\n</code></pre>"},{"location":"slurm/dependencies.html","title":"Job Dependencies","text":""},{"location":"slurm/dependencies.html#description","title":"Description","text":"<p>This pages describes the SLURM depencency feature.</p>"},{"location":"slurm/dependencies.html#use-cases","title":"Use cases","text":"<p>This feature is used when you need to chain jobs, due to dependencies. For example: </p> <ul> <li>a preprocessing job with 1 core should be followed by a simulation with 40 cores. Results should be post processed with a single core job. </li> <li>a post processing job should be submitted after all tasks of a job array are finished</li> </ul>"},{"location":"slurm/dependencies.html#usage","title":"Usage","text":"<p>The follow-up job need to specify the dependency using the <code>sbatch</code> option <code>--dependency=&lt;type&gt;:&lt;listOfJobIDs&gt;</code>.  The type can be <code>after</code>, <code>afterok</code>, <code>afterany</code>, <code>afternotok</code>, <code>aftercorr</code>, <code>expand</code>, <code>singleton</code>. (see <code>man sbatch</code> for more info). </p> <p>The underlying job (which this job depends on) need to be submitted first. The related job ID can be caught, by collecting the sbatch output with the <code>--parsable</code> option, e.g.</p> <pre><code>jid_w01=$(sbatch --parsable job01.sh)\n</code></pre>"},{"location":"slurm/dependencies.html#example","title":"Example","text":"<p>A pipeline should be build with: - preparation: <code>job_prep.sh</code> - 2 worker jobs (<code>job01.sh</code> and <code>job02.sh</code>)  - if successfull: a collector job (<code>job_coll.sh</code>) - otherwise: a handling the error job (<code>job_handle_err.sh</code>) - The following script would submit all 3 job with respect to their dependencies. </p> <pre><code>jid_pre=$(sbatch --parsable job_prep.sh)\njid_w01=$(sbatch --parsable --dependency=afterok:${jid_pre} job01.sh)\njid_w02=$(sbatch --parsable --dependency=afterok:${jid_pre} job02.sh)\nsbatch --dependency=afterok:${jid_w01}:${jid_w02} job_coll.sh\nsbatch --dependency=afternotok:${jid_w01}:${jid_w02} job_handle_err.sh\n</code></pre>"},{"location":"slurm/dependencies.html#dependency-on-array-job","title":"Dependency on array job","text":"<p>When specifying a dependency to an array job only one job ID need to be specified, no matter how many array tasks are included. </p> <p>Thus, a 100 task array job and a postprocessing job can be launched using:</p> <p><pre><code>jid=$(sbatch --parsable --array=1-100 job_arr.sh)\nsbatch --dependency=afterok:${jid} job_post.sh\n</code></pre> Where the postprocessing job only runs if all 100 array task ended without error. </p>"},{"location":"slurm/fair-share.html","title":"Fair Share","text":""},{"location":"slurm/fair-share.html#description","title":"Description","text":"<p>The provided resources of UBELIX are meant to be provided in a fair faishon between all interested research groups. Therefore, Workspaces belong to research groups. Every participating research group is entitled for the same amount of resources. This entitlement is shared between all Workspace accounts and its members using that accounts. </p> <p>The fair-share system is designed to encourage users to balance their use of resources over time and de-prioritize users/accounts with over average usage.  Thus no user group/research group gets the opportunity to over-dominate on the systems, while others request resources.  Fair-share is the largest factor in determining priority, but not the only one. </p>"},{"location":"slurm/fair-share.html#fair-share-score","title":"Fair Share Score","text":"<p>Your Fair Share score is a number between 0 and 1. Projects with a larger Fair Share score receive a higher priority in the queue.</p> <p>If an entity \u2014 a research group or its Workspace account \u2014 is using the machine more slowly than expected, for Fair Share purposes it is considered a light user. By contrast, one using the machine faster than expected is a heavy user.</p> <ul> <li>Workspaces at lightly using research groups get a higher Fair Share score than those at heavily using research groups.</li> <li>Within each research group, lightly using Workspaces get a higher Fair Share score than heavily using Workspaces.</li> <li>Using faster than your expected rate of usage will usually cause your Fair Share score to decrease. The more extreme the overuse, the more severe the likely drop.</li> <li>Using slower than your expected rate of usage will usually cause your Fair Share score to increase. The more extreme the underuse, the greater the Fair Share bonus.</li> <li>Using the cluster unevenly will cause your Fair Share score to decrease.</li> </ul>"},{"location":"slurm/fair-share.html#priorities","title":"Priorities","text":"<p>The Fair Share score is used to set the job priorities. It is based on a share of the cluster, which is a fraction of the cluster\u2019s overall computing capacity. </p>"},{"location":"slurm/fair-share.html#fair-share_1","title":"Fair Share","text":"<p>under Construction</p> <p>detailed information will follow soon</p> <p>For investors there priorities may vary. </p>"},{"location":"slurm/gpus.html","title":"GPUs","text":""},{"location":"slurm/gpus.html#description","title":"Description","text":"<p>This page contains all information you need to successfully submit GPU-jobs on UBELIX.</p>"},{"location":"slurm/gpus.html#important-information-on-gpu-usage","title":"Important Information on GPU Usage","text":"<p>Code that runs on the CPU will not auto-magically make use of GPUs by simply submitting a job to the \u2018gpu\u2019 partition! You have to explicitly adapt your code to run on the GPU, e.g. an CUDA or OpenACC implementation. Also, code that runs on a GPU will not necessarily run faster than it runs on the CPU. For example, GPUs require a huge amount of highly parallelizable tasks. In other words, you must understand the characteristics of your job, and make sure that you only submit jobs to the \u2018gpu\u2019 partition that can actually benefit from GPUs.</p> <p>When submitting to the GPU partition the GPU type specification is required. </p>"},{"location":"slurm/gpus.html#gpu-types","title":"GPU Types","text":"<p>UBELIX currently features four types of GPUs. You have to choose an architecture and use one of the following <code>--gres</code> option to select it.</p> Type SLURM gres option Nvidia Geforce GTX 1080 Ti <code>--gres=gpu:gtx1080ti:&lt;number_of_gpus&gt;</code> Nvidia Geforce RTX 2080 Ti <code>--gres=gpu:rtx2080ti:&lt;number_of_gpus&gt;</code> Nvidia Geforce RTX 3090 <code>--gres=gpu:rtx3090:&lt;number_of_gpus&gt;</code> Nvidia Tesla P100 <code>--gres=gpu:teslap100:&lt;number_of_gpus&gt;</code>"},{"location":"slurm/gpus.html#job-submission","title":"Job Submission","text":"<p>Use the following options to submit a job to the <code>gpu</code> partition using the default job QoS:</p> <pre><code>#SBATCH --partition=gpu\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number_of_gpus&gt;\n</code></pre>"},{"location":"slurm/gpus.html#qos-job_gpu_preempt","title":"QoS <code>job_gpu_preempt</code>","text":"<p>For investors we provide the <code>gpu-invest</code> investor partitions with a specific QoS per investor that guarantees instant access to the purchased resources. Nevertheless, to efficiently use all resources, the QoS <code>job_gpu_preempt</code> exists in the <code>gpu</code> partition. Jobs, submitted with this QoS have access to all GPU resources, but  may be interrupted if resources are required for investor jobs. Short jobs, and jobs that make use of checkpointing will benefit from these additional resources.</p> <p>Example: Requesting any four RTX2080Ti from the resource pool in the <code>gpu</code> partition: <pre><code>#SBATCH --partition=gpu\n#SBATCH --qos=job_gpu_preempt\n#SBATCH --gres=gpu:rtx2080ti:4\n## Use the following option to ensure that the job, if preempted,\n## won't be re-queued but canceled instead:\n#SBATCH --no-requeue\n</code></pre></p>"},{"location":"slurm/gpus.html#application-adaptation","title":"Application Adaptation","text":"<p>Applications do only run on GPUs if they are built specifically to run on GPUs that means with GPU support. There are multiple ways to implement algorithms for GPU usage. The most common ones are low level languages like CUDA or pragma oriented implementations like OpenACC.</p>"},{"location":"slurm/gpus.html#cuda","title":"CUDA","text":"<p>We provide compiler and library to build CUDA-based application. These are accessible using environment modules. Use <code>module avail</code> to see which versions are available:</p> <pre><code>module avail CUDA\n---- /software.el7/modulefiles/all ----\n   CUDA/8.0.61                           cuDNN/7.1.4-CUDA-9.2.88\n   CUDA/9.0.176                          cuDNN/7.6.0.64-gcccuda-2019a (D)\n   CUDA/9.1.85                           fosscuda/2019a\n   CUDA/9.2.88                           fosscuda/2019b               (D)\n   CUDA/10.1.105-GCC-8.2.0-2.31.1        gcccuda/2019a\n   CUDA/10.1.243                  (D)    gcccuda/2019b                (D)\n   cuDNN/6.0-CUDA-8.0.61                 OpenMPI/3.1.3-gcccuda-2019a\n   cuDNN/7.0.5-CUDA-9.0.176              OpenMPI/3.1.4-gcccuda-2019b\n   cuDNN/7.0.5-CUDA-9.1.85\n</code></pre> <p>Run <code>module load &lt;module&gt;</code> to load a specific version of CUDA:</p> <pre><code>module load cuDNN/7.1.4-CUDA-9.2.88\n</code></pre> <p>If you need cuDNN you must only load the cuDNN module. The appropriate CUDA version is then loaded automatically as a dependency.</p>"},{"location":"slurm/gpus.html#gpu-usage-monitoring","title":"GPU Usage Monitoring","text":"<p>To verify the usage of one or multiple GPUs the <code>nvidia-smi</code> tool can be utilized. The tool needs to be launched on the related node. After the job started running, a new job step can be created using <code>srun</code> and call <code>nvidia-smi</code> to display the resource utilization. Here we attach the process to an job with the jobID <code>123456</code>. You need to replace the jobId with your gathered jobID previously presented in the sbatch output.</p> <pre><code>$ sbatch job.sh\nSubmitted batch job 123456\n$ squeue --me\n# verify that job gets started\n$ srun --ntasks-per-node=1 --jobid 123456 nvidia-smi\nFri Nov 11 11:11:11 2021\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  On   | 00000000:04:00.0 Off |                  N/A |\n| 23%   25C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  NVIDIA GeForce ...  On   | 00000000:08:00.0 Off |                  N/A |\n| 23%   24C    P8     8W / 250W |      1MiB / 11178MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n</code></pre> <p>Therewith the GPU core utilization and memory usage can be displayed for all GPU cards belonging to that job.</p> <p>Note that this is a one-off presentation of the usage and the called <code>nvidia-smi</code> command runs within your allocation. The required resources for this job step should be minimal and should not noticeably influence your job performance.</p>"},{"location":"slurm/gpus.html#further-information","title":"Further Information","text":"<p>CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf Nvidia Geforce GTX 1080 Ti: https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti Nvidia Geforce RTX 2080 Ti: https://www.nvidia.com/de-de/geforce/graphics-cards/rtx-2080-ti/ Nvidia Geforce RTX 3090: https://www.nvidia.com/de-de/geforce/graphics-cards/30-series/rtx-3090/ Nvidia Tesla P100: http://www.nvidia.com/object/tesla-p100.html</p>"},{"location":"slurm/interactive-jobs.html","title":"Interactive Jobs","text":""},{"location":"slurm/interactive-jobs.html#description","title":"Description","text":"<p>This page describes how to request resources for interactive jobs and how to use the allocated resources when working interactively.</p> <p>There are various use cases for requesting interactive resources, e.g.</p> <ul> <li>debugging: launching a job, tweaking the setup (e.g. compile options), launching job again, tweaking, \u2026</li> <li>interactive interfaces: inspecting a node, or when launching a GUI</li> </ul> <p>In general there are 3 major ways to allocate resources:</p> <ul> <li><code>sbatch</code>: submitting a script which gets scheduled and launched when resources are available, after submitting the shell is coming back immediately</li> <li><code>srun</code>: submitting a single task (no serial preparation or post-processes required), <ul> <li>output on screen</li> <li>shell is blocked until job finished</li> </ul> </li> <li><code>salloc</code>: requesting resources but no specification of tasks. <ul> <li>requested resources will be blocked until job gets killed/canceled</li> <li>shell block until job starts</li> <li><code>srun</code> command launches task on these resources</li> </ul> </li> </ul>"},{"location":"slurm/interactive-jobs.html#salloc","title":"<code>salloc</code>","text":"<p><code>salloc</code> creates a SLURM job allocation with the specified (of default) resources, including CPUs/GPUs, memory, walltime. Here 4 tasks on 1 node with 2GB (8GB total) and 1h is requested from <code>submit01</code></p> <p><pre><code>bash-4.2$ hostname\nsubmit01.ubelix.unibe.ch\nbash-4.2$ salloc --nodes=1 --ntasks-per-node=4 --mem-per-cpu=2G --time=01:00:00\nsalloc: Pending job allocation 63752579\nsalloc: job 63752579 queued and waiting for resources\nsalloc: job 63752579 has been allocated resources\nsalloc: Granted job allocation 63752579\nbash-4.2$ hostname\nsubmit01.ubelix.unibe.ch\nbash-4.2$ srun hostname\nbnode007\n</code></pre> After submitting the <code>salloc</code> command the terminal will be blocked until the job gets granted. Then the session still persists on the login node <code>submit01</code>. Only when using <code>srun</code> commands are executed on the requested compute node. The task send with <code>srun</code> can run immediately, since the resources are allocated already. </p> <p>! type Attention \u201cquit session\u201d     Please release resources immediately if not needed anymore, by using <code>exit</code>, <code>Ctrl-d</code> or <code>scancel $SLURM_JOB_ID</code></p>"},{"location":"slurm/interactive-jobs.html#interactive-shell-session","title":"Interactive shell session","text":"<p>An interactive shell session on a compute node can be established using </p> <p><pre><code>bash-4.2$ srun --pty bash \nsrun: job 9173384 queued and waiting for resources\nsrun: job 9173384 has been allocated resources\nbash-4.2$ hostname\nbnode026\n</code></pre> The command is blocking until the resources are granted. The session then is established directly on the first compute node. </p>"},{"location":"slurm/interactive-jobs.html#graphical-interfaces-using-x11-forwarding","title":"Graphical Interfaces using X11 Forwarding","text":"<p>Requirements:</p> <ul> <li>You must login to UBELIX with X11 forwarding enabled: <code>ssh -Y &lt;user&gt;@submit03.unibe.ch</code> . Make this the default with ForwardX11 yes in  <code>~/.ssh/config</code> .<ul> <li>Password-less communication between all nodes within UBELIX. In order to make this possible, generate a new SSH key (without passphrase) on the login node (submit) and add the public key to <code>~/.ssh/authorized_keys</code> : <code>ssh-keygen -t rsa -b 4096</code> <code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></li> </ul> </li> <li>A X-Server on your local workstation, e.g.<ul> <li>MAC:  Xquartz (X11 no longer included in macOS)</li> <li>Windows: MobaXTerm or Xming</li> </ul> </li> </ul> <p>DO NOT reuse an existing SSH key for this purpose, i.e. do not copy an existing private key from your local machine to UBELIX.</p> <p>With all the requirements in place can now submit an interactive job and export an X11 display on the allocated compute node, e.g:</p> <pre><code>srun -n 1 --x11 gnuplot ex_gnuplot.p\n</code></pre> <p>You can also use X11 forwarding with non interactive jobs adding the option  <pre><code>#SBATCH --x11\n</code></pre> in your job script and using again <code>srun --x11</code> to launch your application. </p>"},{"location":"slurm/investigating-job-failure.html","title":"Investigating a Job Failure","text":""},{"location":"slurm/investigating-job-failure.html#description","title":"Description","text":"<p>Not always jobs execute successfully. There are a list of reasons jobs or applications stop or crash.  The most common causes are:</p> <ul> <li>exceeding resource limits and </li> <li>software-specific errors </li> </ul> <p>Here, discussed are ways to gather information, aspects of avoiding misleading information and aspects of common issues. </p> <p>It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the <code>--error</code>/<code>--output</code> option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure.</p>"},{"location":"slurm/investigating-job-failure.html#exceeding-resource-limits","title":"Exceeding Resource Limits","text":"<p>Each partition defines maximum and default limits for runtime and memory usage.  Within the job specification the current limits can be defined within the ranges.  For better scheduling, the job requirements should be estimated and the limits should be adapted to the needs. The lower the limits the better SLURM can find a spot.  Furthermore, the less resource overhead is specified the less resources are wasted, e.g. for memory. </p> <p>If a job exceeds the runtime or memory limit, it will get killed by SLURM. </p>"},{"location":"slurm/investigating-job-failure.html#error-information","title":"Error Information","text":"<p>In both cases, the error file provides appropriate information:</p> <p>Time limit:</p> <pre><code>(...)\nslurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016-11-30T11:22:57 DUE TO TIME LIMIT ***\n(...)\n``````\n\nMemory limit:\n\n```Bash\n(...)\nslurmstepd: error: Job 41176 exceeded memory limit (3940736 &gt; 2068480), being killed\nslurmstepd: error: Exceeded job memory limit\nslurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016-11-30T10:21:37 ***\n(...)\n</code></pre>"},{"location":"slurm/investigating-job-failure.html#software-errors","title":"Software Errors","text":"<p>The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon).</p> <p>Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified example:</p> <p>fail.R <pre><code>var&lt;-sq(1,1000000000)\n</code></pre></p> <p>job.sbatch <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=mustermann@unibe.ch\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --job-name=\"Simple Example\"\n#SBATCH --time=00:05:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nR CMD BATCH --vanilla fail.R\necho \"Script finished\"\n</code></pre></p> <p>The exit code and state wrongly indicates that the job finished successfully:</p> <pre><code>$ sbatch job_slurm.sh\nSubmitted batch job 41585\n\n$ sacct -j 41585\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n41585        Simple E +        all         id          1  COMPLETED      0:0\n41585.batch       batch                    id          1  COMPLETED      0:0\n</code></pre> <p>Only the R-specific output file shows the error:</p> <p>fail.Rout <pre><code>(...)\n&gt; var&lt;-sq(1,1000000000)\nError: could not find function \"sq\"\nExecution halted\n</code></pre></p> <p>You can bypass this problem by exiting with a proper exit code as soon as the command failed:</p> <p>jobsbatch</p> <pre><code>#!/bin/bash\n# Slurm options\n#SBATCH --mail-user=nico.faerber@id.unibe.ch\n#SBATCH --mail-type=begin,end,fail\n#SBATCH --job-name=\"Simple Example\"\n#SBATCH --time=00:05:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nR CMD BATCH --vanilla fail.R || exit 91\necho \"Script finished\"\n</code></pre> <p>Now, the exit code and state matches the true outcome:</p> <pre><code>$ sbatch job_slurm.sh\nSubmitted batch job 41925\n\n$ sacct -j 41925\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n41925        Simple E +        all         id          1     FAILED     91:0\n41925.batch       batch                    id          1     FAILED     91:0\n</code></pre> <p>Always check application-specifc output files for error messages.</p>"},{"location":"slurm/monitoring-jobs.html","title":"Monitoring Jobs","text":""},{"location":"slurm/monitoring-jobs.html#description","title":"Description","text":"<p>This page provides information about monitoring user jobs.</p> <p>Different Slurm commands provide information about jobs/job steps on different levels. The command squeue provides high-level information about jobs in the Slurm scheduling queue (state information, allocated resources, runtime, \u2026). The command sstat provides detailed usage information about running jobs, and sacct provides accounting information about active and completed (past) jobs. The command scontrol provides even more detailed information about jobs and job steps.</p> <p>The output format of most commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options.</p> <p>Most command options support a short form as well as a long form (e.g. -u , and \u2013user=). Because few options only support the long form, we will consistently use the long form throughout this documentation."},{"location":"slurm/monitoring-jobs.html#squeue","title":"squeue","text":"<p>Use the squeue command to get a high-level overview of all active (running and pending) jobs in the cluster.</p> <p>Syntax</p> <pre><code> squeue [options]\n</code></pre> <p>Common options</p> <pre><code>--user=&lt;user[,user[,...]]&gt;          Request jobs from a comma separated list of users. \n--jobs=&lt;job_id[,job_id[,...]]&gt;      Request specific jobs to be displayed\n--partition=&lt;part[,part[,...]]&gt;     Request jobs to be displayed from a comma separated list of partitions\n--states=&lt;state[,state[,...]]&gt;      Display jobs in specific states. Comma separated list or \"all\". Default: \"PD,R,CG\"\n</code></pre> <p>The default output format is as follows:</p> <pre><code>JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n</code></pre> <p>where</p> <pre><code>JOBID              Job or step ID. For array jobs, the job ID format will be of the form &lt;job_id&gt;_&lt;index&gt;\nPARTITION          Partition of the job/step\nNAME               Name of the job/step\nUSER               Owner of the job/step\nST                 State of the job/step. See below for a description of the most common states\nTIME               Time used by the job/step. Format is days-hours:minutes:seconds\n                   (days,hours only printed as needed)\nNODES              Number of nodes allocated to the job or the minimum amount of nodes required\n                   by a pending job\nNODELIST(REASON)   For pending jobs: Reason why pending. For failed jobs: Reason why failed.\n                   For all other job states: List of allocated nodes. See below for a list of the most\n                   common reason codes\n</code></pre> <p>You can easily tailor the output format of squeue to your own needs. Use the \u2013format (-o) or \u2013Format (-O) options to request a comma separated list of job information to be displayed. See the man page for more information: man squeue</p>"},{"location":"slurm/monitoring-jobs.html#job-states","title":"Job States","text":"<p>During its lifetime, a job passes through several states. The most common states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED.</p> <pre><code>PD                 Pending. Job is waiting for resource allocation\nR                  Running. Job has an allocation and is running\nS                  Suspended. Execution has been suspended and resources have been released for other jobs\nCA                 Cancelled. Job was explicitly cancelled by the user or the system administrator\nCG                 Completing. Job is in the process of completing. Some processes on some nodes may still be active\nCD                 Completed. Job has terminated all processes on all nodes with an exit code of zero\nF                  Failed. Job has terminated with non-zero exit code or other failure condition\n</code></pre>"},{"location":"slurm/monitoring-jobs.html#why-is-my-job-still-pending","title":"Why is my job still pending?","text":"<p>The REASON column of the squeue output gives you a hint why your job is not running.</p> <p>(Resources)</p> <p>The job is waiting for resources to become available so that the jobs resource request can be fulfilled.</p> <p>(Priority)</p> <p>The job is not allowed to run because at least one higher prioritized job is waiting for resources.</p> <p>(Dependency)</p> <p>The job is waiting for another job to finish first (\u2013dependency=\u2026 option).</p> <p>(DependencyNeverSatisfied)</p> <p>The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs.</p> <p>(QOSMaxCpuPerUserLimit)</p> <p>The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish.</p> <p>(AssocGrpCpuLimit)</p> <p>dito.</p> <p>(AssocGrpJobsLimit)</p> <p>The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish.</p> <p>(ReqNodeNotAvail, UnavailableNodes:\u2026)</p> <p>Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation.</p>"},{"location":"slurm/monitoring-jobs.html#why-cant-i-submit-further-jobs","title":"Why can\u2019t I submit further jobs?","text":"<p>sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.</p>"},{"location":"slurm/monitoring-jobs.html#examples","title":"Examples","text":"<p>List all currently running jobs of user foo:</p> <pre><code>squeue --user=foo --states=PD,R\n</code></pre> <p>List all currently running jobs of user foo in partition bar:</p> <pre><code>squeue --user=foo --partition=bar --states=R\n</code></pre>"},{"location":"slurm/monitoring-jobs.html#scontrol","title":"scontrol","text":"<p>Use the scontrol command to show more detailed information about a job</p> <p>Syntax</p> <pre><code> scontrol [options] [command]\n</code></pre>"},{"location":"slurm/monitoring-jobs.html#examples_1","title":"Examples","text":"<p>Show detailed information about job with ID 500:</p> <pre><code>scontrol show jobid 500\n</code></pre> <p>Show even more detailed information about job with ID 500 (including the jobscript):</p> <pre><code>scontrol -dd show jobid 500\n</code></pre>"},{"location":"slurm/monitoring-jobs.html#sacct","title":"sacct","text":"<p>Use the sacct command to query information about past jobs</p> <p>Syntax</p> <pre><code> sacct [options]\n</code></pre> <p>Common options</p> <pre><code>--endtime=end_time            Select jobs in any state before the specified time.\n--starttime=start_time        Select jobs in any state after the specified time.\n--state=state[,state[,...]]   Select jobs based on their state during the time period given.\n                              By default, the start and end time will be the current time\n                              when the --state option is specified, and hence only currently\n                              running jobs will be displayed.\n</code></pre>"},{"location":"slurm/partitions.html","title":"SLURM partition and QOS","text":""},{"location":"slurm/partitions.html#description","title":"Description","text":"<p>UBELIX provides different CPU and GPU architectures. Furthermore, we provide job different queues for different job priorities and limitations. </p> <p>restructuring 04.05.2021</p> <p>With the maintainace 04.05.2021 we restructured the Slurm partition to:</p> <ul> <li>more effcient resource usage: All users get access to all resources. Investors have priviledeged access</li> <li>general access to GPUs without preempting</li> <li>resource sharing (fair share) based on research groups using Workspaces, instead of institute based sharing </li> <li>simpler partition design</li> </ul> <p>There are three main control mechanisms to specify queues and resources: </p> <ul> <li>Partitions and </li> <li>Quality of Service (QoS)</li> <li><code>--gres</code> to select the targeted GPU architecture, see GPUs</li> </ul>"},{"location":"slurm/partitions.html#partitions","title":"Partitions","text":"<p>There are the current 3 partitions, epyc2 is default:</p> Partition job type CPU / GPU node / GPU memory local Scratch epyc2 single and multi-core AMD Epyc2 2x64 cores 1TB 1TB bdw full nodes only (x*20cores) Intel Broadwell 2x10 cores 156GB 1TB gpu GPU  (8 GPUs per node,  varying CPUs) Nvidia GTX 1080 Ti  Nvidia RTX 2080 Ti  Nvidia RTX 3090  Nvidia Tesla P100 11GB  11GB  24GB  12GB 800GB  2x960GB  1.92TB  800GB <p>The current usage can be listed on the UBELIX status page</p>"},{"location":"slurm/partitions.html#qos","title":"QoS","text":"<p>Within these partitions, QoS are used to distinguish different job limits. In each partition there is a default QoS (bold below). Each QoS has specific limits:</p> Partition QoS time limit cores/node/GPU limit max jobs epyc2 job_epyc2 4 days 512 cores array jobs up to 10000 tasks job_epyc2_debug 20 min 20 cores 1 job_epyc2_long 15 days 64 cores 50 job_epyc2_short 6h 10 nodes 50 bdw job_bdw 24 h 40 nodes 300 job_bdw_debug 20 min 2 nodes 1 job_bdw_short 6 h 2 nodes 10 gpu job_gpu 24 h 6x GTX 1080 Ti  2x RTX 2080 Ti  1x RTX 3090  1x Tesla P100 10 <sup>1</sup>  4 CPU cores per requested GPU job_gpu_debug 20 min 1 GPU 1  4 CPU cores per requested GPU job_gpu_preempt 24 h 12x GTX 1080 Ti  4x RTX 2080 Ti  4x RTX 3090  4x Tesla P100 24 <sup>1</sup>  4 CPU cores per requested GPU <p>The QoS job_epyc2_short and job_gpu_preempt have access extended resources. In case of job_gpu_preempt, these jobs will be pre-empted if not enough resources for the investors. </p> <p>Thus a job can be submitted to the gpu partition using a RTX3090 and allow pre-emption:</p> <pre><code>sbatch --partition=gpu --qos=job_gpu_preempt --gres=gpu:rtx3090 myjob.sh\n</code></pre> <p>Please see for more details: GPUs</p>"},{"location":"slurm/partitions.html#default-and-investor-partition","title":"Default and Investor Partition","text":"<p>All resources are tried to privide in a most accessible way, preventing idle time. Thus the resouces meant for investors can be used by non-investing users too.  Therefore, from the whole resources a certain amount of CPUs/GPUs are \u201creserved\u201d in the investor partition. But if not used, jobs with <code>job_gpu_preempt</code> or <code>job_{bdw|epyc2}_short</code> can run on these resources. These are floating resources and do not bind to specific hardware, only certain amount per hardware type.</p>"},{"location":"slurm/partitions.html#investor-qos","title":"Investor QoS","text":"<p>Investors get elvated priviledges on certain resources.  The membership to these investor priviledges will/are managed on basis of HPC Workspaces, see Workspace Management.</p> <p>To utilize the invested resources, members need to specify one of the following investor partitions:</p> <ul> <li><code>epyc2-invest</code>, </li> <li><code>bdw-invest</code>, </li> <li><code>gpu-invest</code></li> </ul> <p>As an example, the members of an GPU investor, submit jobs with: <pre><code>module load Workspace         # use the Workspace account\nsbatch --partition=gpu-invest job.sh\n</code></pre></p> <p>previous <code>empi</code> investors now use <code>bdw-invest</code> partition and GPU investors use <code>gpu-invest</code></p>"},{"location":"slurm/partitions.html#technical-details","title":"Technical Details:","text":"<p>Within the investor partition, investor QoS are specified. These have the format <code>job_&lt;partitionBase&gt;_&lt;investorID&gt;</code>, e.g. \u201cjob_bdw_aschauer\u201d.  These QoS will be default for the members in the investor partition. And therewith do not need to be specified.  You can check your investor QoS and the used partitions using: <pre><code>sacctmgr show assoc where user=$USER format=user%20,account%20,partition%16,qos%40,defaultqos%20\n</code></pre></p> <ol> <li> <p>The gpu job limits are determined by the maximum of single GPU jobs on all architecture. But keep in mind, that jobs should be submitted to the best suited architecture.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"slurm/submission.html","title":"Job Submission","text":""},{"location":"slurm/submission.html#description","title":"Description","text":"<p>This section describes the interaction with the resource manager. The subchapters contain information about submitting jobs to the cluster, monitoring active jobs and retrieving useful information about resource usage.</p> <p>A cluster is a set of connected computers that work together to solve computational tasks (user jobs) and presents itself to the user as a single system. For the resources of a cluster (e.g. CPUs, GPUs, memory) to be used efficiently, a resource manager (also called workload manager or batch-queuing system) is vital. While there are many different resource managers available, the resource manager of choice on UBELIX is SLURM. After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. If the requested resources are already available, the job can start immediately. Otherwise, the start of the job is delayed (pending) until enough resources are available. SLURM allows you to monitor active (pending, running) jobs and to retrieve statistics about finished jobs e.g. (peak CPU usage). The subchapters describe individual aspects of SLURM.</p> <p>This page describes the job submission process with Slurm.</p> <p>keep output</p> <p>It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the <code>--error</code>/<code>-\u2013output</code> option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure.</p> <p>job series</p> <p>Submit series of jobs (collection of similar jobs) as array jobs instead of one by one. This is crucial for backfilling performance and hence job throughput. instead of submitting the same job repeatedly. See Array jobs</p>"},{"location":"slurm/submission.html#simple-example","title":"Simple Example","text":"<p>Batch sbmission script, <code>jobs.sh</code>: <pre><code>#!/bin/bash\n#SBATCH --job-name=\"First example\"\n#SBATCH --time=00:10:00\n#SBATCH --mem-per-cpu=1G\n\n# Your code below this line\nmodule load Python\nsrun python3 script.py\n</code></pre></p> <p>Submit the job script:</p> <pre><code>sbatch job.sh\nSubmitted batch job 30215045\n</code></pre> <p>See below for more examples</p>"},{"location":"slurm/submission.html#resource-allocation","title":"Resource Allocation","text":"<p>Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is valid for a specific amount of time, and can be created using the <code>salloc</code>, <code>sbatch</code> or <code>srun</code> commands. Whereas <code>salloc</code> and <code>sbatch</code> only create resource allocations, <code>srun</code> launches parallel tasks within such a resource allocation, or implicitly creates an allocation if not started within one. The usual procedure is to combine resource requests and task execution (job steps) in a single batch script (job script) and then submit the script using the <code>sbatch</code> command.</p> <p>Most command options support a short form as well as a long form (e.g. <code>-u &lt;username&gt;</code>, and <code>--user=&lt;username&gt;</code>). Because few options only support the long form, we will consistently use the long form throughout this documentation.</p> <p>Some options have default values if not specified: The <code>--time</code> option has partition-specific default values (see <code>scontrol show partition &lt;partname&gt;</code>). The <code>--mem-per-cpu</code> option has a global default value of 2048MB.</p> <p>The default partition is epyc2. To select another partition one must use the <code>--partition</code> option, e.g. <code>--partition=gpu</code>.</p>"},{"location":"slurm/submission.html#sbatch","title":"sbatch","text":"<p>The <code>sbatch</code> command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by <code>#SBATCH</code> directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script</p> <p>Syntax</p> <pre><code>sbatch [options] script [args...]\n</code></pre>"},{"location":"slurm/submission.html#job-script","title":"Job Script","text":"<p>Usually a job script consists of two parts. The first part is optional but highly recommended:</p> <ul> <li>Slurm-specific options used by the scheduler to manage the resources (e.g. memory) and configure the job environment</li> <li>Job-specific shell commands</li> </ul> <p>The job script acts as a wrapper for your actual job. Command-line options can still be used to overwrite embedded options.</p> <p>Although you can specify all Slurm options on the command-line, we encourage you, for clarity and reusability, to embed Slurm options in the job script</p>"},{"location":"slurm/submission.html#options","title":"Options","text":"Option Description Example <code>--job-name</code> Specify a job name <code>--job-name=\"Simple Matlab\"</code> <code>--time</code> Expected runtime of the job. Format: dd-hh:mm:ss <code>--time=12:00:00</code> <code>--time=2-06:00:00</code> <code>--ntasks</code> Number of tasks (processes). Used for MPI jobs that may run distributed on multiple compute nodes <code>--ntasks=4</code> <code>--nodes</code> Request a certain number of nodes <code>--nodes=2</code> <code>--ntasks-per-node</code> Specifies how many tasks will run on each allocated node. Meant to be used with <code>--nodes</code>. If used with the <code>--ntasks</code> option, the <code>--ntasks</code> option will take precedence and the <code>--ntasks-per-node</code> will be treated as a maximum count of tasks per node. <code>--ntasks-per-node=2</code> <code>--cpus-per-task</code> Number of CPUs per task (threads). Used for shared memory jobs that run locally on a single compute node. Default is <code>1</code> <code>--cpus-per-task=4</code> <code>--mem-per-cpu</code> Minimum memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G]. Default 2048 MB <code>--mem-per-cpu=2G</code> <code>--output</code> Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file <code>slurm-%j.out</code>, where <code>%j</code> is the job allocation number. <code>--output=myCal_%j.out</code> <code>--error</code> Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file <code>slurm-%j.out</code>, where <code>%j</code> is the job allocation number. <code>--output=myCal_%j.err</code> <code>--partition</code> Select a different partition with different hardware. See Partition/QoS page. Default: <code>epyc2</code> <code>--partition=bdw</code> <code>--partition=gpu</code> <code>--qos</code> Specify \u201cQuality of Service\u201d. This can be used to change job limits, e.g. for long jobs or short jobs with large resources. See Partition/QoS page <code>--tmp</code> Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable <code>$TMPDIR</code>. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <code>--tmp=8G</code> <code>--mail-user</code> Mail address to contact job owner.  Must be a valid email address, if used! <code>--mail-user=foo.bar@unibe.ch</code> <code>--mail-type</code> When to notify a job owner: <code>none</code>, <code>all</code>, <code>begin</code>, <code>end</code>, <code>fail</code>, <code>requeue</code>, <code>array_tasks</code> <code>--mail-type=end,fail</code> <code>--array</code> Submit an array job. Specify the used indices and use \u201c%\u201d to specify the max number of tasks allowed to run concurrently. <code>--array=1,4,16-32:4</code> <code>--array=1-100%20</code> <code>--chdir</code> Set the working directory of the batch script to directory before it is executed. The path can be specified as full path or relative path to the directory where the command is executed. Environment variables are not supported. <code>--chdir=subdir1</code> <code>--dependency</code> Defer the start of this job until the specified dependencies have been satisfied. See <code>man sbatch</code> for a description of all valid dependency types <code>--dependency=afterok:11908</code> <code>--immediate</code> Only submit the job if all requested resources are immediately available <code>--exclusive</code> Use the compute node(s) exclusively, i.e. do not share nodes with other jobs. CAUTION: Only use this option if you are an experienced user, and you really understand the implications of this feature. If used improperly, the use of this option can lead to a massive waste of computational resources <code>--test-only</code> Validate the batch script and return the estimated start time considering the current cluster state <code>--account</code> Specifies account to charge. Please use <code>Workspace</code> module to select Workspace account. Regular users don\u2019t need to specify this option."},{"location":"slurm/submission.html#salloc","title":"salloc","text":"<p>The <code>salloc</code> command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the <code>srun</code> command is used to launch parallel tasks.</p> <p>Syntax <pre><code>salloc [options] [&lt;command&gt; [args...]]\n</code></pre></p> <p>Example <pre><code>bash$ salloc -N 2 -t 10\nsalloc: Granted job allocation 247\nbash$ module load foss\nbash$ srun ./mpi_hello_world\nHello, World.  I am 1 of 2 running on knlnode03.ubelix.unibe.ch\nHello, World.  I am 0 of 2 running on knlnode02.ubelix.unibe.ch\nbash$ exit\nsalloc: Relinquishing job allocation 247\n</code></pre></p>"},{"location":"slurm/submission.html#srun","title":"srun","text":"<p>The <code>srun</code> command creates job steps. One or multiple <code>srun</code> invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation.</p> <p>Do not submit a job script using <code>srun</code>. Embedded Slurm options (<code>#SBATCH</code>) are not parsed by <code>srun</code>.</p> <p>Syntax <pre><code>srun [options] executable [args...]\n</code></pre></p>"},{"location":"slurm/submission.html#when-do-i-use-srun-in-my-job-script","title":"When do I use srun in my job script?","text":"<p>Use <code>srun</code> in your job script for all main executables, especially if these are:</p> <ul> <li>MPI applications</li> <li>multiple job tasks (serial or parallel jobs) simultaneously within an allocation</li> </ul> <p>Example Run MPI task:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Open MPI example\"\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=20\n#SBATCH --mem-per-cpu=2G\n#SBATCH --time=06:00:00\n\n# Your code below this line\nmodule load foss\nsrun ./mpi_app.exe\n</code></pre> <p>Run two jobs simultaneously:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"Open MPI example\"\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=4\n\n# Your code below this line\n# run 2 threaded applications side-by-side\nsrun --tasks=1 --cpus-per-task=4 ./app1 inp1.dat &amp;\nsrun --tasks=1 --cpus-per-task=4 ./app2 inp2.dat &amp;\nwait\n# wait: Wait for both background commands to finish. This is important when running bash commands in the background (using &amp;)! Otherwise, the job ends immediately. \n</code></pre> <p>Please run series of similar tasks as job array. See Array Jobs</p>"},{"location":"slurm/submission.html#requesting-a-partition-qos-queue","title":"Requesting a Partition / QoS (Queue)","text":"<p>Per default jobs are submitted to the <code>epyc2</code> partition and the default QoS <code>job_epyc2</code>.  The partition option can be used to request different hardware, e.b. <code>gpu</code> partition. And the QoS can be used to run in a specific queue, e.g. <code>job_gpu_debug</code>:</p> <pre><code>#SBATCH --partition=gpu --qos=job_gpu_debug\n</code></pre> <p>See Partitions / QoS for a list of available partitions and QoS and its specifications.</p>"},{"location":"slurm/submission.html#accounts","title":"Accounts","text":"<p>By default a user has a \u201cprivate\u201d account. When belonging to a Workspace your private account gets deactivated and you can submit with the Workspace account. We strongly suggest to use the Workspace module (<code>module load Workspace</code>), which automatically sets the Workspace account for you.  If really necessary, the can be selected by the <code>--account</code> option. </p> <p>If a wrong account/partition combination is requested, you will experience the following error message:</p> <pre><code>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified\n</code></pre> <p>If you did not specified <code>--account</code>, and belong to a Workspace, please load the Workspace module fist.</p>"},{"location":"slurm/submission.html#parallel-jobs","title":"Parallel Jobs","text":"<p>A parallel job requires multiple compute cores. These could be within one node or across the machine in multiple nodes. We distinguish following types:</p> <ul> <li> <p>shared memory jobs: SMP, parallel jobs that run on a single compute node. The executable is called once. Within the execution (OMP) threads are spawned and merged.  <pre><code>#SBATCH --ntasks=1           # default value\n#SBATCH --cpus-per-task=4\n</code></pre></p> </li> <li> <p>MPI jobs: parallel jobs that may be distributed over multiple compute nodes. Each task starts the executable. Within the application different workflows need to be defined for the different tasks. The tasks can communicate using Message Passing Interface (MPI). A job with 40 tasks: <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=20\n</code></pre></p> </li> <li> <p>hybrid: jobs using a combination of MPI tasks and (OMP) threads.  <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=5\n#SBATCH --cpus-per-task=4\n</code></pre></p> </li> </ul> <p>The requested node,task, and CPU resources must match! For example, you cannot request one node (<code>--nodes=1</code>) and more tasks (<code>--ntasks-per-node</code>) than CPU cores are available on a single node in the partition. In such a case you will experience an error message:  <pre><code>sbatch: error: Batch job submission failed: Requested node configuration is not available.\n</code></pre></p> <p>parallel launcher</p> <p>Parallel applications, esp. MPI, need a launcher to setup the environment. We strongly suggest to use <code>srun</code> instead of mpirun. </p>"},{"location":"slurm/submission.html#environment-variables","title":"Environment Variables","text":"<p>Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job.</p> Environment Variable Set By Option Description <code>SLURM_JOB_NAME</code> <code>--job-name</code> Name of the job <code>SLURM_ARRAY_JOB_ID</code> ID of your job <code>SLURM_ARRAY_TASK_ID</code> <code>--array</code> ID of the current array task <code>SLURM_ARRAY_TASK_MAX</code> <code>--array</code> Job array\u2019s maximum ID (index) number <code>SLURM_ARRAY_TASK_MIN</code> <code>--array</code> Job array\u2019s minimum ID (index) number <code>SLURM_ARRAY_TASK_STEP</code> <code>--array</code> Job array\u2019s index step size <code>SLURM_NTASKS</code> <code>--ntasks</code> Same as <code>-n</code>, <code>--ntasks</code> <code>SLURM_NTASKS_PER_NODE</code> <code>--ntasks-per-node</code> Number of tasks requested per node.  Only set if the <code>--ntasks-per-node</code> option is specified <code>SLURM_CPUS_PER_TASK</code> <code>--cpus-per-task</code> Number of cpus requested per task.  Only set if the <code>--cpus-per-task</code> option is specified <code>TMPDIR</code> References the disk space for the job on the local scratch <p>For the full list, see <code>man sbatch</code></p>"},{"location":"slurm/submission.html#job-examples","title":"Job Examples","text":""},{"location":"slurm/submission.html#sequential-job","title":"Sequential Job","text":"<p>Running a serial job with email notification in case of error (1 task is default value):</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@baz.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"Serial Job\"\n#SBATCH --time=04:00:00\n\n# Your code below this line\necho \"I'm on host: $HOSTNAME\"\n</code></pre>"},{"location":"slurm/submission.html#parallel-jobs_1","title":"Parallel Jobs","text":"<p>Shared Memory Jobs (e.g. OpenMP)</p> <p>SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is <code>--cpus-per-task</code>. To run N threads in parallel, we request N CPUs on the node (<code>--cpus-per-task=N</code>). </p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@baz.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"SMP Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --cpus-per-task=16\n#SBATCH --time=04:00:00\n\n# Your code below this line\nsrun ./my_binary\n</code></pre> <p>MPI Jobs (e.g. Open MPI)</p> <p>MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option <code>--ntasks</code> to request a certain number of tasks (processes) that can be distributed over multiple nodes:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@baz.unibe.ch\n#SBATCH --mail-type=end\n#SBATCH --job-name=\"MPI Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --ntasks=8\n#SBATCH --time=04:00:00\n\n# Your code below this line\n# First set the environment for using Open MPI\nmodule load foss\nsrun ./my_binary\n</code></pre> <p>On the \u2018bdw\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do:</p> <pre><code>#!/bin/bash\n#SBATCH --mail-user=foo.bar@baz.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=\"MPI Job\"\n#SBATCH --mem-per-cpu=2G\n#SBATCH --nodes=4     ## or --ntasks=80\n#SBATCH --ntasks-per-node=20\n#SBATCH --time=12:00:00\n\n# Your code below this line\nmodule load foss\nsrun ./my_binary\n</code></pre>"},{"location":"slurm/submission.html#performance-considerations","title":"Performance considerations","text":""},{"location":"slurm/submission.html#job-throughput","title":"Job Throughput","text":"<p>It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job abortion, while requesting too much will have a negative impact on job start time and job throughput: Firstly, jobs with a shorter runtime have a greater chance to benefit from being backfilled between long running jobs and may therefore start earlier if resources are scarce. Secondly, a short running job may still start when a scheduled downtime is getting closer while long running jobs won\u2019t start because they are not guaranteed to finish before the start of the downtime.</p> <p>It is crucial to request the correct amount of cores for your job. Requesting cores that your job cannot utilize is a waste of resources that could otherwise be allocated to other jobs. Hence, jobs that theoretically could run have to wait for the resources to become available. For potential consequences of requesting too less cores on job performance, see below.</p> <p>It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs.</p>"},{"location":"slurm/submission.html#job-performanceruntime","title":"Job Performance/Runtime","text":"<p>It is crucial to request the correct amount of cores for your job. For parallel jobs (shared memory, MPI, hybrid) requesting less cores than processes/threads are spawned by the job will lead to potentially overbooked compute nodes. This is because your job will nevertheless spawn the required number of processes/threads (use a certain number of cores) while to the scheduler it appears that some of the utilized resources are still available, and thus the scheduler will allocate those resources to other jobs. Although under certain circumstances it might make sense to share cores among multiple processes/threads, the above reasoning should be considered as a general guideline, especially for inexperienced user.</p>"},{"location":"slurm/submission.html#advanced-settings","title":"Advanced settings","text":""},{"location":"slurm/submission.html#exportnone","title":"EXPORT=NONE","text":"<p>By default the environment from the session, where the job is submitted is forwarded into the job environment. As a result all loaded modules and environment variables present during submit time are also present during run time. </p> <p>To start from a clean environment the environment forwarding can be prevented. Therefore, we have to keep in mind the 2 stages. First, after submission the job script is launched on a compute node. Second, the parallel tasks are launched using srun. We want to preserve the environment forwarding from within the job script to the parallel tasks.  Using just the option <code>sbatch --export=none</code> or the environment variable <code>SBATCH_EXPORT=NONE</code> will prevent the forwarding in both cases.  Therewith, issues may occur, like dynamically linked binaries do not find libraries (e.g. <code>executable.xyz: error while loading shared libraries: libgfortran.so.5: cannot open shared object file: No such file or directory</code>).</p> <p>Thus we suggest to set:</p> <p><pre><code>export SBATCH_EXPORT=NONE\nexport SLURM_EXPORT_ENV=ALL\n</code></pre> This will provide you with a clean environment during job launch and forward the environment set in the job script to the parallel tasks.  This is necessary to have e.g. LD_LIBRARY_PATH set correctly for dynamically linked binaries. </p>"},{"location":"software/Anaconda.html","title":"Anaconda / Python","text":"<p>There are multiple versions of Python available on our HPCs.  On the one hand there are <code>Python</code> modules for Python 3. These already have a longer list of additional packages installed, including <code>pip3</code>. On the other hand there is Anaconda installed, which brings an even longer list of packages with it. We suggest using the <code>Anaconda3</code> modules whenever possible.</p> <pre><code>module load Anaconda3\n</code></pre> <p>Anaconda provides Python and a long list of packages as well as Jupyter and environment and package manager conda and pip. Anaconda brings a long list of Python packages. You can list them using </p> <pre><code>conda list\n</code></pre>"},{"location":"software/Anaconda.html#jupyter","title":"Jupyter","text":"<p>For Jupyter information please see JupyterLab</p>"},{"location":"software/Anaconda.html#managing-virtual-environments-versions-with-anaconda","title":"Managing Virtual Environments, Versions with Anaconda","text":"<p>Anaconda is a high performance distribution of Python that includes the most popular packages for data science (numpy, scipy,\u2026). It also features conda, a package, dependency and environment manager. With Anaconda you can run multiple versions of Python in isolated environments.</p>"},{"location":"software/Anaconda.html#conda","title":"conda","text":"<p>when using conda the system may complain about: <pre><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n    $ conda init &lt;SHELL_NAME&gt;\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\nSee 'conda init --help' for more information and options.\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n</code></pre></p> <p>Please do not run <code>conda init</code>. This would add hard coded environment changes in your <code>$HOME/.bashrc</code>.  Instead initialise the conda environment using: <pre><code>module load Anaconda3\neval \"$(conda shell.bash hook)\"\n</code></pre> This should also be used in your batch submission scripts when working with conda environments.</p>"},{"location":"software/Anaconda.html#conda-environments","title":"conda environments","text":"<p>By default conda environments are located into the <code>$HOME/.conda</code> directory. This can be changed using <code>$CONDA_ENVS_PATH</code>. This variable is set in the Workspace module. Which enables you to share conda environments. </p>"},{"location":"software/Anaconda.html#move-migration-of-conda-environments","title":"Move / Migration of conda environments","text":"<p>If conda environments need to be transfered on the system, e.g. from <code>$HOME</code> to <code>$WORKSPACE</code> you can use the <code>conda pack</code> (see official conda pack documentation). </p> <p>If your environment is already moved file system locations, you can recreate a new environment with the specification of the old environment. Therefore, we specify the location of the old environment, load the Anaconda module, initialize conda, and get the specification of the old environment. Then importantly unset the CONDA_ENVS_PATH to install the new conda environment in the default location and create it. </p> <pre><code>export CONDA_ENVS_PATH=${HOME}/anaconda3/envs ## or where you had your old envs\nmodule load Anaconda3\neval \"$(conda shell.bash hook)\"\nconda info --envs\nconda activate oldEnvName     ## choose your old environment name\nconda list --explicit &gt; spec-list.txt\nunset CONDA_ENVS_PATH\nconda create --name myEnvName --file spec-list.txt  # select a name\n</code></pre>"},{"location":"software/CUDA.html","title":"CUDA","text":""},{"location":"software/CUDA.html#description","title":"Description","text":"<p>Libraries for parallel programm on NVIDIA GPUs. </p>"},{"location":"software/CUDA.html#run-only","title":"run only","text":"<p>If you only want to use the CUDA with your already compiled application you only need to load: <pre><code>module load CUDA\n</code></pre></p>"},{"location":"software/CUDA.html#compile-applications","title":"compile applications","text":"<p>CUDA has restriction in supported compilers. E.g. CUDA/10.1.243 is not compatible with GCC/9.3.9 We suggest to load complete toolchains, e.g.  <pre><code>module load fosscuda\n</code></pre> This provides beside CUDA and GCC also OpenMPI, OpenBLAS, FFTW, ScaLAPACK and others. </p>"},{"location":"software/EasyBuild.html","title":"EasyBuild","text":""},{"location":"software/EasyBuild.html#description","title":"Description","text":"<p>EasyBuild can install software packages including the related modules. The location will be controlled using our modules, e.g. the <code>Workspace</code> module, see Installing Custom Software. On top of the usual EasyBuild framework we added some extensions which allows you to build for specific architectures or a generic software stack in your user/group space.  Therefore, use the <code>eb</code> command to search and try and the <code>eb-install-all</code> command to install the package. </p> <p>The following steps need are necessary:</p> <ul> <li>load modules</li> <li>find the package specification</li> <li>decide the desired software stack</li> <li>run EasyBuild installation using <code>eb-install-all</code></li> </ul>"},{"location":"software/EasyBuild.html#modules","title":"Modules","text":"<p>Depending if you want to install the package in user or a group space you need to load the related module and the <code>EasyBuild</code> module, e.g.:</p> <pre><code>module load Workspace  ### if you want to install into your HOME use module load Workspace_Home\nmodule load EasyBuild\n</code></pre> <p>Therewith, our EasyBuild tools and EasyBuild itself are available. </p> <p>Note</p> <p>Specify the WorkspaceID if necessary when loading the Workspace module. See module instructions</p>"},{"location":"software/EasyBuild.html#package-specification","title":"Package Specification","text":"<p>EasyBuild has a large repository of available packages in different versions.  You can use these specifications as is or copy/download and modify the EasyConfigs (see below).</p> <p>Available packages can be searched using the following command, here for the gatk package</p> <pre><code>eb --search gatk\n[...]\neb --search gatk\n== found valid index for /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs, so using it...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-1.0.5083.eb\n...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.1.2-Java-1.8.eb\n...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.12.0-foss-2018b-Python-3.6.6.eb\n...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.4.0-intel-2018a-Python-3.6.4.eb\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.5.1-foss-2018a-Python-3.6.4.eb\n...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.8.1-foss-2018b-Python-2.7.15.eb\n...\n * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb\n</code></pre> <p>As shown above there are different versions of GATK and for different toolchains available (<code>foss</code>, <code>intel</code>, <code>GCCcore</code>). </p> <p>Select one, here <code>GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb</code> is chosen.</p> <p>Alternatively, the packages can be listed or selected using the package name and target toolchain and version:</p> <pre><code>eb --software-name=GATK --toolchain-name=GCC --toolchain-version=9.3.0\n</code></pre> <p>You can list all dependencies using:</p> <pre><code>eb -Dr /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb\n...\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs\n * [ ] $CFGS/j/Java/Java-1.8.0_281.eb (module: Java/1.8.0_281)\n... \n * [x] $CFGS/p/Python/Python-2.7.18-GCCcore-9.3.0.eb (module: Python/2.7.18-GCCcore-9.3.0)\n * [ ] $CFGS/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb (module: GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8)\n</code></pre> <p>Dependencies marked with <code>x</code> are already installed, the other dependencies will be installed if using the robot option.</p> <p>Additional options, e.g. for selecting a specific software version can be found using <code>eb --help</code>.</p>"},{"location":"software/EasyBuild.html#using-easyconfig-files","title":"Using EasyConfig files","text":"<p>You can use the directly selected EasyConfig or if necessary copy and adapt it. EasyConfig files are text files specifying the software version, toolchain version, dependencies, compile arguments and more. If you need more information see EasyBuild documentation, and if necessary ask our support team for assistance.</p>"},{"location":"software/EasyBuild.html#meta-module-and-toolchains","title":"Meta module and Toolchains","text":"<p>Modules specify related dependencies, which gets loaded with that module. These dependencies may have further dependencies.</p> <p>The chain of dependencies is called toolchain. For example:</p> <ul> <li><code>GCC</code> consists of <code>GCCcore</code> and <code>binutils</code></li> <li><code>gompi</code> consists of <code>GCC</code> and <code>OpenMPI</code></li> <li><code>foss</code> consists of <code>gompi</code>, <code>OpenBLAS</code>, <code>FFTW</code> and <code>ScaLAPACK</code></li> </ul> <p>Within a toolchain the versions of the utilized libraries should be consistent. Thus, building a new package with <code>foss/2020b</code> and <code>PyTorch</code> should rely on a PyTorch version build with the same versions of the underlying libraries. Thus e.g. <code>PyTorch-1.9.0-foss-2020b.eb</code> is also build with <code>foss/2020b</code> as well as the <code>Python/3.8.6</code>. The latter one is build with <code>GCCcore/10.2.0</code> which is part of <code>foss/2020b</code>. </p>"},{"location":"software/EasyBuild.html#selecting-a-software-stack","title":"Selecting a software stack","text":"<p>Depending on the package and its target usage one or more software stacks should be selected. Therefore, the installation command starts with one for the following command:</p> <ul> <li>all architectural software stacks: <code>eb-install-all</code></li> <li>a specific architectural software stack (e.g. only targeting Broadwell nodes): <code>eb-install-all --archs='broadwell'</code> OR</li> </ul>"},{"location":"software/EasyBuild.html#installation","title":"Installation","text":"<p>After selecting the package installation recipe and the target software stack, the installation process can be submitted.  With the following commands, SLURM job files will be created, and submitted to the desired compute nodes. There the packages are build and module files created. The general syntax is:  <pre><code>eb-install-all [options] [easybuild options] &lt;easyconfig&gt;.eb\n</code></pre> Additional SLURM arguments can be selected using the <code>--slurm-args</code> option, e.g. <code>--slurm-args='--account=xyz --time=00:10:00 --cpus-per-task'</code>. If specific architectures should be selected use e.g. <code>--arch='broadwell ivy'</code>. After this options, EasyBuild arguments can be provided without prefix, e.g. <code>--robot</code>. </p> <p>Few examples:</p> <ul> <li>for a selected or custom EasyConfig and all missing dependencies in all architectural software stacks (here we go with the above selected GATK): <pre><code>eb-install-all --robot GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb\n</code></pre></li> <li>only in the Broadwell and Login software stack installing FFTW in GCC toolchain (newest version): <pre><code>eb-install-all --robot --archs='login broadwell' --software-name=FFTW --toolchain-name=GCC\n</code></pre></li> </ul> <p>This will need time to get scheduled and processed.  The job output is presented in the <code>eb_out.*</code> files, one for each architecture. </p> <p>If the build could not be finished in the default time of 1h, the walltime can be extended using:</p> <pre><code>eb-install-all --robot --slurm-args='--time=05:00:00' ...\n</code></pre> <p>Note</p> <p>Please check the end of the out file for the COMPLETED: Installation ended successfully statement.</p> <p>When finished you (and your collaborators) should be able to use use the software, by just loading the user/workspace related module and the module for the installed package. </p>"},{"location":"software/EasyBuild.html#adapting-easyconfigs","title":"Adapting EasyConfigs","text":"<p>in the following description and example we update an existing old EasyConfig for newer versions. In our case we want to update the version of Relion, the toolchain, and dependent libraries it is build with. </p> <ul> <li> <p>setup EasyBuild environment <pre><code>module load EasyBuild\nmodule load Workspace   ### OR Workspace_Home\n</code></pre></p> </li> <li> <p>find a suitable easyconfig <pre><code>$ eb --search Relion \n</code></pre> alternatively you may find easyconfigs online, e.g. https://github.com/easybuilders/easybuild-easyconfigs</p> </li> <li> <p>copy the easyconfig into a working directory (here <code>.</code>) <pre><code>$ cp $EBROOTEASYBUILD/easybuild/easyconfigs/r/RELION/RELION-3.0.4-foss-2017b.eb .\n</code></pre></p> </li> <li> <p>rename to the targeted versions (here newer relion, newer toolchain) <pre><code>$ mv RELION-3.0.4-foss-2017b.eb RELION-3.1.2-foss-2020b.eb\n</code></pre></p> </li> <li> <p>find the new versions of toolchain and libraries</p> <ul> <li>all installed version of a package can be listed using <code>module avail package</code>, e.g. <code>module avail foss</code></li> <li>available easyconfigs of non-installed packages can be listed using <code>eb --search package</code>. If there is a targeted version available, you can just define that dependency version in the above easyconfig and EasyBuild will find and use it. </li> </ul> </li> <li> <p>update the versions settings in the file</p> <ul> <li>package version, the toolchain version, and all related libraries</li> <li>Keep in mind that toolchain versions need to match (see toolchains above) <pre><code>easyblock = 'CMakeMake'\n\nname = 'RELION'\nversion = '3.1.2'                            #### The Relion version was '3.0.4' before\n\nhomepage = 'http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page'\ndescription = \"\"\"RELION (for REgularised LIkelihood OptimisatioN, pronounce rely-on) is a stand-alone computer\n program that employs an empirical Bayesian approach to refinement of (multiple) 3D reconstructions or 2D class\n averages in electron cryo-microscopy (cryo-EM).\"\"\"\n\ntoolchain = {'name': 'foss', 'version': '2020b'}   ### the foss toolchain version was 2020b before\ntoolchainopts = {'openmp': True}\n\nsource_urls = ['https://github.com/3dem/relion/archive']\nsources = ['%(version)s.tar.gz']\nchecksums = ['2580d66088923a644bc7d3b02efd154b775a3ec3d010426f382bb3be5db9c98b']\n\nbuilddependencies = [('CMake', '3.18.4')]    ### was 3.9.5\n\ndependencies = [\n    ('X11', '20201008'),                     ### was 20171023\n    ('FLTK', '1.3.5'),                       ### was 1.3.4\n    ('LibTIFF', '4.1.0'),                    ### 4.0.9\n    ('tbb', '2020.3'),                       ### 2018_U5\n]\n\nconfigopts = \"-DCMAKE_SHARED_LINKER_FLAGS='-lpthread'  -DMPI_INCLUDE_PATH=$EBROOTOPENMPI/include \"\nconfigopts += \"-DMPI_C_COMPILER=$EBROOTOPENMPI/bin/mpicc -DMPI_CXX_COMPILER=$EBROOTOPENMPI/bin/mpicxx \"\nconfigopts += \"-DCUDA=OFF -DCudaTexture=OFF \"\nconfigopts += \"-DALTCPU=ON -DFORCE_OWN_TBB=OFF \"\n\nsanity_check_paths = {\n    'files': ['bin/relion'],\n    'dirs': []\n}\n\nmoduleclass = 'bio'\n</code></pre></li> </ul> </li> <li> <p>update the checksum (if package version is changed) The downloaded source packages are typically checked with SHA256 checksums. When we change to a different source code versio, the checksum changes too. And need to be updated. <pre><code>$ eb --force --inject-checksums sha256 RELION-3.1.2-foss-2020b.eb\n</code></pre></p> </li> <li> <p>build the new package as described in Installation above, e.g. <pre><code>$ eb-install-all --robot RELION-3.1.2-foss-2020b.eb\n</code></pre></p> </li> </ul>"},{"location":"software/EasyBuild.html#tips-and-tricks","title":"Tips and tricks","text":"<p>Even if EasyBuild tries to simplify the installation process, not always EasyConfigs are Build without issues. There can be several types of issues. Starting form issues in finding exiting packages up to compilation issues. </p>"},{"location":"software/EasyBuild.html#more-information","title":"More information","text":"<p>In the EasyBuild output <code>eb_out.*</code> files are issues summarized. Often more details are required. There are more detailed log files created in the temporary directory.  On the compute nodes they are deleted at the end of the job, but on the login node (epyc2) they are kept. The location is mentioned near the end of the output and typically is after <code>Results of the build can be found in the log file</code>.</p>"},{"location":"software/EasyBuild.html#lock-directories","title":"Lock directories","text":"<p>EasyBuild has a procedure to prevent building the same package (same version, same software stack) using lock files. If <code>eb-install-*</code> crashes due to time limit, the lock files are not removed properly. Therewith the next time you start <code>eb-install-*</code> for that package a message like will be presented at the end of the output:</p> <p><pre><code>ERROR: Build of /path/to/easyconfig.eb failed (err: 'build failed (first 300 chars): Lock /path/to/.locks/packageVersion.lock already exists, aborting!')\n</code></pre> In that moment the lock file should be already removed and the process can finally be started successfully again. </p>"},{"location":"software/EasyBuild.html#hidden-modules","title":"Hidden Modules","text":"<p>Sometimes packages are not defined consistently. On UBELIX many packages are provided as hidden modules. This keeps the list nice and tidy. Nevertheless, if a package (or worse one of its dependency) is looking for an existing packages, but it is not mentioned to be hidden, it will not find and need to rebuild again. </p> <p>Hidden packages can be searched using <code>module --show-hidden avail &lt;PackageXYZ&gt;</code>. If existing as hidden and the target package or dependency does not define it as hidden, EasyBuild can be advised to treat it as hidden using the <code>--hide-deps</code> option. E.g. for binutils, gettext and Mesa, the command would look like:</p> <pre><code>$ eb-install-all --hide-deps=binutils,gettext,Mesa &lt;PackageXYZ&gt;\n</code></pre>"},{"location":"software/EasyBuild.html#directly-on-the-compute-node","title":"Directly on the compute node","text":"<p>The eb-install-all tool builds the packages directly on a compute node. The node type can be limited/specified by using the option <code>--archs=&lt;type&gt;</code> (see <code>eb-install-all --help</code>). </p> <p>If this fails, an investigation step may be running directly on the node, without more control of the setup, e.g. build directories. Therefore, EasyBuild can be started directly in a session on the compute node. First, an interactive session is established on the compute node. For example building Relion in the <code>$HOME</code> on an epyc2 node using a local copy of the EasyConfig file:</p> <pre><code>$ srun --pty --partition epyc2 bash\n$ module load Workspace_Home EasyBuild\n$ eb --tmpdir=$TMPDIR --robot --hide-deps=binutils,gettext,Mesa RELION-3.1.3-fosscuda-2020b.eb\n</code></pre> <p>This may also be used when compiling on a specific GPU architecture. </p>"},{"location":"software/JupyterLab.html","title":"Jupyter Lab","text":""},{"location":"software/JupyterLab.html#description","title":"Description","text":"<p>Some useful information on using Jupyter Lab on UBELIX compute nodes.  </p> <p>IMPORTANT: in the following we show how to start the server on a compute node.  Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources.  Please quit your session as soon as you don\u2019t use it anymore, even for a lunch break. Your notebook will maintain all you input/output.</p>"},{"location":"software/JupyterLab.html#overview","title":"Overview","text":"<p>On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks.  JupyterLab is a single-user web-based Notebook server, running in the user space.  JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads.  After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly.  On this page we describe:</p> <ul> <li>Launch JupyterLab<ul> <li>Connect to UBELIX and establishing SSH port forwarding </li> <li>SSH with port forwarding</li> <li>Launch the JupyterLab server</li> <li>Launch JupyterLab in your local browser</li> </ul> </li> <li>Kernels</li> <li>Packages</li> </ul>"},{"location":"software/JupyterLab.html#launch-jupyterlab","title":"Launch JupyterLab","text":"<p>Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to.  This port numbers need to be between 2000 and 65000 and need to be unique on the present machine.  The default port for JupyterLab is 8888, but only one user can use this at a time.</p> <p>To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 (please select another number).</p>"},{"location":"software/JupyterLab.html#passwordless-ssh-within-the-hpcs","title":"Passwordless SSH within the HPCs","text":"<p>Please verify that you created and registered a SSH key within UBLEIX. If you can perform the following command without entering your password your are ready to go: <pre><code>ssh localhost\n</code></pre> otherwise create and register a new key on a login node. <pre><code>ssh-keygen -t rsa -b 4096\n# without passphrase\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 640 .ssh/authorized_keys\n</code></pre> This creates a ssh key pair on the login node, which than can be used for any ssh sessions within UBELIX. </p>"},{"location":"software/JupyterLab.html#setup-ssh-with-port-forwarding","title":"Setup SSH with port forwarding","text":"<p>First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. This ports are numbers between 2000 and 65000, which needs to be unique on the both sides. The default port for JupyterLab is 8888, but only one user can use this at a time. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal.</p> <p>The <code>ssh</code> command from your local machine to the ubelix login node  needs to be called with following arguments:</p> <p><pre><code>ssh -L 15051:localhost:15051 &lt;user&gt;@submit03.unibe.ch\n</code></pre> If configured in your <code>.ssh/config</code>, you can also use the alias instead of the full name for UBELIX. Where <code>&lt;user&gt;</code> is you Campus Account username.</p> <p>Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used.</p>"},{"location":"software/JupyterLab.html#launch-the-jupyterlab-server","title":"Launch the JupyterLab server","text":"<p>On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels):</p> <pre><code>module load Anaconda3\n</code></pre> <p>A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. </p> <p><pre><code>jupyter-compute 15051 --time=00:45:00  # please change port number\n</code></pre> This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to:</p> <pre><code>...\n[C 21:43:35.291 LabApp]\n\n    To access the notebook, open this file in a browser:\n        file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html\n    Or copy and paste one of these URLs:\n        http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073\n     or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073\n</code></pre> <p>The last line needs to be copied in your local browser.</p> <p>Attention</p> <p>do not use <code>Ctrl + C</code> for copying the link, this will abort the server process and kill your job. </p> <p>QOS</p> <p>the <code>jupyter-compute</code> tool uses an special Slurm Quality of Service (QoS), which should reduce queuing times for interactive jobs.  Since interactive jobs are considered to be finished within less than a working day, the walltime limit cannot exceed 8h (default run time is 6h, afterwards you are expected to have a break ;) ).  You can disable that qos using the option <code>--no-qos</code>, but please release the resources as soon as you are not actively working with the resources anymore.</p>"},{"location":"software/JupyterLab.html#jupyterlab-in-your-local-browser","title":"JupyterLab in your local browser","text":"<p>The full address on the last line (starting with the 127.0.0.1) of the Jupyter Server statement including the token needs to be copied into your browser on your local machine.  After initializing Jupyter Lab you should see a page similar to:</p> <p></p> <p>Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python:</p> <pre><code>import socket\nprint(socket.gethostname())\n</code></pre> <p>IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. </p> <p>Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session.</p>"},{"location":"software/JupyterLab.html#jupyterlab-with-multiple-cpu-cores","title":"JupyterLab with multiple CPU cores","text":"<p>More resources can be requested, e.g. by using:</p> <p><pre><code>jupyter-compute 15051 --ntasks 1 --time=01:00:00 --cpus-per-task 5 --mem 512MB\n</code></pre> Where 5 cores are requested for threading and a total memory of 3GB.  Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node.  Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected).</p> <p>OR requesting GPU resources on a node with a NVIDIA graphics card: <pre><code>jupyter-compute 15051 --ntasks 1 --time=01:00:00 --partition=gpu --gres=gpu:gtx1080ti:1\n</code></pre></p>"},{"location":"software/JupyterLab.html#kernels","title":"Kernels","text":"<p>The following JupyterLab kernel are installed:</p> <ul> <li>Python3</li> <li>R</li> </ul>"},{"location":"software/JupyterLab.html#r","title":"R","text":"<p>verify that the module IRkernel is loaded</p> <pre><code>module load IRkernel\n</code></pre>"},{"location":"software/JupyterLab.html#packages","title":"Packages","text":"<p>There are a long list of default packages provided by Anaconda3 (list all using <code>!pip list</code>) and R (list using <code>installed.packages(.Library)</code>, note the list is shortened). </p> <p>Furthermore, you can install additional packages in Python using <code>pip install --user</code> or in R using <code>install.packages(\"sampling\")</code>. </p>"},{"location":"software/ParaView.html","title":"ParaView","text":""},{"location":"software/ParaView.html#description","title":"Description","text":"<p>This article note specific information for launching ParaView Server on compute nodes and connect local ParaView with it. </p>"},{"location":"software/ParaView.html#prerequisites","title":"Prerequisites","text":"<p>Passwordless SSH need to be activated as well as connection established with Port forwarding. </p> <p>Furthermore, the local (on your local machine) ParaView version need to match the version loaded on the HPCs. </p>"},{"location":"software/ParaView.html#launch-paraview-server","title":"Launch ParaView server","text":"<p>First, as mentioned, establish a SSH session with Port forwarding, another port in the range [2000-65000] should be selected: </p> <pre><code>ssh -Y -L 15051:localhost:15051 submit03.unibe.ch\n</code></pre> <p>Then load the modules:</p> <pre><code>module load ParaView\n</code></pre> <p>The ParaView version can be displayed using:</p> <pre><code>module list ParaView\n\nCurrently Loaded Modules Matching: ParaView\n  1) ParaView/5.8.1-foss-2020b-mpi\n</code></pre> <p>Thus, in this example ParaView 5.8.1 need to be used in the local machine. </p> <p>To start the ParaView server on a compute node you can use:</p> <pre><code>pvserver-parallel 15051   ### use your selected port number\n</code></pre> <p>This submits a job with <code>1 core</code> for <code>1h</code> in the <code>epyc2</code> partition.  The tool prints a reminder to stop the job if not required anymore and shows the queueing information regularly:</p> <pre><code>ParaView remote Server submitted to compute nodes\n  when finished please kill the server using:\n     scancel 2394231\njob 2394231 status:\n   JOBID  PARTITION    STATE               START_TIME\n 2394231      epyc2  PENDING                      N/A\npvserver ready to connect on port 15051.\nWhen finished, please stop ParaView Server using\n scancel 2394231\n</code></pre> <p>Please cancel your job with <code>scancel $JOBID</code> or all your running jobs using <code>scancel -u $USER</code> if not needed anymore. </p> <p>Additional resources can be requested by <code>--slurm-args=\"\"</code> option with the desired slurm options, e.g. <code>3 cores</code> for <code>20 min</code>:</p> <pre><code>pvserver-parallel 15051 --slurm-args=\"--cpus-per-task=3 --time=00:20:00\" \n</code></pre> <p>In addition ParaView arguments can be added without any prefix. </p>"},{"location":"software/ParaView.html#connect-local-client","title":"Connect local client","text":"<p>Finally, the client on your local machine can connect using <code>localhost</code> and the selected port, here <code>15051</code>. E.g. using <code>pvpython</code>:</p> <pre><code>pvpython\n\nWARNING: Python 2.7 is not recommended.\nThis version is included in macOS for compatibility with legacy software.\nFuture versions of macOS will not include Python 2.7.\nInstead, it is recommended that you transition to using 'python3' from within Terminal.\n\nPython 2.7.16 (default, May  8 2021, 11:48:02)\n[GCC Apple LLVM 12.0.5 (clang-1205.0.19.59.6) [+internal-os, ptrauth-isa=deploy on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from paraview.simple import *\n&gt;&gt;&gt; Connect(\"localhost\", 15051)\nConnection (cs://localhost:15051) [2]\n</code></pre>"},{"location":"software/bzip2.html","title":"Parallel BZIP2","text":""},{"location":"software/bzip2.html#description","title":"Description","text":"<p>Data need to be packed and compressed for archiveing or transfer. There are multiple tools available like tar and gzip, bzip. Pbzip2 is a paeallel implementation of bzip2. For general information see bzip2 and pbzip2</p>"},{"location":"software/bzip2.html#availability","title":"Availability","text":"<p>The tool is available on all nodes without loading any module. </p>"},{"location":"software/bzip2.html#usage","title":"Usage","text":"<p>Parallel packing and compressing can be performed on a compute node using <code>tar</code> with a specified number of threads</p> <p>As an example, a file or directory <code>/data/to/pack</code> can be packed and compressed into a file <code>/packed/file.tar.bz2</code> using the job script:</p> <pre><code>#SBATCH --job-name=\"pbzip2\"\n#SBATCH --time=01:00:00\n#SBATCH --mem-per-cpu=2G\n## For parallel jobs with 8 cores\n#SBATCH --cpus-per-task=8           ## select the amount of cores required\n\nsource=\"data/to/pack\"               ## specify your data to compress\ntarget=\"/packed/file.tar.bz2\"       ## specify directory and filename\n\n# archive dir data_unibe to a tar file and compress it using pbzip2\nsrun tar -cS $source | pbzip2 -p$SLURM_CPUS_PER_TASK &gt; $target\n\n# Generate a sha256 fingerprint, to later check the integrity \nsha256sum $target &gt; ${target}.sha256sum\n</code></pre>"},{"location":"software/hpc-modules.html","title":"HPC software environment","text":""},{"location":"software/hpc-modules.html#description","title":"Description","text":"<p>With the operation system, a list of basic tools (mainly command line tools) are provided, including editors, file analyzing and manipulation tools, packing and transfer tools etc. These commands are accessible all the time. Other software packages (libraries and applications) are available through software modules. Using modules, a lot of different packages even in different versions can be provided, without unwanted influences.</p>"},{"location":"software/hpc-modules.html#basic-concept","title":"Basic concept","text":"<p>Many Linux settings are in environment variables. These include search paths for applications (<code>$PATH</code>) and libraries (<code>$LD_LIBRARY_PATH</code>). Adding or removing a directory to these lists, provides access or remove access to additional software. </p> <p>Our software modules are an user friendly way to search and manage software packages without dealing with complicated directory names.</p> <p>In general every software package has its related module. When loading the module the software package and its dependencies get accessible. </p> <p>But let\u2019s do it step by step.</p>"},{"location":"software/hpc-modules.html#find-available-modules","title":"Find available Modules","text":"<p>You can search for an packages or module containing a specific string using <code>module spider</code>, e.g. to find all versions of GCC: <pre><code>module spider GCC\n</code></pre></p> <p>You can list all currently available packages using: <pre><code>module avail\n</code></pre> Beware, this list is very long! It may be more useful to use <code>module spider</code> instead.</p>"},{"location":"software/hpc-modules.html#loadadd-a-modules","title":"Load/Add a Modules","text":"<p>Loading a module will provide access to the software package and it will additionally load all required dependencies.</p> <pre><code>module load OpenMPI/3.1.3-GCC-8.2.0-2.31.1\n</code></pre> <p>or equivalently:</p> <pre><code>$ module add OpenMPI/3.1.3-GCC-8.2.0-2.31.1\n</code></pre> <p>This will provide access to OpenMPI, but also to GCC and other libraries. With this principle it is verified that the library versions are loaded, which were used to build the package. </p>"},{"location":"software/hpc-modules.html#list-all-loaded-modules","title":"List all Loaded Modules","text":"<p>You can list the currently loaded modules using </p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) GCCcore/8.2.0\n  2) zlib/.1.2.11-GCCcore-8.2.0       (H)\n  3) binutils/.2.31.1-GCCcore-8.2.0   (H)\n  4) GCC/8.2.0-2.31.1\n  5) numactl/2.0.12-GCCcore-8.2.0\n  6) XZ/.5.2.4-GCCcore-8.2.0          (H)\n  7) libxml2/.2.9.8-GCCcore-8.2.0     (H)\n  8) libpciaccess/.0.14-GCCcore-8.2.0 (H)\n  9) hwloc/1.11.11-GCCcore-8.2.0\n 10) OpenMPI/3.1.3-GCC-8.2.0-2.31.1\n\n  Where:\n   H:  Hidden Module\n</code></pre>"},{"location":"software/hpc-modules.html#toolchains-version-consistency","title":"Toolchains / version consistency","text":"<p>When loading multiple modules it is strongly suggested to stay within one toolchain version.</p> <p>A toolchain is a set of modules all building on top of each other. The related packages and versions can be listed with the command above.  There are two basic toolchains families: </p> Toolchain packages foss GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK intel Intel compiler, (GCC required), MKL, Intel MPI <p>Furthermore, toolchains are provided in different versions and updated regularly.</p> <p>The two main toolchains foss and intel are subdivided into sub-toolchains that belong to the same family:</p> Family Subtoolchains foss GCC, gompi intel iompi, iomkl <p>When loading multiple packages, they should be based on the same toolchain (or at least the same toolchain family) and the same version.</p> <p>In the following are two examples where <code>netCDF</code> and <code>FFTW</code> should be loaded, but the based toolchains and versions do not match. </p>"},{"location":"software/hpc-modules.html#problematic-example-different-toolchains","title":"Problematic example: different toolchains","text":"<p><pre><code>module load netCDF/4.7.3-gompi-2019b\nmodule load FFTW/3.3.8-intel-2019b\n</code></pre> NetCDF is loaded in the <code>gompi</code> toolchain, including OpenMPI. <code>FFTW</code> is loaded in the <code>intel</code> toolchain, including <code>iimpi</code> (IntelMPI). Beside other, now two different MPI implementation are loaded. We cannot verify that each package uses the MPI library it is build with. In best case this leads to errors during run time. In worse case results are incorrect. </p>"},{"location":"software/hpc-modules.html#problematic-example-toolchain-versions","title":"Problematic example: toolchain versions","text":"<pre><code>$ module load netCDF/4.7.4-gompi-2020b\n$ module load FFTW/3.3.7-gompi-2018a\n\nThe following have been reloaded with a version change:\n  1) GCC/10.2.0 =&gt; GCC/6.4.0-2.28                                      5) gompi/2020b =&gt; gompi/2018a\n  2) GCCcore/10.2.0 =&gt; GCCcore/6.4.0                                   6) hwloc/2.2.0-GCCcore-10.2.0 =&gt; hwloc/1.11.8-GCCcore-6.4.0\n  3) OpenMPI/4.0.5-GCC-10.2.0 =&gt; OpenMPI/2.1.2-GCC-6.4.0-2.28          7) numactl/2.0.13-GCCcore-10.2.0 =&gt; numactl/2.0.11-GCCcore-6.4.0\n  4) binutils/.2.35-GCCcore-10.2.0 =&gt; binutils/.2.28-GCCcore-6.4.0\n</code></pre> <p>LMOD already notes the version changes. In this case, netCDF build with 2020a version will utilize libraries of 2018a. If interfaces have changed, errors may occur.</p>"},{"location":"software/hpc-modules.html#unloadremove-modules","title":"Unload/remove Modules","text":"<p>To prevent unwanted influences between software packages, it is advisable to keep the loaded software stack small and clean. </p> <p>Certain modules can be unloaded using:</p> <p><pre><code>$ module unload OpenMPI/3.1.3-GCC-8.2.0-2.31.1\n</code></pre> or equivalently:</p> <pre><code>$ module rm OpenMPI/3.1.3-GCC-8.2.0-2.31.1\n</code></pre> <p>This will only unload the specified module.  Dependencies stay loaded, which were automatically loaded when loading the specified modulefile.  A clean environment can be obtained with purge (see below).</p>"},{"location":"software/hpc-modules.html#purge-all-modules","title":"Purge all Modules","text":"<p>All currently loaded modules an be unloaded using:</p> <pre><code>$ module purge\n</code></pre>"},{"location":"software/hpc-modules.html#show-information","title":"Show information","text":"<p>Most modules provide a short description which software package they contain and a link to the homepage, as well as information about the changes of environment undertaken. From short to full detail:</p> <pre><code>$ module whatis OpenMPI\n</code></pre> <pre><code>$ module help OpenMPI\n</code></pre> <pre><code>$ module show OpenMPI\n</code></pre>"},{"location":"software/hpc-modules.html#environment-definitions","title":"Environment definitions","text":"<p>Working on different projects or with different types of task may require to load different sets of modules. </p> <p>There are two ways of providing a user environment setups, e.g. for development, production, post processing etc., a custom module (also for Workspaces) or a module user collections (per user). </p> <p>Adding module load into <code>.bashrc</code> may lead to issues. If you diverge from this \u201cdefault\u201d environment and additionally load conflicting modules, e.g. form another toolchain. </p>"},{"location":"software/hpc-modules.html#module-user-collections","title":"Module User Collections","text":"<p>Sets of modules can be stored and reloaded in LMOD using the \u201cuser collection\u201d feature. </p> <p>As an example, a set of module for development consiting of SciPy-bundle and netCDF should be stored under the name devel. Further module lists can be managed in the same way (here lists for test and prod already exist). </p> <p><pre><code>$ module load SciPy-bundle netCDF\n$ module save devel\nSaved current collection of modules to: \"devel\"\n$ module savelist\nNamed collection list :\n  1) devel  2) test  3) prod\n</code></pre> Therewith the set of modules can be loaded using:</p> <pre><code>$ module restore devel\n</code></pre> <p>This will unload all other previously loaded modules beforehand and then load the set specified in the collection. </p> <p>This method is preferred against defining/loading a set of modules in Bash configuration like .bashrc.</p> <p>More information can be found in the LMOD documentation</p>"},{"location":"software/hpc-modules.html#custom-modules","title":"Custom Modules","text":"<p>A so called \u201cMeta Module\u201d can be used to specify a set of modules. Additionally, environment variables can be defined. These custom modules can be placed in the custom software stack, e.g. in a Workspace. Thus default working environments could be defined for the users that workspace. You may want to decide if you want to specify the environment with all versions of the modules (advisable), or always the latest versions (no version specified). </p> <p>The modules can be placed into <code>$WORKSPACE/modulefiles/$NAME/$VERSION.lua</code>.</p>"},{"location":"software/hpc-modules.html#example-lua-module-for-development-environment","title":"Example: Lua module for development environment","text":"<p>Here an environment is defined using foss and netCDF of version 2021a. Additionally an environment variable <code>WS_ENV</code> is set to <code>devel</code>.  Therefore, a file <code>$WORKSPACE/modulefiles/WS_devel/2021a.lua</code> is created with the following content:</p> <pre><code>whatis([==[Description: Workspace XXX development environment]==])\nif not ( isloaded(\"foss/2021a\") ) then\n    load(\"foss/2021a\")\nend\nif not ( isloaded(\"netCDF/4.8.0-gompi-2021a\") ) then\n    load(\"netCDF/4.8.0-gompi-2021a\")\nend\nsetenv(\"WS_ENV\", \"devel\")\nsetenv(\"CFLAGS\", \"-DNDEBUG\")\n</code></pre> <p>The all workspace members can load this environment using:</p> <pre><code>module purge   ### better start with a clean environment\nmodule load Workspace WS_devel\n</code></pre>"},{"location":"software/installing-custom-software.html","title":"Installing Custom Software","text":""},{"location":"software/installing-custom-software.html#description","title":"Description","text":"<p>UBELIX comes with a plethora of software pre-installed.  And there are tools provided installing additional packages in the user/group space. The present CustomRepo and Workspace modules provide easy access even for multiple versions of the same Software package. </p> <p>This article describes a procedure for installing custom software stacks in your user/group space. An EasyBuild and a manual approach is presented.</p> <p>Note</p> <p>You cannot use the package managers apt, dnf or yum for this, since these commands require root privileges to install software system wide. Instead you have to compile and install the software yourself. </p> <p>Note</p> <p>If you know that some missing software could be of general interest for a wide community on our machines, you can ask us to install the software system wide.</p> <p>Note</p> <p>If you need further assistance in installing your software packages or optimizing for our machine architecture, get in touch with our support team.</p> <p>The LMOD module system allows to enable software package by package. Thus, influences between different packages can be minimized. It even allows to have multiple versions of the same software product installed side by side. </p> <p>When possible we use EasyBuild to provision software packages.  EasyBuild is a software installation framework. Installation procedures are defined in so called EasyConfigs, including the location of the sources, dependencies, its versions, used environments, compile arguments, and more.  These EasyConfigs are publicly available on EasyBuild github repository and can be downloaded used and if necessary adapted, e.g. for new versions.  </p> <p>There are modules providing access to your user/group software stacks and assisting you with building packages into them. </p>"},{"location":"software/installing-custom-software.html#building-software-packages","title":"Building Software packages","text":"<p>There are mainly two options to build a software package and its module:</p> <ul> <li>using EasyBuild, with an existing, an modified or a newly created EasyConfig</li> <li>performing manual installation, and creating a module file</li> </ul> <p>Both methods are described in more details. Additionally, with the <code>Workspace</code> and <code>Workspace_Home</code> module such software stacks can be created in your user/group space.</p>"},{"location":"software/installing-custom-software.html#easybuild","title":"EasyBuild","text":"<p>For detailed instructions read the EasyBuild article.</p> <p>If you are installing your own application you may want to consider to create an EasyConfig for it. Have a look in the EasyBuild documentation, examples on the EasyBuild github. And if necessary ask our support team for assistance. </p>"},{"location":"software/installing-custom-software.html#python-and-r","title":"Python and R","text":"<p>You can use the Python/R package managers to install additional packages into your HOME/Workspace directory. Please see the Anaconda/Python and R pages.</p>"},{"location":"software/installing-custom-software.html#manually-compiling","title":"Manually compiling","text":"<p>There are very different ways of manually installing software packages, starting from just using a compiler, having a makefile, up to complex build systems.  A few considerations need to kept in mind while building for our systems:</p> <ul> <li>Compilers: different compilers are available and can be loaded by modules. Toolchains bundle compiler with additionally libraries and tools, like MPI, FFTW, MKL. We advise to always use toolchains instead of compiler modules to eliminate a source of common errors. Furthermore, complex algorithms are optimised differently in the compilers. It is worthwhile to try and compare multiple toolchains. </li> <li>CPU architectures: since there are different CPU architectures available, applications should be build for the targeted architecture. Often significant performance improvements can be obtained compiling for the correct instruction sets. Therefore, launch your build processes on the targeted architecture (i.e., bdw or epyc2).</li> </ul> <p>One and probably the most used procedure is the GNU configure / make:</p> <pre><code>tar xzvf some-software-0.1.tar.gz\ncd some-software-0.1\n./configure --prefix=/path/to/target/some-software/0.1\nmake\nmake install\nmake clean\n</code></pre> <p>Note</p> <p>Consider creating an Easyconfig if you already have a well tested procedure, see EasyBuild. </p> <p><code>configure</code> is usually a complex shell script that gathers information about the system and prepares the compile process. With the <code>--prefix</code> option you can specify a base installation directory, where <code>make install</code> will install the files into subdirectories like <code>bin</code>, <code>lib</code>, <code>include</code>, etc. </p> <p>The make utility is what does the actual compiling and linking.  If, for example, some additional libraries are missing on the system or not found in the expected location, the command will exit immediately. </p> <p>Detailed documentation can be found on the GNU make documentation page</p>"},{"location":"software/matlab.html","title":"MATLAB","text":""},{"location":"software/matlab.html#description","title":"Description","text":"<p>UBELIX is always featuring the latest two (b)-releases of MATLAB.</p>"},{"location":"software/matlab.html#facts-about-matlab-on-ubelix","title":"Facts about MATLAB on UBELIX","text":"<ul> <li>It can run in parallel on one node, thanks to the Parallel Computing ToolBox</li> <li>It can take advantage of GPUs</li> <li>It cannot run on more than one node as we do not have the Distributed Computing Toolbox.</li> </ul>"},{"location":"software/matlab.html#running-matlab-on-the-compute-nodes","title":"Running MATLAB on the Compute Nodes","text":"<p>Submitting a MATLAB job to the cluster is very similar to submitting any other serial job. Lets try to run a simple MATLAB script which we will put in a file boxfilter.m</p> <p>boxfilter.m <pre><code>% Compute a local mean filter over a neighborhood of 11x11 pixels\n\n% Read image into workspace:\noriginal = imread('girlface.png');\n% Perform the mean filtering:\nfiltered = imboxfilt(original, 11);\n% Save the original and the filtered image side-by-side:\nimwrite([original, filtered],'comparison.png');\n</code></pre></p> <p>Now we need a submission script</p> <p>boxfilter.qsub <pre><code>!#/bin/bash\n#SBATCH -mail-user=foo@bar.unibe.ch\n#SBATCH --mail-type=end,fail\n#SBATCH --job-name=boxfilter\n#SBATCH --time=00:10:00\n#SBATCH --mem-per-cpu=2G\n\n# Load MATLAB form the environment modules\nmodule load MATLAB\n# Tell MATLAB to run our box filter.m file and exit\nmatlab -nodisplay -r \"boxfilter, exit\"\n</code></pre></p>"},{"location":"software/matlab.html#passing-arguments-to-a-m-file","title":"Passing Arguments to a m-File","text":"<p>There are several ways to provide input arguments in MATLAB.</p>"},{"location":"software/matlab.html#define-the-variables-before-running-the-script","title":"Define the Variables Before Running the Script","text":"<p>Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by:</p> <p>boxfilter.m</p> <pre><code>% Compute a local mean filter over a neighborhood of 11x11 pixels\n\n% Read image into workspace:\noriginal = imread(inputImg);\n% Perform the mean filtering:\nfiltered = imboxfilt(original, x);\n% Save the original and the filtered image side-by-side:\nimwrite([original, filtered],'comparison.png');\n</code></pre> <p>and then:</p> <p>boxfilter.qsub</p> <pre><code>!#/bin/bash\n(...)\n# Load MATLAB form the environment modules\nmodule load MATLAB\n# Tell MATLAB to run our box filter.m file and exit\nmatlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\"\n</code></pre>"},{"location":"software/matlab.html#advanced-topics","title":"Advanced Topics","text":""},{"location":"software/matlab.html#multithreading","title":"Multithreading","text":"<p>By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores.</p> <p>Disable Computational Multithreading</p> <p>If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB:</p> <pre><code>matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\"\n</code></pre> <p>Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB:</p> <pre><code>matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\"\n</code></pre> <p>Running MATLAB in Multithreaded Mode</p> <p>Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:</p>"},{"location":"software/matlab.html#infos-about-featured-matlab","title":"Infos about featured MATLAB","text":"<pre><code>-----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.11.0.1769968 (R2021b)\nMATLAB License Number: 40639324\nOperating System: Linux 3.10.0-1160.76.1.el7.x86_64 #1 SMP Wed Aug 10 16:21:17 UTC 2022 x86_64\nJava Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n-----------------------------------------------------------------------------------------------------\nMATLAB                                                Version 9.11        (R2021b)\nSimulink                                              Version 10.4        (R2021b)\n5G Toolbox                                            Version 2.3         (R2021b)\nAUTOSAR Blockset                                      Version 2.5         (R2021b)\nAerospace Blockset                                    Version 5.1         (R2021b)\nAerospace Toolbox                                     Version 4.1         (R2021b)\nAntenna Toolbox                                       Version 5.1         (R2021b)\nAudio Toolbox                                         Version 3.1         (R2021b)\nAutomated Driving Toolbox                             Version 3.4         (R2021b)\nBioinformatics Toolbox                                Version 4.15.2      (R2021b)\nCommunications Toolbox                                Version 7.6         (R2021b)\nComputer Vision Toolbox                               Version 10.1        (R2021b)\nControl System Toolbox                                Version 10.11       (R2021b)\nCurve Fitting Toolbox                                 Version 3.6         (R2021b)\nDDS Blockset                                          Version 1.1         (R2021b)\nDSP System Toolbox                                    Version 9.13        (R2021b)\nDatabase Toolbox                                      Version 10.2        (R2021b)\nDatafeed Toolbox                                      Version 6.1         (R2021b)\nDeep Learning HDL Toolbox                             Version 1.2         (R2021b)\nDeep Learning Toolbox                                 Version 14.3        (R2021b)\nEconometrics Toolbox                                  Version 5.7         (R2021b)\nEmbedded Coder                                        Version 7.7         (R2021b)\nFilter Design HDL Coder                               Version 3.1.10      (R2021b)\nFinancial Instruments Toolbox                         Version 3.3         (R2021b)\nFinancial Toolbox                                     Version 6.2         (R2021b)\nFixed-Point Designer                                  Version 7.3         (R2021b)\nFuzzy Logic Toolbox                                   Version 2.8.2       (R2021b)\nGPU Coder                                             Version 2.2         (R2021b)\nGlobal Optimization Toolbox                           Version 4.6         (R2021b)\nHDL Coder                                             Version 3.19        (R2021b)\nHDL Verifier                                          Version 6.4         (R2021b)\nImage Acquisition Toolbox                             Version 6.5         (R2021b)\nImage Processing Toolbox                              Version 11.4        (R2021b)\nInstrument Control Toolbox                            Version 4.5         (R2021b)\nLTE Toolbox                                           Version 3.6         (R2021b)\nLidar Toolbox                                         Version 2.0         (R2021b)\nMATLAB Coder                                          Version 5.3         (R2021b)\nMATLAB Compiler                                       Version 8.3         (R2021b)\nMATLAB Compiler SDK                                   Version 6.11        (R2021b)\nMATLAB Report Generator                               Version 5.11        (R2021b)\nMapping Toolbox                                       Version 5.2         (R2021b)\nMixed-Signal Blockset                                 Version 2.1         (R2021b)\nModel Predictive Control Toolbox                      Version 7.2         (R2021b)\nMotor Control Blockset                                Version 1.3         (R2021b)\nNavigation Toolbox                                    Version 2.1         (R2021b)\nOptimization Toolbox                                  Version 9.2         (R2021b)\nParallel Computing Toolbox                            Version 7.5         (R2021b)\nPartial Differential Equation Toolbox                 Version 3.7         (R2021b)\nPhased Array System Toolbox                           Version 4.6         (R2021b)\nPowertrain Blockset                                   Version 1.10        (R2021b)\nPredictive Maintenance Toolbox                        Version 2.4         (R2021b)\nRF Blockset                                           Version 8.2         (R2021b)\nRF PCB Toolbox                                        Version 1.0         (R2021b)\nRF Toolbox                                            Version 4.2         (R2021b)\nROS Toolbox                                           Version 1.4         (R2021b)\nRadar Toolbox                                         Version 1.1         (R2021b)\nReinforcement Learning Toolbox                        Version 2.1         (R2021b)\nRisk Management Toolbox                               Version 1.10        (R2021b)\nRobotics System Toolbox                               Version 3.4         (R2021b)\nRobust Control Toolbox                                Version 6.11        (R2021b)\nSatellite Communications Toolbox                      Version 1.1         (R2021b)\nSensor Fusion and Tracking Toolbox                    Version 2.2         (R2021b)\nSerDes Toolbox                                        Version 2.2         (R2021b)\nSignal Integrity Toolbox                              Version 1.0         (R2021b)\nSignal Processing Toolbox                             Version 8.7         (R2021b)\nSimBiology                                            Version 6.2         (R2021b)\nSimEvents                                             Version 5.11        (R2021b)\nSimscape                                              Version 5.2         (R2021b)\nSimscape Driveline                                    Version 3.4         (R2021b)\nSimscape Electrical                                   Version 7.6         (R2021b)\nSimscape Fluids                                       Version 3.3         (R2021b)\nSimscape Multibody                                    Version 7.4         (R2021b)\nSimulink 3D Animation                                 Version 9.3         (R2021b)\nSimulink Check                                        Version 5.2         (R2021b)\nSimulink Code Inspector                               Version 4.0         (R2021b)\nSimulink Coder                                        Version 9.6         (R2021b)\nSimulink Compiler                                     Version 1.3         (R2021b)\nSimulink Control Design                               Version 6.0         (R2021b)\nSimulink Coverage                                     Version 5.3         (R2021b)\nSimulink Design Optimization                          Version 3.10        (R2021b)\nSimulink Design Verifier                              Version 4.6         (R2021b)\nSimulink PLC Coder                                    Version 3.5         (R2021b)\nSimulink Report Generator                             Version 5.11        (R2021b)\nSimulink Requirements                                 Version 1.8         (R2021b)\nSimulink Test                                         Version 3.5         (R2021b)\nSoC Blockset                                          Version 1.5         (R2021b)\nStateflow                                             Version 10.5        (R2021b)\nStatistics and Machine Learning Toolbox               Version 12.2        (R2021b)\nSymbolic Math Toolbox                                 Version 9.0         (R2021b)\nSystem Composer                                       Version 2.1         (R2021b)\nSystem Identification Toolbox                         Version 9.15        (R2021b)\nText Analytics Toolbox                                Version 1.8         (R2021b)\nUAV Toolbox                                           Version 1.2         (R2021b)\nVehicle Dynamics Blockset                             Version 1.7         (R2021b)\nVehicle Network Toolbox                               Version 5.1         (R2021b)\nVision HDL Toolbox                                    Version 2.4         (R2021b)\nWLAN Toolbox                                          Version 3.3         (R2021b)\nWavelet Toolbox                                       Version 6.0         (R2021b)\nWireless HDL Toolbox                                  Version 2.3         (R2021b)\n</code></pre>"},{"location":"software/r.html","title":"R","text":""},{"location":"software/r.html#description","title":"Description","text":"<p>R is provided by an environment module and must be loaded explicitly:</p> <pre><code>module load R ##OR## module load R/3.4.4-foss-2018a-X11-20180131\nR --version\n  R version 4.0.0 (2020-04-24) -- \"Arbor Day\"\n  ...\n</code></pre> <p>The Vital-IT project is also providing some versions. The following commands will list the available versions:</p> <pre><code>module load vital-it\nmodule avail 2&gt;&amp;1 | grep \" R\\/\"\n  R/3.4.2\n  R/latest\n</code></pre> <p>To use one of these version, you have to load the respective module, which then masks the system\u2019s version, i.e.</p> <pre><code>module load vital-it\nmodule load R/3.4.2\n</code></pre> <p>Do not forget to put those two lines into your job script as well in order to use the same version from within the job later on a compute node!</p>"},{"location":"software/r.html#basic-topics","title":"Basic Topics","text":""},{"location":"software/r.html#customizing-the-r-workspace","title":"Customizing the R Workspace","text":"<p>At startup, unless \u2013no-init-file, or \u2013vanilla was given, R searches for a user profile in the current directory (from where R was started), or in the user\u2019s home directory (in that order). A different path of the user profile file can be specified by the R_PROFILE_USER environment variable. The found user profile is then sourced into the workspace. You can use this file to customize your workspace, i.e., to set specific options, define functions, load libraries, and so on. Consider the following example:</p> <p>.Rprofile <pre><code># Set some options\noptions(stringsAsFactors=FALSE)\noptions(max.print=100)\noptions(scipen=10)\n\n# Load  class library\nlibrary(class)\n\n# Don't save workspace by default\nq &lt;- function (save=\"no\", ...) {\n  quit(save=save, ...)\n}\n\n# User-defined function for setting standard seed\nmySeed &lt;- function() set.seed(5450)\n\n\n# User-defined function for calculating L1/L2-norm, returns euclidean distance (L2-norm) by default\nmyDistance &lt;- function(x, y, type=c(\"Euclidean\", \"L2\", \"Manhattan\", \"L1\")) {\n  type &lt;- match.arg(type)\n  if ((type == \"Manhattan\") | (type == \"L1\")) {\n    d &lt;- sum( abs(x - y) )\n  } else {\n    d &lt;- sqrt( sum( (x - y) ^ 2) )\n  }\n  return(d)\n}\n</code></pre></p>"},{"location":"software/r.html#installing-packages","title":"Installing Packages","text":"<p>R is installed as global Module in various versions. There are already a longer list of pre-installed packages available. If you need additional packages you can install them by yourself. The default location would be the R installation directory, which is not writeable for users. Nevertheless, in the following is shown how to install into a shared HPC Workspace or into your private HOME. </p>"},{"location":"software/r.html#a-into-a-shared-workspace","title":"A) Into a shared Workspace","text":"<p>With the Workspace tools we provide short-cuts to install R packages in the shared Workspace location. Therefore, the environment variable <code>$R_LIBS</code> is set to <code>$WORKSPACE/RPackages</code>. Initially this directory need to be created, using:</p> <p><pre><code>module load Workspace\nmkdir $R_LIBS\n</code></pre> If you get the error <code>mkdir: cannot create directory ...</code> verify that you just loaded one Workspace while installing the package.</p> <p>Then R packages can be installed using the <code>install.packages()</code> routine in an interactive R shell, e.g. for doParallel:</p> <pre><code>module load R\nR\n...\n&gt; install.packages(\"doParallel\")\n</code></pre> <p>Please follow the procedure as shown below at installation routine.</p> <p>Then the installed packaged will be available to you and all other Workspace members by simply loading the <code>Workspace</code> module. </p> <p>Please remember to add the Workspace and the R module to your job scripts: <pre><code>module load Workspace\nmodule load R\n</code></pre></p>"},{"location":"software/r.html#b-into-your-home","title":"B) Into your HOME","text":"<p>Note</p> <p>you can also use procedure A) and load <code>Workspace_Home</code> to install into your HOME directory.</p> <p>If you are not using a Workspace module and try to install a package, at the first time R tries to install the package into a global/generic location, which is not writeable by users. You can then select to install in a \u201cpersonal library\u201d into your HOME:</p> <pre><code>module load R\nR\n&gt; install.packages(\"doParallel\")\nInstalling package into \u2018/usr/lib64/R/library\u2019\n(as \u2018lib\u2019 is unspecified)\nWarnung in install.packages(\"doParallel\")\n  'lib = \"/usr/lib64/R/library\" ist nicht schreibbar\nWould you like to use a personal library instead?  (y/n)\n</code></pre> <p>Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory:</p> <pre><code>Would you like to create a personal library\n~/R/x86_64-redhat-linux-gnu-library/4.0\n</code></pre>"},{"location":"software/r.html#installation-routine","title":"Installation Routine","text":"<p>Next, select a CRAN mirror to download from. The mirror list will be not the same as below. The mirror list is constantly changing, but will look like it.</p> <p>Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below</p> <pre><code>--- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen ---\nError in download.file(url, destfile = f, quiet = TRUE) :\n  nicht unterst\u00fctztes URL Schema\nHTTPS CRAN mirror\n 1: 0-Cloud [https]                2: Austria [https]\n 3: Chile [https]                  4: China (Beijing 4) [https]\n 5: Colombia (Cali) [https]        6: France (Lyon 2) [https]\n 7: France (Paris 2) [https]       8: Germany (M\u00fcnster) [https]\n 9: Iceland [https]               10: Mexico (Mexico City) [https]\n11: Russia (Moscow) [https]       12: Spain (A Coru\u00f1a) [https]\n13: Switzerland [https]           14: UK (Bristol) [https]\n15: UK (Cambridge) [https]        16: USA (CA 1) [https]\n17: USA (KS) [https]              18: USA (MI 1) [https]\n19: USA (TN) [https]              20: USA (TX) [https]\n21: USA (WA) [https]              22: (HTTP mirrors)\n\nSelection: 22\nHTTP CRAN mirror\n 1: 0-Cloud                       2: Algeria\n 3: Argentina (La Plata)          4: Australia (Canberra)\n 5: Australia (Melbourne)         6: Austria\n 7: Belgium (Antwerp)             8: Belgium (Ghent)\n(...)\n65: Slovakia                     66: South Africa (Cape Town)\n67: South Africa (Johannesburg)  68: Spain (A Coru\u00f1a)\n69: Spain (Madrid)               70: Sweden\n71: Switzerland                  72: Taiwan (Chungli)\n73: Taiwan (Taipei)              74: Thailand\n75: Turkey (Denizli)             76: Turkey (Mersin)\n(...)\n93: USA (OH 2)                   94: USA (OR)\n95: USA (PA 2)                   96: USA (TN)\n97: USA (TX)                     98: USA (WA)\n99: Venezuela\nSelection: 71\n</code></pre> <p>Finally, the package gets installed. After installing the package you can close the interactive session by typing q().</p> <p>Do not forget to load the corresponding library (for each R session) before using functions provided by the package:</p> <pre><code>&gt; library(doParallel)\n</code></pre>"},{"location":"software/r.html#batch-execution-of-r","title":"Batch Execution of R","text":"<p>The syntax for running R non-interactively with input read from infile and output send to outfile is:</p> <pre><code>R CMD BATCH [options] infile [outfile]\n</code></pre> <p>Suppose you placed your R code in a file called foo.R:</p> <p>foo.R</p> <pre><code>set.seed(3000)\nvalx&lt;-seq(-2,2,0.01)\nvaly&lt;-2*valx+rnorm(length(valx),0,4)\n# Save plot to pdf\npdf('histplot.pdf')\nhist(valy,prob=TRUE,breaks=20, main=\"Histogram and PDF\",xlab=\"y\", ylim=c(0,0.15))\ncurve(dnorm(x,mean(valy),sd(valy)),add=T,col=\"red\")\ndev.off()\n</code></pre> <p>To execute foo.R on the cluster, add the R call to your job script\u2026</p> <p>Rbatch.sh</p> <pre><code>#! /bin/bash\n#SBATCH --mail-user=&lt;put your valid email address here!&gt;\n#SBATCH --mail-type=end,fail\n#SBATCH --time=01:00:00\n#SBATCH --mem-per-cpu=2G\n\n# Put your code below this line\nmodule load vital-it\nmodule load R/3.4.2\nR CMD BATCH --no-save --no-restore foo.R\n</code></pre> <p>\u2026and submit your job script to the cluster:</p> <pre><code>sbatch Rbatch.sh\n</code></pre>"},{"location":"software/r.html#advanced-topics","title":"Advanced Topics","text":""},{"location":"software/r.html#parallel-r","title":"Parallel R","text":"<p>By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded:</p> <pre><code>&gt; library(doParallel)\n&gt; library(foreach)\n</code></pre> <p>The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker:</p> <pre><code>&gt; getDoParWorkers()\n[1] 1\n</code></pre> <p>Hence, the following R code will execute on a single core (even with the %dopar% operator):</p> <pre><code>&gt; start.time &lt;- Sys.time()\n&gt; foreach(i=4:1, .combine='c', .inorder=FALSE) %dopar% {\n+ Sys.sleep(3*i)\n+ i\n+ }\nend.time &lt;- Sys.time()\nexec.time &lt;- end.time - start.time\n[1] 4 3 2 1\n</code></pre> <p>Let\u2019s measure the runtime of the sequentiall execution:</p> <pre><code>&gt; start.time &lt;- Sys.time(); foreach(i=4:1, .combine='c', .inorder=TRUE) %dopar% { Sys.sleep(3*i); i }; end.time &lt;- Sys.time(); exec.time &lt;- end.time - start.time; exec.time\n[1] 4 3 2 1\nTime difference of 30.04088 secs\n</code></pre> <p>Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node</p> <pre><code>&gt; detectCores()\n[1] 24\n</code></pre> <p>To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d), or if not set, to half of the cores detected by the parallel package. </p> <pre><code> registerDoParallel()\n&gt; getDoParWorkers()\n[1] 12\n</code></pre> <p>To assign 4 cores to the parallel backend:</p> <pre><code>&gt; registerDoParallel(cores=4)\n&gt; getDoParWorkers()\n[1] 4\n</code></pre> <p>Request the correct number of slots</p> <p>Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: <code>registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\"))</code></p> <p>Now, run the example again:</p> <pre><code>&gt; foreach(i=4:1, .combine='c', .inorder=FALSE) %dopar% {\n+ Sys.sleep(3*i)\n+ i\n+ }\n[1] 4 3 2 1\n</code></pre> <p>Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores:</p> <p>The binary operator %do% will always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met:</p> <ul> <li>registerDoParallel() must be called with a certain number of cores</li> <li>The %dopar% operator must be used in the foreach-loop to have it run in parallel!</li> </ul>"},{"location":"software/r.html#installing-deseq2-from-bioconductor-packages","title":"Installing DESeq2 from Bioconductor packages","text":"<p>DESeq2<sup>1</sup> installed from Bioconductor<sup>2</sup> has many dependencies. Two odd facts are hindering a succesful build of DESeq2 in first place:</p> <ul> <li>data.table is needed by Hmisc, which in turn is needed by DESeq2. While Hmisc is automatically installed prior to DESeq2, data.table is not and has to be installed manually first.</li> </ul> <ol> <li> <p>https://bioconductor.org/packages/release/bioc/html/DESeq2.html \u21a9</p> </li> <li> <p>https://bioconductor.org/ \u21a9</p> </li> </ol>"},{"location":"software/relion.html","title":"Relion","text":""},{"location":"software/relion.html#description","title":"Description","text":"<p>Some useful information on using Relion.</p>"},{"location":"software/relion.html#running-relion","title":"Running Relion","text":"<p>A standard submission script serves as a template for your Relion jobs. Create a file with the following content within your home directory:</p> <p>qsub.sh <pre><code>#!/bin/bash\n#SBATCH --mail-user=&lt;put your valid email address&gt;\n#SBATCH --mail-type=end,fail\n#SBATCH --ntasks=XXXmpinodesXXX\n#SBATCH --time=XXXextra1XXX\n#SBATCH --mem-per-cpu=XXXextra2XXX\n#SBATCH --partition=XXXqueueXXX\n#SBATCH --error=XXXerrfileXXX\n#SBATCH --output=XXXoutfileXXX\n\nmodule load relion/1.4\n\nmpiexec XXXcommandXXX\n####the end\n</code></pre></p> <p>Substitute your own email address!</p> <p>Keywords starting and finishing with \u201cXXX\u201d are recognized by Relion and should not be edited.</p> <p>To select a specific processor family you can edit the -pe option and subsitute \u201corte-sandy\u201d, \u201corte-ivy\u201d or \u201corte-broadwell\u201d for \u201corte\u201d.</p> <p>Now, you can set up tasks that will run on the cluster as follows:</p> <ol> <li>Start the Relion GUI</li> <li>Click on the \u201cRunning\u201d tab</li> <li>Add appropriate values for each option: Number of MPI procs: The number of processes the job should use. Number of threads: Currently only 1 thread is supported on UBELIX. Available RAM per thread: Relion jobs can be quite eager but it is impossible to precisely predict how much RAM each process will need. 4 is usually a good place to start. This option here is only indicative and puts no limit on the RAM that Relion can use. Nonetheless to prevent stupid mistakes, you should always enter the same amount of RAM here as in the option \u201cMaximum RAM per process\u201d (see below). Submit to queue: Must be set to yes if the aim is to run on UBELIX queuing system. Queue name: In general set it to \u00ab all \u00bb. If you want to use a specific queue, please refer to https://docs.id.unibe.ch/ubelix/advanced-topics/parallel-jobs     Queue submit command: Set it to \u201csbatch\u201d. Maximum CPU time: The maximum allowed running time. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for UBELIX usage. Maximum RAM process: The maximum allowed RAM per process allowed by UBELIX. If you ask for too much RAM your job is less likely to start fast. If you ask for too little RAM your job will crash. The error output by Relion and UBELIX in such case is not always explicit. Nevertheless, too little RAM is the most common cause of crash. Therefore if you experience an unexpected crash, try increasing the available RAM per thread. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for UBELIX usage. Note that unlike in \u201cAvailable RAM per thread\u201d option, you must append a \u201cG\u201d to the desired number of Gigabytes (for example, 4G). To prevent stupid mistake, you should always enter the same amount of RAM here as in the option \u201cAvailable RAM per thread\u201d.     Standard submission script: Path to the standard submission script described above.     Minimum dedicated core per node: Set to 1.</li> </ol>"},{"location":"software/relion.html#further-information","title":"Further Information","text":"<p>Relion wiki: http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page</p> <p>Tutorial: http://www2.mrc-lmb.cam.ac.uk/groups/scheres/relion13_tutorial.pdf</p>"},{"location":"software/singularity.html","title":"Singularity","text":""},{"location":"software/singularity.html#description","title":"Description","text":"<p>Put your scientific workflows, software and libraries in a Singularity container and run it on UBELIX</p>"},{"location":"software/singularity.html#examples","title":"Examples","text":""},{"location":"software/singularity.html#work-interactively","title":"Work interactively","text":"<p>Submit an interactive SLURM job and then use the shell command to spawn an interactive shell within the Singularity container:</p> <pre><code>srun --time=01:00:00 --mem-per-cpu=2G --pty bash\nsingularity shell &lt;image&gt;\n</code></pre>"},{"location":"software/singularity.html#execute-the-containers-runscript","title":"Execute the containers \u201crunscript\u201d","text":"<pre><code>#!/bin/bash\n#SBATCH --partition=all\n#SBATCH --mem-per-cpu=2G\n\nsingularity run &lt;image&gt;   #or ./&lt;image&gt;\n</code></pre>"},{"location":"software/singularity.html#run-a-command-within-your-container-image","title":"Run a command within your container image","text":"<pre><code>singularity exec &lt;image&gt; &lt;command&gt;\n\ne.g:\nsingularity exec container.img cat /etc/os-release\n</code></pre>"},{"location":"software/singularity.html#bind-directories","title":"Bind directories","text":"<p>Per default the started application (e.g. <code>cat</code> in the last example) runs withing the container. The container works like a seperate machine with own operation system etc. Thus, per default you have no access to files and directories outside the container. This can be changed using binding paths. </p> <p>If files are needed outside the container, e.g. in your HOME you can add the a path to <code>SINGULARITY_BINDPATH=\"src1[:dest1],src2[:dest2]</code>. All subdirectories and files will be accessible. Thus you could bind your HOME directory as:</p> <pre><code>export SINGULARITY_BINDPATH=\"$HOME/:$HOME/\"   \n# or simply \nexport SINGULARITY_BINDPATH=\"$HOME\"\n</code></pre>"},{"location":"software/singularity.html#further-information","title":"Further Information","text":"<p>Official Singularity Documentation can be found at https://sylabs.io/docs/</p>"},{"location":"software/terminal-multiplexer-tmux.html","title":"Terminal Multiplexer (tmux)","text":""},{"location":"software/terminal-multiplexer-tmux.html#description","title":"Description","text":"<p>Frequently, people want to run programs on the submit host independently from an SSH session. Besides allowing a user to access multiple terminal sessions inside a single terminal window, tmux also lets you separate a program from the Unix shell that started the program. Tmux allows you detach from your running tmux session (the session will keep running in the background) and attach to the same session later on. Because the tmux session is running on the remote server, your session persists even on logout.</p>"},{"location":"software/terminal-multiplexer-tmux.html#working-example","title":"Working Example","text":"<p>Start a new tmux session on the submit host:</p> <pre><code>$ tmux new -s first_session\n</code></pre> <p>This will automatically attach you to a tmux session named first_session.</p> <p>Do your work within your tmux session.</p> <p>Detach from the session:</p> <pre><code>Ctrl-b d\n</code></pre> <p>Now you cloud disconnect from the server and reconnect later on.</p> <p>List all your existing tmux session:</p> <pre><code>$ tmux ls\nfirst_session: 1 windows (created Wed Jan 14 15:23:11 2016) [80x85]\n```Bash\n\nReattach to an existing tmux session:\n\n```Bash\n$ tumb attach -t first_session\n</code></pre>"},{"location":"software/terminal-multiplexer-tmux.html#further-information","title":"Further Information","text":"<p>A tmux primer: https://danielmiessler.com/study/tmux</p>"}]}